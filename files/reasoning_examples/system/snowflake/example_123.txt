# [Question]: Retrieve all non-empty, non-commented lines from `README.md` files in GitHub repositories, excluding lines that are comments (either starting with `#` for Markdown or `//` for code comments). For each line, calculate how often each unique line appears across all repositories and return a comma-separated list of the programming languages used in each repository containing that line, sorted alphabetically, with the results ordered by the frequency of occurrence in descending order.
# [Schema links]: 
# [Analysis]: # Output: Letâ€™s think step by step.
1. Identify relevant tables: `SAMPLE_CONTENTS` (file content), `SAMPLE_FILES` (file metadata including repo_name and path), and `LANGUAGES` (repository language data).
2. Extract README.md files by filtering `SAMPLE_FILES` where `path` contains 'readme.md' in lowercase.
3. Join `SAMPLE_CONTENTS` with filtered `SAMPLE_FILES` on `id` to link content with repository metadata.
4. Extract programming languages from `LANGUAGES` table using `LATERAL FLATTEN` to unnest the language array.
5. Split the `content` column into individual lines using `LATERAL FLATTEN` and create a CTE `content_extracted` with line data and language information.
6. Filter out empty lines and comments by checking `TRIM(line.value)` is non-empty and does not start with `#` or `//`.
7. Aggregate lines by counting occurrences (`frequency`) and collecting associated programming languages into an array (`languages`).
8. Sort the languages array alphabetically, convert it to a comma-separated string, and remove quotes from the line text using `REGEXP_REPLACE`.
9. Order final results by `frequency` in descending order to prioritize most common lines.
# [Sql]: WITH content_extracted AS (     SELECT          "D"."id" AS "id",         "repo_name",         "path",         SPLIT("content", '\n') AS "lines",         "language_name"     FROM          (             SELECT                  "id",                 "content"             FROM                  "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS"         ) AS "D"     INNER JOIN          (             SELECT                  "id",                 "C"."repo_name" AS "repo_name",                 "path",                 "language_name"             FROM                  (                     SELECT                          "id",                         "repo_name",                         "path"                     FROM                          "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"                     WHERE                          LOWER("path") LIKE '%readme.md'                 ) AS "C"             INNER JOIN                  (                     SELECT                          "repo_name",                         "language_struct".value:"name" AS "language_name"                     FROM                          (                             SELECT                                  "repo_name",                                  "language"                             FROM                                  "GITHUB_REPOS"."GITHUB_REPOS"."LANGUAGES"                         )                     CROSS JOIN                          LATERAL FLATTEN(INPUT => "language") AS "language_struct"                 ) AS "F"             ON                  "C"."repo_name" = "F"."repo_name"         ) AS "E"     ON          "E"."id" = "D"."id" ), non_empty_lines AS (     SELECT          "line".value AS "line_",         "language_name"     FROM          content_extracted,         LATERAL FLATTEN(INPUT => "lines") AS "line"     WHERE          TRIM("line".value) != ''         AND NOT STARTSWITH(TRIM("line".value), '#')         AND NOT STARTSWITH(TRIM("line".value), '//') ), aggregated_languages AS (     SELECT          "line_",         COUNT(*) AS "frequency",         ARRAY_AGG("language_name") AS "languages"     FROM          non_empty_lines     GROUP BY          "line_" )  SELECT      REGEXP_REPLACE("line_", '^"|"$', '') AS "line",     "frequency",     ARRAY_TO_STRING(ARRAY_SORT("languages"), ', ') AS "languages_sorted" FROM      aggregated_languages ORDER BY      "frequency" DESC;