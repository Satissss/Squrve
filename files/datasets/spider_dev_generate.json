[
    {
        "instance_id": "sf_bq029",
        "db_id": "PATENTS",
        "question": "Get the average number of inventors per patent and the total count of patent publications in Canada (CA) for each 5-year period from 1960 to 2020, based on publication dates. Only include patents that have at least one inventor listed, and group results by 5-year intervals (1960-1964, 1965-1969, etc.).",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq029.csv"
    },
    {
        "instance_id": "sf_bq026",
        "db_id": "PATENTS",
        "question": "For the assignee who has been the most active in the patent category 'A61', I'd like to know the five patent jurisdictions code where they filed the most patents during their busiest year, separated by commas.",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq026.csv"
    },
    {
        "instance_id": "sf_bq091",
        "db_id": "PATENTS",
        "question": "In which year did the assignee with the most applications in the patent category 'A61' file the most?",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "WITH AA AS (\n    SELECT \n        FIRST_VALUE(\"assignee_harmonized\") OVER (PARTITION BY \"application_number\" ORDER BY \"application_number\") AS assignee_harmonized,\n        FIRST_VALUE(\"filing_date\") OVER (PARTITION BY \"application_number\" ORDER BY \"application_number\") AS filing_date,\n        \"application_number\"\n    FROM \n        PATENTS.PATENTS.PUBLICATIONS AS pubs\n        , LATERAL FLATTEN(input => pubs.\"cpc\") AS c\n    WHERE \n        c.value:\"code\" LIKE 'A61%'\n),\n\nPatentApplications AS (\n    SELECT \n        ANY_VALUE(assignee_harmonized) as assignee_harmonized,\n        ANY_VALUE(filing_date) as filing_date\n    FROM AA\n    GROUP BY \"application_number\"\n),\n\nAssigneeApplications AS (\nSELECT \n    COUNT(*) AS total_applications,\n    a.value::STRING AS assignee_name,\n    CAST(FLOOR(filing_date / 10000) AS INT) AS filing_year\nFROM \n    PatentApplications\n    , LATERAL FLATTEN(input => assignee_harmonized) AS a\nGROUP BY \n    a.value::STRING, filing_year\n),\n\nTotalApplicationsPerAssignee AS (\n    SELECT\n        assignee_name,\n        SUM(total_applications) AS total_applications\n    FROM \n        AssigneeApplications\n    GROUP BY \n        assignee_name\n    ORDER BY \n        total_applications DESC\n    LIMIT 1\n),\n\nMaxYearForTopAssignee AS (\n    SELECT\n        aa.assignee_name,\n        aa.filing_year,\n        aa.total_applications\n    FROM \n        AssigneeApplications aa\n    INNER JOIN\n        TotalApplicationsPerAssignee tapa ON aa.assignee_name = tapa.assignee_name\n    ORDER BY \n        aa.total_applications DESC\n    LIMIT 1\n)\n\nSELECT filing_year\nFROM \n    MaxYearForTopAssignee",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq091.csv"
    },
    {
        "instance_id": "sf_bq099",
        "db_id": "PATENTS",
        "question": "For patent class A01B3, I want to analyze the information of the top 3 assignees based on the total number of applications. Please provide the following five pieces of information: the name of this assignee,  total number of applications, the year with the most applications, the number of applications in that year, and the country code with the most applications during that year.",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "WITH PatentApplications AS (\n   SELECT \n        \"assignee_harmonized\" AS assignee_harmonized,\n        \"filing_date\" AS filing_date,\n        \"country_code\" AS country_code,\n        \"application_number\" AS application_number\n    FROM \n        PATENTS.PATENTS.PUBLICATIONS AS pubs,\n        LATERAL FLATTEN(input => pubs.\"cpc\") AS c\n    WHERE c.value:\"code\" LIKE 'A01B3%'\n\n),\n\nAssigneeApplications AS (\n    SELECT \n        COUNT(*) AS year_country_cnt,\n        a.value:\"name\" AS assignee_name,\n        CAST(FLOOR(filing_date / 10000) AS INT) AS filing_year,\n        apps.country_code as country_code\n    FROM \n        PatentApplications as apps,\n        LATERAL FLATTEN(input => assignee_harmonized) AS a\n    GROUP BY \n        assignee_name, filing_year, country_code\n),\n\nRankedApplications AS (\n    SELECT\n        assignee_name,\n        filing_year,\n        country_code,\n        year_country_cnt,\n        SUM(year_country_cnt) OVER (PARTITION BY assignee_name, filing_year) AS total_cnt,\n        ROW_NUMBER() OVER (PARTITION BY assignee_name, filing_year ORDER BY year_country_cnt DESC) AS rn\n    FROM\n        AssigneeApplications\n),\n\nAggregatedData AS (\n    SELECT\n        total_cnt AS year_cnt,\n        assignee_name,\n        filing_year,\n        country_code\n    FROM\n        RankedApplications\n    WHERE\n        rn = 1\n)\n\n\nSELECT \n    total_count,\n    REPLACE(assignee_name, '\"', '') AS assignee_name,\n    year_cnt,\n    filing_year,\n    country_code\nFROM (\n    SELECT \n        year_cnt,\n        assignee_name,\n        filing_year,\n        country_code,\n        SUM(year_cnt) OVER (PARTITION BY assignee_name) AS total_count,\n        ROW_NUMBER() OVER (PARTITION BY assignee_name ORDER BY year_cnt DESC) AS rn\n    FROM\n        AggregatedData\n    ORDER BY assignee_name\n) sub\nWHERE rn = 1\nORDER BY total_count\nDESC\nLIMIT 3",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq099.csv"
    },
    {
        "instance_id": "sf_bq033",
        "db_id": "PATENTS",
        "question": "How many U.S. publications related to IoT (where the abstract includes the phrase 'internet of things') were filed each month from 2008 to 2022, including months with no filings?",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "WITH Patent_Matches AS (\n    SELECT\n      TO_DATE(CAST(ANY_VALUE(patentsdb.\"filing_date\") AS STRING), 'YYYYMMDD') AS Patent_Filing_Date,\n      patentsdb.\"application_number\" AS Patent_Application_Number,\n      MAX(abstract_info.value:\"text\") AS Patent_Title,\n      MAX(abstract_info.value:\"language\") AS Patent_Title_Language\n    FROM\n      PATENTS.PATENTS.PUBLICATIONS AS patentsdb,\n      LATERAL FLATTEN(input => patentsdb.\"abstract_localized\") AS abstract_info\n    WHERE\n      LOWER(abstract_info.value:\"text\") LIKE '%internet of things%'\n      AND patentsdb.\"country_code\" = 'US'\n    GROUP BY\n      Patent_Application_Number\n),\n\nDate_Series_Table AS (\n    SELECT\n        DATEADD(day, seq4(), DATE '2008-01-01') AS day,\n        0 AS Number_of_Patents\n    FROM\n        TABLE(\n            GENERATOR(\n                ROWCOUNT => 5479\n            )\n        )\n    ORDER BY\n        day\n)\n\nSELECT\n  TO_CHAR(Date_Series_Table.day, 'YYYY-MM') AS Patent_Date_YearMonth,\n  COUNT(Patent_Matches.Patent_Application_Number) AS Number_of_Patent_Applications\nFROM\n  Date_Series_Table\n  LEFT JOIN Patent_Matches\n    ON Date_Series_Table.day = Patent_Matches.Patent_Filing_Date\nWHERE\n    Date_Series_Table.day < DATE '2023-01-01'\nGROUP BY\n  TO_CHAR(Date_Series_Table.day, 'YYYY-MM')\nORDER BY\n  Patent_Date_YearMonth;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq033.csv"
    },
    {
        "instance_id": "sf_bq209",
        "db_id": "PATENTS",
        "question": "Can you calculate the number of utility patents that were granted in 2010 and have exactly one forward citation within a 10-year window following their application/filing date? For this analysis, forward citations should be counted as distinct citing application numbers that cited the patent within 10 years after the patent's own filing date.",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "WITH patents_sample AS (\n    SELECT\n        t1.\"publication_number\",\n        t1.\"application_number\"\n    FROM\n        PATENTS.PATENTS.PUBLICATIONS t1\n    WHERE\n        TO_DATE(\n            CASE\n                WHEN t1.\"grant_date\" != 0 THEN TO_CHAR(t1.\"grant_date\")\n                ELSE NULL\n            END, \n            'YYYYMMDD'\n        ) BETWEEN TO_DATE('20100101', 'YYYYMMDD') AND TO_DATE('20101231', 'YYYYMMDD')\n),\nforward_citation AS (\n    SELECT\n        patents_sample.\"publication_number\",\n        COUNT(DISTINCT t3.\"citing_application_number\") AS \"forward_citations\"\n    FROM\n        patents_sample\n        LEFT JOIN (\n            SELECT\n                x2.\"publication_number\",\n                TO_DATE(\n                    CASE\n                        WHEN x2.\"filing_date\" != 0 THEN TO_CHAR(x2.\"filing_date\")\n                        ELSE NULL\n                    END,\n                    'YYYYMMDD'\n                ) AS \"filing_date\"\n            FROM\n                PATENTS.PATENTS.PUBLICATIONS x2\n            WHERE\n                x2.\"filing_date\" != 0\n        ) t2\n            ON t2.\"publication_number\" = patents_sample.\"publication_number\"\n        LEFT JOIN (\n            SELECT\n                x3.\"publication_number\" AS \"citing_publication_number\",\n                x3.\"application_number\" AS \"citing_application_number\",\n                TO_DATE(\n                    CASE\n                        WHEN x3.\"filing_date\" != 0 THEN TO_CHAR(x3.\"filing_date\")\n                        ELSE NULL\n                    END,\n                    'YYYYMMDD'\n                ) AS \"joined_filing_date\",\n                cite.value:\"publication_number\"::STRING AS \"cited_publication_number\"\n            FROM\n                PATENTS.PATENTS.PUBLICATIONS x3,\n                LATERAL FLATTEN(INPUT => x3.\"citation\") cite\n            WHERE\n                x3.\"filing_date\" != 0\n        ) t3\n            ON patents_sample.\"publication_number\" = t3.\"cited_publication_number\"\n            AND t3.\"joined_filing_date\" BETWEEN t2.\"filing_date\" AND DATEADD(YEAR, 10, t2.\"filing_date\")\n    GROUP BY\n        patents_sample.\"publication_number\"\n)\n\nSELECT\n    COUNT(*)\nFROM\n    forward_citation\nWHERE\n    \"forward_citations\" = 1;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq209.csv"
    },
    {
        "instance_id": "sf_bq027",
        "db_id": "PATENTS",
        "question": "For patents granted between 2010 and 2018, provide the publication number of each patent and the number of backward citations it has received in the SEA category.",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq027.csv"
    },
    {
        "instance_id": "sf_bq210",
        "db_id": "PATENTS",
        "question": "How many US B2 patents granted between 2008 and 2018 contain claims that do not include the word 'claim'?",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "WITH patents_sample AS (\n  SELECT \n    t1.\"publication_number\" AS publication_number,\n    claim.value:\"text\" AS claims_text\n  FROM \n    PATENTS.PATENTS.PUBLICATIONS t1,\n    LATERAL FLATTEN(input => t1.\"claims_localized\") AS claim\n  WHERE \n    t1.\"country_code\" = 'US'\n    AND t1.\"grant_date\" BETWEEN 20080101 AND 20181231\n    AND t1.\"grant_date\" != 0\n    AND t1.\"publication_number\" LIKE '%B2%'\n),\nPublication_data AS (\n  SELECT\n    publication_number,\n    COUNT_IF(claims_text NOT LIKE '%claim%') AS nb_indep_claims\n  FROM\n    patents_sample\n  GROUP BY\n    publication_number\n)\n\nSELECT COUNT(nb_indep_claims)\nFROM Publication_data\nWHERE nb_indep_claims != 0",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq210.csv"
    },
    {
        "instance_id": "sf_bq211",
        "db_id": "PATENTS",
        "question": "Among patents granted between 2010 and 2023 in CN, how many of them belong to families that have a total of over one distinct applications?",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq211.csv"
    },
    {
        "instance_id": "sf_bq213",
        "db_id": "PATENTS",
        "question": "What is the most common 4-digit IPC code among US B2 utility patents granted from June to August in 2022?",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "WITH interim_table as(\nSELECT \n    t1.\"publication_number\", \n    SUBSTR(ipc_u.value:\"code\", 0, 4) as ipc4\nFROM \n    PATENTS.PATENTS.PUBLICATIONS t1,\n    LATERAL FLATTEN(input => t1.\"ipc\") AS ipc_u\nWHERE\n\"country_code\" = 'US'  \nAND \"grant_date\" between 20220601 AND 20220831\n  AND \"grant_date\" != 0\n  AND \"publication_number\" LIKE '%B2%'  \nGROUP BY \n    t1.\"publication_number\", \n    ipc4\n) \nSELECT \nipc4\nFROM \ninterim_table \nGROUP BY ipc4\nORDER BY COUNT(\"publication_number\") DESC\nLIMIT 1",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\patents_info.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq213.csv"
    },
    {
        "instance_id": "sf_bq212",
        "db_id": "PATENTS",
        "question": "For United States utility patents under the B2 classification granted between June and September of 2022, identify the most frequent 4-digit IPC code for each patent. Then, list the publication numbers and IPC4 codes of patents where this code appears 10 or more times.",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\patents_info.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq212.csv"
    },
    {
        "instance_id": "sf_bq214",
        "db_id": "PATENTS_GOOGLE",
        "question": "For United States utility patents under the B2 classification granted between 2010 and 2014, find the one with the most forward citations within a month of its filing date, and identify the most similar patent from the same filing year, regardless of its type.",
        "db_type": "snowflake",
        "db_size": 87,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\patents_info.md",
        "error_info": "Error occurred while executing act() on sample 11: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/PATENTS_GOOGLE/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq216",
        "db_id": "PATENTS_GOOGLE",
        "question": "Identify the top five patents filed in the same year as `US-9741766-B2` that are most similar to it based on technological similarities. Please provide the publication numbers.",
        "db_type": "snowflake",
        "db_size": 87,
        "query": "WITH patents_sample AS (\n    SELECT \n        \"publication_number\", \n        \"application_number\"\n    FROM\n        PATENTS_GOOGLE.PATENTS_GOOGLE.PUBLICATIONS\n    WHERE\n        \"publication_number\" = 'US-9741766-B2'\n),\nflattened_t5 AS (\n    SELECT\n        t5.\"publication_number\",\n        f.value AS element_value,\n        f.index AS pos\n    FROM\n        PATENTS_GOOGLE.PATENTS_GOOGLE.ABS_AND_EMB t5,\n        LATERAL FLATTEN(input => t5.\"embedding_v1\") AS f\n),\nflattened_t6 AS (\n    SELECT\n        t6.\"publication_number\",\n        f.value AS element_value,\n        f.index AS pos\n    FROM\n        PATENTS_GOOGLE.PATENTS_GOOGLE.ABS_AND_EMB t6,\n        LATERAL FLATTEN(input => t6.\"embedding_v1\") AS f\n),\nsimilarities AS (\n    SELECT\n        t1.\"publication_number\" AS base_publication_number,\n        t4.\"publication_number\" AS similar_publication_number,\n        SUM(ft5.element_value * ft6.element_value) AS similarity\n    FROM\n        (SELECT * FROM patents_sample LIMIT 1) t1\n    LEFT JOIN (\n        SELECT \n            x3.\"publication_number\",\n            EXTRACT(YEAR, TO_DATE(CAST(x3.\"filing_date\" AS STRING), 'YYYYMMDD')) AS focal_filing_year\n        FROM \n            PATENTS_GOOGLE.PATENTS_GOOGLE.PUBLICATIONS x3\n        WHERE \n            x3.\"filing_date\" != 0\n    ) t3 ON t3.\"publication_number\" = t1.\"publication_number\"\n    LEFT JOIN (\n        SELECT \n            x4.\"publication_number\",\n            EXTRACT(YEAR, TO_DATE(CAST(x4.\"filing_date\" AS STRING), 'YYYYMMDD')) AS filing_year\n        FROM \n            PATENTS_GOOGLE.PATENTS_GOOGLE.PUBLICATIONS x4\n        WHERE \n            x4.\"filing_date\" != 0\n    ) t4 ON\n        t4.\"publication_number\" != t1.\"publication_number\"\n        AND t3.focal_filing_year = t4.filing_year\n    LEFT JOIN flattened_t5 AS ft5 ON ft5.\"publication_number\" = t1.\"publication_number\"\n    LEFT JOIN flattened_t6 AS ft6 ON ft6.\"publication_number\" = t4.\"publication_number\"\n    AND ft5.pos = ft6.pos  -- Align vector positions\n    GROUP BY\n        t1.\"publication_number\", t4.\"publication_number\"\n)\nSELECT\n    s.similar_publication_number,\n    s.similarity\nFROM (\n    SELECT\n        s.*,\n        ROW_NUMBER() OVER (PARTITION BY s.base_publication_number ORDER BY s.similarity DESC) AS seqnum\n    FROM\n        similarities s\n) s\nWHERE\n    seqnum <= 5;",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\patents_info.md",
        "error_info": "Error occurred while executing act() on sample 12: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/PATENTS_GOOGLE/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq247",
        "db_id": "PATENTS_GOOGLE",
        "question": "From the publications dataset, first identify the top six families with the most publications whose family_id is not '-1'. Then, using the abs_and_emb table (joined on publication_number), provide each of those familiesâ€™ IDs alongside every non-empty abstract associated with their publications.",
        "db_type": "snowflake",
        "db_size": 87,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 13: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/PATENTS_GOOGLE/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq127",
        "db_id": "PATENTS_GOOGLE",
        "question": "For each publication family whose earliest publication was first published in January 2015, please provide the earliest publication date, the distinct publication numbers, their country codes, the distinct CPC and IPC codes, distinct families (namely, the ids) that cite and are cited by this publication family. Please present all lists as comma-separated values, sorted alphabetically",
        "db_type": "snowflake",
        "db_size": 87,
        "query": "WITH fam AS (\n  SELECT DISTINCT\n    \"family_id\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\"\n),\n\ncrossover AS (\n  SELECT\n    \"publication_number\",\n    \"family_id\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\"\n),\n\npub AS (\n  SELECT\n    \"family_id\",\n    MIN(\"publication_date\") AS \"publication_date\",\n    LISTAGG(\"publication_number\", ',') WITHIN GROUP (ORDER BY \"publication_number\") AS \"publication_number\",\n    LISTAGG(\"country_code\", ',') WITHIN GROUP (ORDER BY \"country_code\") AS \"country_code\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS p\n  GROUP BY\n    \"family_id\"\n),\n\ntech_class AS (\n  SELECT\n    p.\"family_id\",\n    LISTAGG(DISTINCT cpc.value:\"code\"::STRING, ',') WITHIN GROUP (ORDER BY cpc.value:\"code\"::STRING) AS \"cpc\",\n    LISTAGG(DISTINCT ipc.value:\"code\"::STRING, ',') WITHIN GROUP (ORDER BY ipc.value:\"code\"::STRING) AS \"ipc\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS p\n    CROSS JOIN LATERAL FLATTEN(input => p.\"cpc\") AS cpc\n    CROSS JOIN LATERAL FLATTEN(input => p.\"ipc\") AS ipc\n  GROUP BY\n    p.\"family_id\"\n),\n\ncit AS (\n  SELECT\n    p.\"family_id\",\n    LISTAGG(crossover.\"family_id\", ',') WITHIN GROUP (ORDER BY crossover.\"family_id\" ASC) AS \"citation\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"PUBLICATIONS\" AS p\n    CROSS JOIN LATERAL FLATTEN(input => p.\"citation\") AS citation\n    LEFT JOIN\n      crossover\n    ON\n      citation.value:\"publication_number\"::STRING = crossover.\"publication_number\"\n  GROUP BY\n    p.\"family_id\"\n),\n\ntmp_gpr AS (\n  SELECT\n    \"family_id\",\n    LISTAGG(crossover.\"publication_number\", ',') AS \"cited_by_publication_number\"\n  FROM\n    \"PATENTS_GOOGLE\".\"PATENTS_GOOGLE\".\"ABS_AND_EMB\" AS p\n    CROSS JOIN LATERAL FLATTEN(input => p.\"cited_by\") AS cited_by\n    LEFT JOIN\n      crossover\n    ON\n      cited_by.value:\"publication_number\"::STRING = crossover.\"publication_number\"\n  GROUP BY\n    \"family_id\"\n),\n\ngpr AS (\n  SELECT\n    tmp_gpr.\"family_id\",\n    LISTAGG(crossover.\"family_id\", ',') WITHIN GROUP (ORDER BY crossover.\"family_id\" ASC) AS \"cited_by\"\n  FROM\n    tmp_gpr\n    CROSS JOIN LATERAL FLATTEN(input => SPLIT(tmp_gpr.\"cited_by_publication_number\", ',')) AS cited_by_publication_number\n    LEFT JOIN\n      crossover\n    ON\n      cited_by_publication_number.value::STRING = crossover.\"publication_number\"\n  GROUP BY\n    tmp_gpr.\"family_id\"\n)\n\nSELECT\n  fam.\"family_id\",\n  pub.\"publication_date\",\n  pub.\"publication_number\",\n  pub.\"country_code\",\n  tech_class.\"cpc\",\n  tech_class.\"ipc\",\n  cit.\"citation\",\n  gpr.\"cited_by\"\nFROM\n  fam\n  LEFT JOIN pub ON fam.\"family_id\" = pub.\"family_id\"\n  LEFT JOIN tech_class ON fam.\"family_id\" = tech_class.\"family_id\"\n  LEFT JOIN cit ON fam.\"family_id\" = cit.\"family_id\"\n  LEFT JOIN gpr ON fam.\"family_id\" = gpr.\"family_id\"\nWHERE\n  pub.\"publication_date\" BETWEEN 20150101 AND 20150131;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 14: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/PATENTS_GOOGLE/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq215",
        "db_id": "PATENTS",
        "question": "Which US patent (with a B2 kind code and a grant date between 2015 and 2018) has the highest originality score calculated as 1 - (the sum of squared occurrences of distinct 4-digit IPC codes in its backward citations divided by the square of the total occurrences of these 4-digit IPC codes)?",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\patents_info.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq215.csv"
    },
    {
        "instance_id": "sf_bq222",
        "db_id": "PATENTS",
        "question": "Find the CPC technology areas in Germany that had the highest exponential moving average (smoothing factor 0.1) of patent filings per year, specifically for patents granted in December 2016. For each CPC group at level 4, show the full title, CPC group, and the year with the highest exponential moving average of patent filings.",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "WITH patent_cpcs AS (\n    SELECT\n        cd.\"parents\",\n        CAST(FLOOR(\"filing_date\" / 10000) AS INT) AS \"filing_year\"\n    FROM (\n        SELECT MAX(\"cpc\") AS \"cpc\", MAX(\"filing_date\") AS \"filing_date\"\n        FROM \"PATENTS\".\"PATENTS\".\"PUBLICATIONS\"\n        WHERE \"application_number\" != ''\n          AND \"country_code\" = 'DE'\n          AND \"grant_date\" >= 20161201\n          AND \"grant_date\" <= 20161231\n        GROUP BY \"application_number\"\n    ), LATERAL FLATTEN(INPUT => \"cpc\") AS cpcs\n    JOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" cd ON cd.\"symbol\" = cpcs.value:\"code\"\n    WHERE cpcs.value:\"first\" = TRUE\n      AND \"filing_date\" > 0\n),\nyearly_counts AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        COUNT(*) AS \"cnt\"\n    FROM (\n        SELECT\n            cpc_parent.VALUE AS \"cpc_group\",  -- Corrected reference to flattened \"parents\"\n            \"filing_year\"\n        FROM patent_cpcs,\n             LATERAL FLATTEN(INPUT => \"parents\") AS cpc_parent  -- Corrected reference to flattened \"parents\"\n    )\n    GROUP BY \"cpc_group\", \"filing_year\"\n),\nmoving_avg AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        \"cnt\",\n        AVG(\"cnt\") OVER (PARTITION BY \"cpc_group\" ORDER BY \"filing_year\" ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS \"moving_avg\"\n    FROM yearly_counts\n)\nSELECT \n    c.\"titleFull\",  -- Ensure correct column name (check case)\n    REPLACE(\"cpc_group\", '\"', '') AS \"cpc_group\",\n    MAX(\"filing_year\") AS \"best_filing_year\"\nFROM moving_avg\nJOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" c ON \"cpc_group\" = c.\"symbol\"\nWHERE c.\"level\" = 4\nGROUP BY c.\"titleFull\", \"cpc_group\"\nORDER BY c.\"titleFull\", \"cpc_group\" ASC;",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\sliding_windows_calculation_cpc.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq222.csv"
    },
    {
        "instance_id": "sf_bq221",
        "db_id": "PATENTS",
        "question": "Identify the CPC technology areas with the highest exponential moving average of patent filings each year (with a smoothing factor of 0.2), considering only the first CPC code for each patent that has a valid filing date and a non-empty application number, and report the full CPC title along with the best year associated with the highest exponential moving average for each CPC group at level 5.",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "WITH patent_cpcs AS (\n    SELECT\n        cd.\"parents\",\n        CAST(FLOOR(\"filing_date\" / 10000) AS INT) AS \"filing_year\"\n    FROM (\n        SELECT\n            MAX(\"cpc\") AS \"cpc\", MAX(\"filing_date\") AS \"filing_date\"\n        FROM\n            PATENTS.PATENTS.PUBLICATIONS\n        WHERE \n            \"application_number\" != ''\n        GROUP BY\n            \"application_number\"\n    ) AS publications\n    , LATERAL FLATTEN(INPUT => \"cpc\") AS cpcs\n    JOIN\n        PATENTS.PATENTS.CPC_DEFINITION cd ON cd.\"symbol\" = cpcs.value:\"code\"\n    WHERE \n        cpcs.value:\"first\" = TRUE\n          AND \"filing_date\" > 0\n\n),\nyearly_counts AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        COUNT(*) AS \"cnt\"\n    FROM (\n        SELECT\n            cpc_parent.value::STRING AS \"cpc_group\",\n            \"filing_year\"\n        FROM patent_cpcs,\n             LATERAL FLATTEN(input => patent_cpcs.\"parents\") AS cpc_parent\n    )\n    GROUP BY \"cpc_group\", \"filing_year\"\n),\nordered_counts AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        \"cnt\",\n        ROW_NUMBER() OVER (PARTITION BY \"cpc_group\" ORDER BY \"filing_year\" ASC) AS rn\n    FROM yearly_counts\n),\nrecursive_ema AS (\n    -- Anchor member: first year per cpc_group\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        \"cnt\",\n        \"cnt\" * 0.2 + 0 * 0.8 AS \"ema\",\n        rn\n    FROM ordered_counts\n    WHERE rn = 1\n\n    UNION ALL\n\n    -- Recursive member: subsequent years\n    SELECT\n        oc.\"cpc_group\",\n        oc.\"filing_year\",\n        oc.\"cnt\",\n        oc.\"cnt\" * 0.2 + re.\"ema\" * 0.8 AS \"ema\",\n        oc.rn\n    FROM ordered_counts oc\n    JOIN recursive_ema re\n        ON oc.\"cpc_group\" = re.\"cpc_group\"\n       AND oc.rn = re.rn + 1\n),\nmax_ema AS (\n    SELECT\n        \"cpc_group\",\n        \"filing_year\",\n        \"ema\"\n    FROM recursive_ema\n),\nranked_ema AS (\n    SELECT\n        me.\"cpc_group\",\n        me.\"filing_year\",\n        me.\"ema\",\n        ROW_NUMBER() OVER (\n            PARTITION BY me.\"cpc_group\" \n            ORDER BY me.\"ema\" DESC, me.\"filing_year\" DESC\n        ) AS rn_rank\n    FROM max_ema me\n)\nSELECT \n    c.\"titleFull\",\n    REPLACE(r.\"cpc_group\", '\"', '') AS \"cpc_group\",\n    r.\"filing_year\" AS \"best_filing_year\"\nFROM ranked_ema r\nJOIN \"PATENTS\".\"PATENTS\".\"CPC_DEFINITION\" c \n    ON r.\"cpc_group\" = c.\"symbol\"\nWHERE \n    c.\"level\" = 5\n    AND r.rn_rank = 1\nORDER BY \n    c.\"titleFull\", \n    \"cpc_group\" ASC;",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\sliding_windows_calculation_cpc.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq221.csv"
    },
    {
        "instance_id": "sf_bq223",
        "db_id": "PATENTS",
        "question": "Which assignees, excluding DENSO CORP itself, have cited patents assigned to DENSO CORP, and what are the titles of the primary CPC subclasses associated with these citations? Provide the name of each citing assignee (excluding DENSO CORP), the full title of the primary CPC subclass (based on the first CPC code), and the count of citations grouped by the citing assignee and the CPC subclass title. Ensure that only citations of patents with valid filing dates are considered, and focus on the first CPC code for each citing patent. The results should specifically exclude DENSO CORP as a citing assignee.",
        "db_type": "snowflake",
        "db_size": 79,
        "query": "SELECT\n    REPLACE(citing_assignee, '\"', '') AS citing_assignee,\n    cpcdef.\"titleFull\" AS cpc_title,\n    COUNT(*) AS number\nFROM (\n    SELECT\n        pubs.\"publication_number\" AS citing_publication_number,\n        cite.value:\"publication_number\" AS cited_publication_number,\n        citing_assignee_s.value:\"name\" AS citing_assignee,\n        SUBSTR(cpcs.value:\"code\", 1, 4) AS citing_cpc_subclass\n    FROM \n        PATENTS.PATENTS.PUBLICATIONS AS pubs\n    , LATERAL FLATTEN(input => pubs.\"citation\") AS cite\n    , LATERAL FLATTEN(input => pubs.\"assignee_harmonized\") AS citing_assignee_s\n    , LATERAL FLATTEN(input => pubs.\"cpc\") AS cpcs\n    WHERE\n        cpcs.value:\"first\" = TRUE\n) AS pubs\nJOIN (\n    SELECT\n        \"publication_number\" AS cited_publication_number,\n        cited_assignee_s.value:\"name\" AS cited_assignee\n    FROM\n        PATENTS.PATENTS.PUBLICATIONS\n    , LATERAL FLATTEN(input => \"assignee_harmonized\") AS cited_assignee_s\n) AS refs\n    ON pubs.cited_publication_number = refs.cited_publication_number\nJOIN\n    PATENTS.PATENTS.CPC_DEFINITION AS cpcdef\n    ON cpcdef.\"symbol\" = pubs.citing_cpc_subclass\nWHERE\n    refs.cited_assignee = 'DENSO CORP'\n    AND pubs.citing_assignee != 'DENSO CORP'\nGROUP BY\n    citing_assignee, cpcdef.\"titleFull\"",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\patents_info.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq223.csv"
    },
    {
        "instance_id": "sf_bq036",
        "db_id": "GITHUB_REPOS",
        "question": "What was the average number of GitHub commits made per month in 2016 for repositories containing Python code?",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq036.csv"
    },
    {
        "instance_id": "sf_bq100",
        "db_id": "GITHUB_REPOS",
        "question": "How can we identify the top 10 most frequently used packages in GitHub repository contents by looking for import statements enclosed in parentheses, splitting any multi-line imports by newlines, extracting package names that appear within double quotes, counting how often these packages appear, ignoring any null results, and finally ordering them in descending order of their frequency? The final answer should remove the quotation marks.",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq100.csv"
    },
    {
        "instance_id": "sf_bq101",
        "db_id": "GITHUB_REPOS",
        "question": "From GitHub Repos contents, how can we identify the top 10 most frequently imported package names in Java source files by splitting each file's content into lines, filtering for valid import statements, extracting only the package portion using a suitable regex, grouping by these extracted package names, counting their occurrences, and finally returning the 10 packages that appear most often in descending order of frequency?",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq101.csv"
    },
    {
        "instance_id": "sf_bq225",
        "db_id": "GITHUB_REPOS",
        "question": "From the GitHub repository files in 'github_repos.sample_files' joined with 'github_repos.sample_contents', which 10 programming languages occur most frequently (based on recognized file extensions) in files that have non-empty content, ordered by their file counts in descending order?",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\lang_and_ext.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq225.csv"
    },
    {
        "instance_id": "sf_bq180",
        "db_id": "GITHUB_REPOS",
        "question": "Get the top 5 most frequently used module names from Python (`.py`) and R (`.r`) scripts, counting occurrences of modules in `import` and `from` statements for Python, and `library()` calls for R. The query should consider only Python and R files, group by module name, and return the top 5 modules ordered by frequency.",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq180.csv"
    },
    {
        "instance_id": "sf_bq233",
        "db_id": "GITHUB_REPOS",
        "question": "Can you analyze the joined data from github repos files and github_repos contents, focusing only on files ending with '.py' or '.r', then extract Python modules from 'import' or 'from ... import' lines and R libraries from 'library(...)' lines, count their occurrences, and finally list the results sorted by language and by the number of occurrences in descending order?",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "WITH extracted_modules AS (\nSELECT \n    el.\"file_id\" AS \"file_id\", \n    el.\"repo_name\", \n    el.\"path\" AS \"path_\", \n    REPLACE(line.value, '\"', '') AS \"line_\",\n    CASE\n        WHEN ENDSWITH(el.\"path\", '.py') THEN 'python'\n        WHEN ENDSWITH(el.\"path\", '.r') THEN 'r'\n        ELSE NULL\n    END AS \"language\",\n    CASE\n        WHEN ENDSWITH(el.\"path\", '.py') THEN\n            ARRAY_CAT(\n                ARRAY_CONSTRUCT(REGEXP_SUBSTR(line.value, '\\\\bimport\\\\s+(\\\\w+)', 1, 1, 'e')),\n                ARRAY_CONSTRUCT(REGEXP_SUBSTR(line.value, '\\\\bfrom\\\\s+(\\\\w+)', 1, 1, 'e'))\n            )\n        WHEN ENDSWITH(el.\"path\", '.r') THEN\n            ARRAY_CONSTRUCT(REGEXP_SUBSTR(line.value, 'library\\\\s*\\\\(\\\\s*([^\\\\s)]+)\\\\s*\\\\)', 1, 1, 'e'))\n        ELSE ARRAY_CONSTRUCT()\n    END AS \"modules\"\nFROM (\n    SELECT\n        ct.\"id\" AS \"file_id\", \n        fl.\"repo_name\" AS \"repo_name\", \n        fl.\"path\", \n        SPLIT(REPLACE(ct.\"content\", '\\n', ' \\n'), '\\n') AS \"lines\"\n    FROM \n        GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_FILES AS fl\n    JOIN \n        GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_CONTENTS AS ct \n        ON fl.\"id\" = ct.\"id\"\n) AS el,\nLATERAL FLATTEN(input => el.\"lines\") AS line \nWHERE\n    (\n        ENDSWITH(\"path_\", '.py') \n        AND \n        (\n            \"line_\" LIKE 'import %' \n            OR \n            \"line_\" LIKE 'from %'\n        )\n    )\n    OR\n    (\n        ENDSWITH(\"path_\", '.r') \n        AND \n        \"line_\" LIKE 'library%('\n    )\n\n),\nmodule_counts AS (\n    SELECT \n        em.\"language\",\n        f.value::STRING AS \"module\",\n        COUNT(*) AS \"occurrence_count\"\n    FROM \n        extracted_modules AS em,\n        LATERAL FLATTEN(input => em.\"modules\") AS f\n    WHERE \n        em.\"modules\" IS NOT NULL\n        AND f.value IS NOT NULL\n    GROUP BY \n        em.\"language\", \n        f.value\n),\npython AS (\n    SELECT \n        \"language\",\n        \"module\",\n        \"occurrence_count\"\n    FROM \n        module_counts\n    WHERE \n        \"language\" = 'python'\n),\nrlanguage AS (\n    SELECT \n        \"language\",\n        \"module\",\n        \"occurrence_count\"\n    FROM \n        module_counts AS mc_inner\n    WHERE \n        \"language\" = 'r'\n)\nSELECT \n    *\nFROM \n    python\nUNION ALL\nSELECT \n    *\nFROM \n    rlanguage\nORDER BY \n    \"language\", \n    \"occurrence_count\" DESC;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq233.csv"
    },
    {
        "instance_id": "sf_bq248",
        "db_id": "GITHUB_REPOS",
        "question": "Among all repositories that do not use any programming language whose name (case-insensitively) includes the substring \"python,\" what is the proportion of files whose paths include \"readme.md\" and whose contents contain the phrase \"Copyright (c)\"?",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "WITH requests AS (\n    SELECT \n        D.\"id\",\n        D.\"content\",\n        E.\"repo_name\",\n        E.\"path\"\n    FROM \n        (\n            SELECT \n                \"id\",\n                \"content\"\n            FROM \n                GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS\n            GROUP BY \n                \"id\", \"content\"\n        ) AS D\n    INNER JOIN \n        (\n            SELECT \n                C.\"id\",\n                C.\"repo_name\",\n                C.\"path\"\n            FROM \n                (\n                    SELECT \n                        \"id\",\n                        \"repo_name\",\n                        \"path\"\n                    FROM \n                        GITHUB_REPOS.GITHUB_REPOS.SAMPLE_FILES\n                    WHERE \n                        LOWER(\"path\") LIKE '%readme.md'\n                    GROUP BY \n                        \"path\", \"id\", \"repo_name\"\n                ) AS C\n            INNER JOIN \n                (\n                    SELECT \n                        \"repo_name\",\n                        language_struct.value:\"name\"::STRING AS \"language_name\"\n                    FROM \n                        GITHUB_REPOS.GITHUB_REPOS.LANGUAGES,\n                        LATERAL FLATTEN(input => \"language\") AS language_struct\n                    WHERE \n                        LOWER(language_struct.value:\"name\"::STRING) NOT LIKE '%python%'\n                    GROUP BY \n                        \"language_name\", \"repo_name\"\n                ) AS F\n            ON \n                C.\"repo_name\" = F.\"repo_name\"\n        ) AS E\n    ON \n        D.\"id\" = E.\"id\"\n)\nSELECT \n    (SELECT COUNT(*) FROM requests WHERE \"content\" LIKE '%Copyright (c)%') / COUNT(*) AS \"proportion\"\nFROM \n    requests;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq248.csv"
    },
    {
        "instance_id": "sf_bq193",
        "db_id": "GITHUB_REPOS",
        "question": "Retrieve all non-empty, non-commented lines from `README.md` files in GitHub repositories, excluding lines that are comments (either starting with `#` for Markdown or `//` for code comments). For each line, calculate how often each unique line appears across all repositories and return a comma-separated list of the programming languages used in each repository containing that line, sorted alphabetically, with the results ordered by the frequency of occurrence in descending order.",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "WITH content_extracted AS (\n    SELECT \n        \"D\".\"id\" AS \"id\",\n        \"repo_name\",\n        \"path\",\n        SPLIT(\"content\", '\\n') AS \"lines\",\n        \"language_name\"\n    FROM \n        (\n            SELECT \n                \"id\",\n                \"content\"\n            FROM \n                \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_CONTENTS\"\n        ) AS \"D\"\n    INNER JOIN \n        (\n            SELECT \n                \"id\",\n                \"C\".\"repo_name\" AS \"repo_name\",\n                \"path\",\n                \"language_name\"\n            FROM \n                (\n                    SELECT \n                        \"id\",\n                        \"repo_name\",\n                        \"path\"\n                    FROM \n                        \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"SAMPLE_FILES\"\n                    WHERE \n                        LOWER(\"path\") LIKE '%readme.md'\n                ) AS \"C\"\n            INNER JOIN \n                (\n                    SELECT \n                        \"repo_name\",\n                        \"language_struct\".value:\"name\" AS \"language_name\"\n                    FROM \n                        (\n                            SELECT \n                                \"repo_name\", \n                                \"language\"\n                            FROM \n                                \"GITHUB_REPOS\".\"GITHUB_REPOS\".\"LANGUAGES\"\n                        )\n                    CROSS JOIN \n                        LATERAL FLATTEN(INPUT => \"language\") AS \"language_struct\"\n                ) AS \"F\"\n            ON \n                \"C\".\"repo_name\" = \"F\".\"repo_name\"\n        ) AS \"E\"\n    ON \n        \"E\".\"id\" = \"D\".\"id\"\n),\nnon_empty_lines AS (\n    SELECT \n        \"line\".value AS \"line_\",\n        \"language_name\"\n    FROM \n        content_extracted,\n        LATERAL FLATTEN(INPUT => \"lines\") AS \"line\"\n    WHERE \n        TRIM(\"line\".value) != ''\n        AND NOT STARTSWITH(TRIM(\"line\".value), '#')\n        AND NOT STARTSWITH(TRIM(\"line\".value), '//')\n),\naggregated_languages AS (\n    SELECT \n        \"line_\",\n        COUNT(*) AS \"frequency\",\n        ARRAY_AGG(\"language_name\") AS \"languages\"\n    FROM \n        non_empty_lines\n    GROUP BY \n        \"line_\"\n)\n\nSELECT \n    REGEXP_REPLACE(\"line_\", '^\"|\"$', '') AS \"line\",\n    \"frequency\",\n    ARRAY_TO_STRING(ARRAY_SORT(\"languages\"), ', ') AS \"languages_sorted\"\nFROM \n    aggregated_languages\nORDER BY \n    \"frequency\" DESC;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq193.csv"
    },
    {
        "instance_id": "sf_bq249",
        "db_id": "GITHUB_REPOS",
        "question": "Please provide a report on the number of occurrences of specific line types across files from the GitHub repository. Categorize a line as 'trailing' if it ends with a blank character, as 'Space' if it starts with a space, and as 'Other' if it meets neither condition. The report should include the total number of occurrences for each category, considering all lines across all files.",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq249.csv"
    },
    {
        "instance_id": "sf_bq375",
        "db_id": "GITHUB_REPOS",
        "question": "Determine which file type among Python (.py), C (.c), Jupyter Notebook (.ipynb), Java (.java), and JavaScript (.js) in the GitHub codebase has the most files with a directory depth greater than 10, and provide the file count.",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq375.csv"
    },
    {
        "instance_id": "sf_bq255",
        "db_id": "GITHUB_REPOS",
        "question": "How many commit messages are there in repositories that use the 'Shell' programming language and 'apache-2.0' license, where the length of the commit message is more than 5 characters but less than 10,000 characters, and the messages do not start with the word 'merge', 'update' or 'test'?",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "SELECT\n  COUNT(commits_table.\"message\") AS \"num_messages\"\nFROM (\n  SELECT\n    L.\"repo_name\",\n    language_struct.value:\"name\"::STRING AS \"language_name\"\n  FROM\n    GITHUB_REPOS.GITHUB_REPOS.LANGUAGES AS L,\n    LATERAL FLATTEN(input => L.\"language\") AS language_struct\n) AS lang_table\nJOIN \n  GITHUB_REPOS.GITHUB_REPOS.LICENSES AS license_table\nON \n  license_table.\"repo_name\" = lang_table.\"repo_name\"\nJOIN (\n  SELECT\n    *\n  FROM\n    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_COMMITS\n) AS commits_table\nON \n  commits_table.\"repo_name\" = lang_table.\"repo_name\"\nWHERE\n  license_table.\"license\" LIKE 'apache-2.0'\n  AND lang_table.\"language_name\" LIKE 'Shell'\n  AND LENGTH(commits_table.\"message\") > 5\n  AND LENGTH(commits_table.\"message\") < 10000\n  AND LOWER(commits_table.\"message\") NOT LIKE 'update%'\n  AND LOWER(commits_table.\"message\") NOT LIKE 'test%'\n  AND LOWER(commits_table.\"message\") NOT LIKE 'merge%';",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq255.csv"
    },
    {
        "instance_id": "sf_bq194",
        "db_id": "GITHUB_REPOS",
        "question": "Among all Python (*.py), R (*.r, *.R, *.Rmd, *.rmd), and IPython notebook (*.ipynb) files in the GitHub sample dataset, which library or module is identified as the second most frequently imported or loaded based on the extracted import statements?",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq194.csv"
    },
    {
        "instance_id": "sf_bq377",
        "db_id": "GITHUB_REPOS",
        "question": "Extract and count the frequency of all package names listed in the require section of JSON-formatted content",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "WITH json_files AS (\n  SELECT\n    c.\"id\",\n    TRY_PARSE_JSON(c.\"content\"):\"require\" AS \"dependencies\"\n  FROM\n    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS c\n),\npackage_names AS (\n  SELECT\n    f.key AS \"package_name\"\n  FROM\n    json_files,\n    LATERAL FLATTEN(input => \"dependencies\") AS f\n)\nSELECT\n  \"package_name\",\n  COUNT(*) AS \"count\"\nFROM\n  package_names\nWHERE\n  \"package_name\" IS NOT NULL\nGROUP BY\n  \"package_name\"\nORDER BY\n  \"count\" DESC;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq377.csv"
    },
    {
        "instance_id": "sf_bq359",
        "db_id": "GITHUB_REPOS",
        "question": "List the repository names and commit counts for the top two GitHub repositories with JavaScript as the primary language and the highest number of commits.",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "WITH repositories AS (\n    SELECT\n        t2.\"repo_name\",\n        t2.\"language\"\n    FROM (\n        SELECT\n            t1.\"repo_name\",\n            t1.\"language\",\n            RANK() OVER (PARTITION BY t1.\"repo_name\" ORDER BY t1.\"language_bytes\" DESC) AS \"rank\"\n        FROM (\n            SELECT\n                l.\"repo_name\",\n                lang.value:\"name\"::STRING AS \"language\",\n                lang.value:\"bytes\"::NUMBER AS \"language_bytes\"\n            FROM\n                GITHUB_REPOS.GITHUB_REPOS.LANGUAGES AS l,\n                LATERAL FLATTEN(input => l.\"language\") AS lang\n        ) AS t1\n    ) AS t2\n    WHERE t2.\"rank\" = 1\n),\npython_repo AS (\n    SELECT\n        \"repo_name\",\n        \"language\"\n    FROM\n        repositories\n    WHERE\n        \"language\" = 'JavaScript'\n)\nSELECT \n    sc.\"repo_name\", \n    COUNT(sc.\"commit\") AS \"num_commits\"\nFROM \n    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_COMMITS AS sc\nINNER JOIN \n    python_repo \nON \n    python_repo.\"repo_name\" = sc.\"repo_name\"\nGROUP BY \n    sc.\"repo_name\"\nORDER BY \n    \"num_commits\" DESC\nLIMIT 2;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq359.csv"
    },
    {
        "instance_id": "sf_bq252",
        "db_id": "GITHUB_REPOS",
        "question": "Could you please find the name of the repository that contains the most copied non-binary Swift file in the dataset, ensuring each file is uniquely identified by its ID?",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "WITH selected_repos AS (\n  SELECT\n    f.\"id\",\n    f.\"repo_name\" AS \"repo_name\",\n    f.\"path\" AS \"path\"\n  FROM\n    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_FILES AS f\n),\ndeduped_files AS (\n  SELECT\n    f.\"id\",\n    MIN(f.\"repo_name\") AS \"repo_name\",\n    MIN(f.\"path\") AS \"path\"\n  FROM\n    selected_repos AS f\n  GROUP BY\n    f.\"id\"\n)\nSELECT\n  f.\"repo_name\"\nFROM\n  deduped_files AS f\n  JOIN GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS AS c \n  ON f.\"id\" = c.\"id\"\nWHERE\n  NOT c.\"binary\"\n  AND f.\"path\" LIKE '%.swift'\nORDER BY c.\"copies\" DESC\nLIMIT 1;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq252.csv"
    },
    {
        "instance_id": "sf_bq251",
        "db_id": "PYPI",
        "question": "I want to know the GitHub project URLs for the top 3 most downloaded PyPI packages based on download count. First, extract PyPI package metadata including name, version, and project URLs. Filter these URLs to only include those that link to GitHub repositories. Use a regular expression to clean the GitHub URLs by removing unnecessary parts like 'issues', 'pull', 'blob', and 'tree' paths, keeping only the main repository URL. For packages with multiple versions, use only the most recent version based on upload time. Join this data with download metrics to determine the most downloaded packages. Return only the cleaned GitHub repository URLs (without quotation marks) for the top 3 packages by total download count, ensuring that only packages with valid GitHub URLs are included in the results.",
        "db_type": "snowflake",
        "db_size": 45,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq251.csv"
    },
    {
        "instance_id": "sf_bq291",
        "db_id": "NOAA_GLOBAL_FORECAST_SYSTEM",
        "question": "Can you provide a daily weather summary for July 2019 within a 5 km radius of latitude 26.75 and longitude 51.5? I need the maximum, minimum, and average temperatures; total precipitation; average cloud cover between 10 AM and 5 PM; total snowfall (when average temperature is below 32Â°F); and total rainfall (when average temperature is 32Â°F or above) for each forecast date. The data should correspond to forecasts created in July 2019 for the following day.",
        "db_type": "snowflake",
        "db_size": 90,
        "query": "WITH daily_forecasts AS (\n    SELECT\n        \"TRI\".\"creation_time\",\n\n        CAST(DATEADD(hour, 1, TO_TIMESTAMP_NTZ(TO_NUMBER(\"forecast\".value:\"time\") / 1000000)) AS DATE) AS \"local_forecast_date\",\n        MAX(\n            CASE \n                WHEN \"forecast\".value:\"temperature_2m_above_ground\" IS NOT NULL \n                THEN \"forecast\".value:\"temperature_2m_above_ground\" \n                ELSE NULL \n            END\n        ) AS \"max_temp\",\n        MIN(\n            CASE \n                WHEN \"forecast\".value:\"temperature_2m_above_ground\" IS NOT NULL \n                THEN \"forecast\".value:\"temperature_2m_above_ground\" \n                ELSE NULL \n            END\n        ) AS \"min_temp\",\n        AVG(\n            CASE \n                WHEN \"forecast\".value:\"temperature_2m_above_ground\" IS NOT NULL \n                THEN \"forecast\".value:\"temperature_2m_above_ground\" \n                ELSE NULL \n            END\n        ) AS \"avg_temp\",\n        SUM(\n            CASE \n                WHEN \"forecast\".value:\"total_precipitation_surface\" IS NOT NULL \n                THEN \"forecast\".value:\"total_precipitation_surface\" \n                ELSE 0 \n            END\n        ) AS \"total_precipitation\",\n        AVG(\n            CASE \n                WHEN CAST(DATEADD(hour, 1, TO_TIMESTAMP_NTZ(TO_NUMBER(\"forecast\".value:\"time\") / 1000000)    ) AS TIME) BETWEEN '10:00:00' AND '17:00:00'\n                     AND \"forecast\".value:\"total_cloud_cover_entire_atmosphere\" IS NOT NULL \n                THEN \"forecast\".value:\"total_cloud_cover_entire_atmosphere\" \n                ELSE NULL \n            END\n        ) AS \"avg_cloud_cover\",\n        CASE\n            WHEN AVG(\"forecast\".value:\"temperature_2m_above_ground\") < 32 THEN \n                SUM(\n                    CASE \n                        WHEN \"forecast\".value:\"total_precipitation_surface\" IS NOT NULL \n                        THEN \"forecast\".value:\"total_precipitation_surface\" \n                        ELSE 0 \n                    END\n                )\n            ELSE 0\n        END AS \"total_snow\",\n        CASE\n            WHEN AVG(\"forecast\".value:\"temperature_2m_above_ground\") >= 32 THEN \n                SUM(\n                    CASE \n                        WHEN \"forecast\".value:\"total_precipitation_surface\" IS NOT NULL \n                        THEN \"forecast\".value:\"total_precipitation_surface\" \n                        ELSE 0 \n                    END\n                )\n            ELSE 0\n        END AS \"total_rain\"\n    FROM\n        \"NOAA_GLOBAL_FORECAST_SYSTEM\".\"NOAA_GLOBAL_FORECAST_SYSTEM\".\"NOAA_GFS0P25\" AS \"TRI\"\n    CROSS JOIN LATERAL FLATTEN(input => \"TRI\".\"forecast\") AS \"forecast\"\n    WHERE\n        TO_TIMESTAMP_NTZ(TO_NUMBER(\"TRI\".\"creation_time\") / 1000000) BETWEEN '2019-07-01' AND '2021-07-31'  \n        AND ST_DWITHIN(\n            ST_GEOGFROMWKB(\"TRI\".\"geography\"),\n            ST_POINT(26.75, 51.5),\n            5000\n        )\n        AND CAST(TO_TIMESTAMP_NTZ(TO_NUMBER(\"forecast\".value:\"time\") / 1000000) AS DATE) = DATEADD(day, 1, CAST( TO_TIMESTAMP_NTZ(TO_NUMBER(\"TRI\".\"creation_time\") / 1000000) AS DATE))\n    GROUP BY\n        \"TRI\".\"creation_time\",\n        \"local_forecast_date\"\n)\n\nSELECT\n    TO_TIMESTAMP_NTZ(TO_NUMBER(\"creation_time\") / 1000000),\n    \"local_forecast_date\" AS \"forecast_date\",\n    \"max_temp\",\n    \"min_temp\",\n    \"avg_temp\",\n    \"total_precipitation\",\n    \"avg_cloud_cover\",\n    \"total_snow\",\n    \"total_rain\"\nFROM\n    daily_forecasts\nORDER BY\n    \"creation_time\",\n    \"forecast_date\";",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\functions_st_within.md",
        "error_info": "Error occurred while executing act() on sample 35: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/NOAA_GLOBAL_FORECAST_SYSTEM/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq017",
        "db_id": "GEO_OPENSTREETMAP",
        "question": "What are the five longest types of highways within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35') by total length, analyzed through planet features?",
        "db_type": "snowflake",
        "db_size": 86,
        "query": "WITH bounding_area AS (\n    SELECT \"geometry\" AS geometry\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES,\n    LATERAL FLATTEN(INPUT => planet_features.\"all_tags\") AS \"tag\"\n    WHERE \"feature_type\" = 'multipolygons'\n      AND \"tag\".value:\"key\" = 'wikidata'\n      AND \"tag\".value:\"value\" = 'Q35'\n),\n\nhighway_info AS (\n    SELECT \n        SUM(ST_LENGTH(\n                ST_GEOGRAPHYFROMWKB(planet_features.\"geometry\")\n            )\n        ) AS highway_length,\n        \"tag\".value:\"value\" AS highway_type\n    FROM \n        GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES AS planet_features,\n        bounding_area\n    CROSS JOIN LATERAL FLATTEN(INPUT => planet_features.\"all_tags\") AS \"tag\"\n    WHERE \"tag\".value:\"key\" = 'highway'\n    AND \"feature_type\" = 'lines'\n    AND ST_DWITHIN(\n        ST_GEOGFROMWKB(planet_features.\"geometry\"), \n        ST_GEOGFROMWKB(bounding_area.geometry),\n        0.0\n    ) \n    GROUP BY highway_type\n)\n\nSELECT \n  REPLACE(highway_type, '\"', '') AS highway_type\nFROM\n  highway_info\nORDER BY \n  highway_length DESC\nLIMIT 5;",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\functions_st_dwithin.md",
        "error_info": "Error occurred while executing act() on sample 36: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GEO_OPENSTREETMAP/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq131",
        "db_id": "GEO_OPENSTREETMAP",
        "question": "What is the number of bus stops for the bus network with the most stops within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35'), analyzed through planet features?",
        "db_type": "snowflake",
        "db_size": 86,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\functions_st_dwithin.md",
        "error_info": "Error occurred while executing act() on sample 37: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GEO_OPENSTREETMAP/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq349",
        "db_id": "GEO_OPENSTREETMAP",
        "question": "Which OpenStreetMap ID from the planet features table corresponds to an administrative boundary, represented as multipolygons, whose total number of 'amenity'-tagged Points of Interest (POIs), as derived from the planet nodes table, is closest to the median count among all such boundaries?",
        "db_type": "snowflake",
        "db_size": 86,
        "query": "WITH bounding_area AS (\n    SELECT \n        \"osm_id\",\n        \"geometry\" AS geometry,\n        ST_AREA(ST_GEOGRAPHYFROMWKB(\"geometry\")) AS area\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES,\n    LATERAL FLATTEN(INPUT => PLANET_FEATURES.\"all_tags\") AS \"tag\"\n    WHERE \n        \"feature_type\" = 'multipolygons'\n        AND \"tag\".value:\"key\" = 'boundary'\n        AND \"tag\".value:\"value\" = 'administrative'\n),\n\npoi AS (\n    SELECT \n        nodes.\"id\" AS poi_id,\n        nodes.\"geometry\" AS poi_geometry,\n        tags.value:\"value\" AS poitype\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_NODES AS nodes,\n    LATERAL FLATTEN(INPUT => nodes.\"all_tags\") AS tags\n    WHERE tags.value:\"key\" = 'amenity'\n),\n\npoi_counts AS (\n    SELECT\n        ba.\"osm_id\",\n        COUNT(poi.poi_id) AS total_pois\n    FROM bounding_area ba\n    JOIN poi\n    ON ST_DWITHIN(\n        ST_GEOGRAPHYFROMWKB(ba.geometry), \n        ST_GEOGRAPHYFROMWKB(poi.poi_geometry), \n        0.0\n    )\n    GROUP BY ba.\"osm_id\"\n),\n\nmedian_value AS (\n    SELECT \n        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_pois) AS median_pois\n    FROM poi_counts\n),\n\nclosest_to_median AS (\n    SELECT\n        \"osm_id\",\n        total_pois,\n        ABS(total_pois - (SELECT median_pois FROM median_value)) AS diff_from_median\n    FROM poi_counts\n)\n\nSELECT\n    \"osm_id\"\nFROM closest_to_median\nORDER BY diff_from_median\nLIMIT 1;",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\functions_st_dwithin.md",
        "error_info": "Error occurred while executing act() on sample 38: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GEO_OPENSTREETMAP/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq348",
        "db_id": "GEO_OPENSTREETMAP",
        "question": "Within the rectangular area defined by the geogpoints (31.1798246, 18.4519921), (54.3798246, 18.4519921), (54.3798246, 33.6519921), and (31.1798246, 33.6519921), which are the top three usernames responsible for the highest number of historical nodes, originally tagged with the amenities â€˜hospitalâ€™, â€˜clinicâ€™, or â€˜doctorsâ€™, that do not appear anymore in the current planet_nodes dataset?",
        "db_type": "snowflake",
        "db_size": 86,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\functions_st_intersects_polygon_line.md",
        "error_info": "Error occurred while executing act() on sample 39: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GEO_OPENSTREETMAP/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq253",
        "db_id": "GEO_OPENSTREETMAP",
        "question": "Find the name of the OpenStreetMap relation that encompasses the most features within the same geographic area as the multipolygon tagged with the Wikidata item 'Q1095'. The relation should have a specified name and no 'wikidata' tag, and at least one of its included features must have a 'wikidata' tag. The analysis should be conducted using the planet_features table. Return the name of this relation.",
        "db_type": "snowflake",
        "db_size": 86,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\functions_st_dwithin.md",
        "error_info": "Error occurred while executing act() on sample 40: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GEO_OPENSTREETMAP/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq254",
        "db_id": "GEO_OPENSTREETMAP",
        "question": "Among all multipolygons located within the same geographic area as the multipolygon associated with Wikidata item Q191, but lacking a 'wikidata' tag themselves, which two rank highest by the number of points that lie within their boundaries, and what are their names?",
        "db_type": "snowflake",
        "db_size": 86,
        "query": "WITH bounding_area AS (\n    SELECT \"geometry\" AS geometry\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES,\n    LATERAL FLATTEN(INPUT => \"all_tags\") AS tag\n    WHERE \"feature_type\" = 'multipolygons'\n      AND tag.value:\"key\" = 'wikidata'\n      AND tag.value:\"value\" = 'Q191'\n),\nbounding_area_features AS (\n    SELECT \n        planet_features.\"osm_id\", \n        planet_features.\"feature_type\", \n        planet_features.\"geometry\", \n        planet_features.\"all_tags\"\n    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES AS planet_features,\n         bounding_area\n    WHERE ST_DWITHIN(\n        ST_GEOGFROMWKB(planet_features.\"geometry\"), \n        ST_GEOGFROMWKB(bounding_area.geometry), \n        0.0\n    )\n),\nosm_id_with_wikidata AS (\n    SELECT DISTINCT\n        baf.\"osm_id\"\n    FROM bounding_area_features AS baf,\n         LATERAL FLATTEN(INPUT => baf.\"all_tags\") AS tag\n    WHERE tag.value:\"key\" = 'wikidata'\n),\n\npolygons_wo_wikidata AS (\n    SELECT \n        baf.\"osm_id\",\n        tag.value:\"value\" as name,\n        baf.\"geometry\" as geometry\n    FROM bounding_area_features AS baf\n    LEFT JOIN osm_id_with_wikidata AS wd\n      ON baf.\"osm_id\" = wd.\"osm_id\",\n    LATERAL FLATTEN(INPUT => \"all_tags\") AS tag\n    WHERE wd.\"osm_id\" IS NULL\n    AND baf.\"osm_id\" IS NOT NULL\n    AND baf.\"feature_type\" = 'multipolygons'\n    AND tag.value:\"key\" = 'name'\n)\n\nSELECT \n    TRIM(pww.name) as name\nFROM bounding_area_features AS baf\nJOIN polygons_wo_wikidata AS pww\n    ON ST_DWITHIN(\n        ST_GEOGFROMWKB(baf.\"geometry\"), \n        ST_GEOGFROMWKB(pww.geometry), \n        0.0\n    )\nLEFT JOIN osm_id_with_wikidata AS wd\n    ON baf.\"osm_id\" = wd.\"osm_id\"\nWHERE wd.\"osm_id\" IS NOT NULL\n  AND baf.\"feature_type\" = 'points'\nGROUP BY pww.name\nORDER BY COUNT(baf.\"osm_id\") DESC\nLIMIT 2",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\functions_st_dwithin.md",
        "error_info": "Error occurred while executing act() on sample 41: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GEO_OPENSTREETMAP/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq250",
        "db_id": "GEO_OPENSTREETMAP_WORLDPOP",
        "question": "Based on the most recent 1km population grid data in Singapore before January 2023, using ST_CONVEXHULL to aggregate all population grid centroids into a bounding region and ST_INTERSECTS to identify hospitals from OpenStreetMapâ€™s planet layer (layer_code in (2110, 2120)) that fall within this region, then calculating the distance from each grid cell to its nearest hospital, what is the total population of the grid cell that is farthest from any hospital?",
        "db_type": "snowflake",
        "db_size": 94,
        "query": "WITH country_name AS (\n  SELECT 'Singapore' AS value\n),\n\nlast_updated AS (\n  SELECT\n    MAX(\"last_updated\") AS value\n  FROM GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM AS pop\n    INNER JOIN country_name ON (pop.\"country_name\" = country_name.value)\n  WHERE \"last_updated\" < '2023-01-01'\n),\n\naggregated_population AS (\n  SELECT\n    \"geo_id\",\n    SUM(\"population\") AS sum_population,\n    ST_POINT(\"longitude_centroid\", \"latitude_centroid\") AS centr  -- è®¡ç®—æ¯ä¸ª geo_id çš„ä¸­å¿ƒç‚¹\n  FROM\n    GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM AS pop\n    INNER JOIN country_name ON (pop.\"country_name\" = country_name.value)\n    INNER JOIN last_updated ON (pop.\"last_updated\" = last_updated.value)\n  GROUP BY \"geo_id\", \"longitude_centroid\", \"latitude_centroid\"\n),\n\npopulation AS (\n  SELECT\n    SUM(sum_population) AS sum_population,\n    ST_ENVELOPE(ST_UNION_AGG(centr)) AS boundingbox  -- ä½¿ç”¨ ST_ENVELOPE æ¥ä»£æ›¿ ST_CONVEXHULL\n  FROM aggregated_population\n),\n\nhospitals AS (\n  SELECT\n    layer.\"geometry\"\n  FROM\n    GEO_OPENSTREETMAP_WORLDPOP.GEO_OPENSTREETMAP.PLANET_LAYERS AS layer\n    INNER JOIN population ON ST_INTERSECTS(population.boundingbox, ST_GEOGFROMWKB(layer.\"geometry\"))\n  WHERE\n    layer.\"layer_code\" IN (2110, 2120)\n),\n\ndistances AS (\n  SELECT\n    pop.\"geo_id\",\n    pop.\"population\",\n    MIN(ST_DISTANCE(ST_GEOGFROMWKB(pop.\"geog\"), ST_GEOGFROMWKB(hospitals.\"geometry\"))) AS distance\n  FROM\n    GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM AS pop\n    INNER JOIN country_name ON pop.\"country_name\" = country_name.value\n    INNER JOIN last_updated ON pop.\"last_updated\" = last_updated.value\n    CROSS JOIN hospitals\n  WHERE pop.\"population\" > 0\n  GROUP BY \"geo_id\", \"population\"\n)\n\nSELECT\n  SUM(pd.\"population\") AS population\nFROM\n  distances pd\nCROSS JOIN population p\nGROUP BY distance\nORDER BY distance DESC\nLIMIT 1;",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\OpenStreetMap_data_in_layered_GIS_format.md",
        "error_info": "Error occurred while executing act() on sample 42: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GEO_OPENSTREETMAP_WORLDPOP/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq012",
        "db_id": "ETHEREUM_BLOCKCHAIN",
        "question": "Calculate the average balance (in quadrillions, 10^15) of the top 10 Ethereum addresses by net balance, including incoming and outgoing transfers from traces (only successful transactions and excluding call types like delegatecall, callcode, and staticcall), miner rewards (sum of gas fees per block), and sender gas fee deductions. Exclude null addresses and round the result to two decimal places.",
        "db_type": "snowflake",
        "db_size": 88,
        "query": "WITH double_entry_book AS (\n  -- Debits\n  SELECT \n    \"to_address\" AS \"address\",\n    \"value\" AS \"value\"\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRACES\"\n  WHERE \"to_address\" IS NOT NULL\n    AND \"status\" = 1\n    AND (\"call_type\" NOT IN ('delegatecall', 'callcode', 'staticcall') OR \"call_type\" IS NULL)\n  \n  UNION ALL\n  \n  -- Credits\n  SELECT \n    \"from_address\" AS \"address\",\n    - \"value\" AS \"value\"\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRACES\"\n  WHERE \"from_address\" IS NOT NULL\n    AND \"status\" = 1\n    AND (\"call_type\" NOT IN ('delegatecall', 'callcode', 'staticcall') OR \"call_type\" IS NULL)\n  \n  UNION ALL\n  \n  -- Transaction fees debits\n  SELECT \n    \"miner\" AS \"address\",\n    SUM(CAST(\"receipt_gas_used\" AS NUMBER) * CAST(\"gas_price\" AS NUMBER)) AS \"value\"\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRANSACTIONS\" AS \"transactions\"\n  JOIN \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"BLOCKS\" AS \"blocks\"\n    ON \"blocks\".\"number\" = \"transactions\".\"block_number\"\n  GROUP BY \"blocks\".\"miner\"\n  \n  UNION ALL\n  \n  -- Transaction fees credits\n  SELECT \n    \"from_address\" AS \"address\",\n    -(CAST(\"receipt_gas_used\" AS NUMBER) * CAST(\"gas_price\" AS NUMBER)) AS \"value\"\n  FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TRANSACTIONS\"\n),\ntop_10_balances AS (\n  SELECT\n    \"address\",\n    SUM(\"value\") AS \"balance\"\n  FROM double_entry_book\n  GROUP BY \"address\"\n  ORDER BY \"balance\" DESC\n  LIMIT 10\n)\nSELECT \n    ROUND(AVG(\"balance\") / 1e15, 2) AS \"average_balance_trillion\"\nFROM top_10_balances;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 43: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/ETHEREUM_BLOCKCHAIN/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq187",
        "db_id": "ETHEREUM_BLOCKCHAIN",
        "question": "Calculate the total circulating supply of 'BNB' tokens (in units divided by 10^18) by summing balances of all non-zero addresses, where each addressâ€™s balance equals its total received BNB minus sent BNB. Exclude transactions involving the zero address (0x000...) for both senders and receivers.",
        "db_type": "snowflake",
        "db_size": 88,
        "query": "WITH tokenInfo AS (\n    SELECT \"address\"\n    FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKENS\"\n    WHERE \"name\" = 'BNB'\n),\n\nreceivedTx AS (\n    SELECT \"tx\".\"to_address\" AS \"addr\", \n           \"tokens\".\"name\" AS \"name\", \n           SUM(CAST(\"tx\".\"value\" AS FLOAT) / POWER(10, 18)) AS \"amount_received\"\n    FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKEN_TRANSFERS\" AS \"tx\"\n    JOIN tokenInfo ON \"tx\".\"token_address\" = tokenInfo.\"address\"\n    JOIN \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKENS\" AS \"tokens\"\n      ON \"tx\".\"token_address\" = \"tokens\".\"address\"\n    WHERE \"tx\".\"to_address\" <> '0x0000000000000000000000000000000000000000'\n    GROUP BY \"tx\".\"to_address\", \"tokens\".\"name\"\n),\n\nsentTx AS (\n    SELECT \"tx\".\"from_address\" AS \"addr\", \n           \"tokens\".\"name\" AS \"name\", \n           SUM(CAST(\"tx\".\"value\" AS FLOAT) / POWER(10, 18)) AS \"amount_sent\"\n    FROM \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKEN_TRANSFERS\" AS \"tx\"\n    JOIN tokenInfo ON \"tx\".\"token_address\" = tokenInfo.\"address\"\n    JOIN \"ETHEREUM_BLOCKCHAIN\".\"ETHEREUM_BLOCKCHAIN\".\"TOKENS\" AS \"tokens\"\n      ON \"tx\".\"token_address\" = \"tokens\".\"address\"\n    WHERE \"tx\".\"from_address\" <> '0x0000000000000000000000000000000000000000'\n    GROUP BY \"tx\".\"from_address\", \"tokens\".\"name\"\n),\n\nwalletBalances AS (\n    SELECT r.\"addr\",\n           COALESCE(SUM(r.\"amount_received\"), 0) - COALESCE(SUM(s.\"amount_sent\"), 0) AS \"balance\"\n    FROM receivedTx AS r\n    LEFT JOIN sentTx AS s\n      ON r.\"addr\" = s.\"addr\"\n    GROUP BY r.\"addr\"\n)\n\nSELECT \n    SUM(\"balance\") AS \"circulating_supply\"\nFROM walletBalances;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 44: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/ETHEREUM_BLOCKCHAIN/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq450",
        "db_id": "ETHEREUM_BLOCKCHAIN",
        "question": "Generate a comprehensive report of all Ethereum addresses active before January 1, 2017, calculating their net balances (adjusted for transaction fees and excluding delegatecall/callcode/staticcall transactions), hourly activity patterns, active days, incoming/outgoing transaction metrics (counts, unique counterparties, average ETH transfers), ERC20 token interactions (in/out counts, unique tokens, counterparties), mining rewards, contract creation frequency, failed transaction counts, and contract bytecode sizes, with all ETH values converted to standard units (divided by 10^18) and excluding addresses with no transaction history.",
        "db_type": "snowflake",
        "db_size": 88,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\ethereum_data_transformation.md",
        "error_info": "Error occurred while executing act() on sample 45: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/ETHEREUM_BLOCKCHAIN/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq035",
        "db_id": "san_francisco",
        "question": "What is the total distance traveled by each bike in the San Francisco Bikeshare program, measured in meters? Use data from bikeshare trips and stations to calculate this.",
        "db_type": "big_query",
        "db_size": 118,
        "query": "SELECT\n  bike_number, \n  AVG(dist_in_m) AS avg_dist_m, \n  SUM(dist_in_m) AS total_dist_m\nFROM (\n  SELECT\n    ST_DISTANCE(\n      ST_GEOGPOINT(start_lon, start_lat),\n      ST_GEOGPOINT(end_lon, end_lat)\n    ) AS dist_in_m,\n    starts.bike_number\n  FROM (\n    SELECT \n      latitude AS start_lat,\n      longitude AS start_lon,\n      bike_number,\n      trip_id\n    FROM `bigquery-public-data.san_francisco.bikeshare_trips` trips\n    LEFT JOIN `bigquery-public-data.san_francisco.bikeshare_stations` stations\n      ON trips.start_station_id = stations.station_id\n  ) starts\n  LEFT JOIN (\n    SELECT \n      latitude AS end_lat,\n      longitude AS end_lon,\n      bike_number,\n      trip_id\n    FROM `bigquery-public-data.san_francisco.bikeshare_trips` trips\n    LEFT JOIN `bigquery-public-data.san_francisco.bikeshare_stations` stations\n      ON trips.end_station_id = stations.station_id\n  ) ends ON ends.trip_id = starts.trip_id\n)\nGROUP BY bike_number\nORDER BY total_dist_m DESC",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 46: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/san_francisco/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq186",
        "db_id": "san_francisco",
        "question": "Please find, for each year-month combination (in the format YYYYMM) derived from the start date of bike share trips in San Francisco, the first trip duration in minutes, the last trip duration in minutes, the highest trip duration in minutes, and the lowest trip duration in minutes, where â€˜firstâ€™ and â€˜lastâ€™ are determined by the chronological order of the trip start date, then group your results by this year-month and sort them by the same year-month key.",
        "db_type": "big_query",
        "db_size": 118,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 47: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/san_francisco/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq014",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Can you help me figure out the revenue for the product category that has the highest number of customers making a purchase in their first non-cancelled and non-returned order?",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq014.csv"
    },
    {
        "instance_id": "sf_bq188",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Among all product categories in the dataset, identify the category with the highest total purchase quantity (based on order_items table), and for that specific category, what is the average time in minutes that users spend on each product page visit? The average time should be calculated as the difference between the timestamp when a user views a product page and the timestamp of the next event within the same session",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq188.csv"
    },
    {
        "instance_id": "sf_bq258",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Generate a monthly report for each product category , where each row corresponds to orders that have a status of 'Complete' and were delivered before the year 2022, grouping by the month and year of delivery. For each category, calculate the total revenue (the sum of sale_price), the total number of completed orders, and compute the month-over-month percentage growth for both revenue and orders by comparing each monthâ€™s totals to the previous monthâ€™s. Then, for the same orders, aggregate and show the total cost (from product costs), total profit (revenue minus total cost), and finally the profit-to-cost ratio for each month.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq258.csv"
    },
    {
        "instance_id": "sf_bq259",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Using data up to the end of 2022 and organized by the month of each user's first purchase, can you provide the percentage of users who made a purchase in each of the first, second, third, and fourth months since their initial purchase, where the \"first month\" refers to the month of their initial purchase?",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq259.csv"
    },
    {
        "instance_id": "sf_bq189",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Based solely on completed orders, calculate the average monthly percentage growth rate in the number of unique orders (counting distinct order IDs) for each product category by comparing each month's count to the previous month within the same category. Identify the product category with the highest average of these monthly order growth rates. Then, for that specific product category, compute the average monthly revenue growth rate by calculating the percentage change in total revenue (sum of sale prices) from month to month and averaging these values over the entire period.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq189.csv"
    },
    {
        "instance_id": "sf_bq260",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "From January 1, 2019, to April 30, 2022, how many users are at the youngest age and how many users are at the oldest age for each gender in the e-commerce platform, counting both youngest and oldest users separately for each gender?",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "WITH filtered_users AS (\n    SELECT \n        \"first_name\", \n        \"last_name\", \n        \"gender\", \n        \"age\",\n        CAST(TO_TIMESTAMP(\"created_at\" / 1000000.0) AS DATE) AS \"created_at\"\n    FROM \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n    WHERE \n        CAST(TO_TIMESTAMP(\"created_at\" / 1000000.0) AS DATE) BETWEEN '2019-01-01' AND '2022-04-30'\n),\nyoungest_ages AS (\n    SELECT \n        \"gender\", \n        MIN(\"age\") AS \"age\"\n    FROM \n        filtered_users\n    GROUP BY \n        \"gender\"\n),\noldest_ages AS (\n    SELECT \n        \"gender\", \n        MAX(\"age\") AS \"age\"\n    FROM \n        filtered_users\n    GROUP BY \n        \"gender\"\n),\nyoungest_oldest AS (\n    SELECT \n        u.\"first_name\", \n        u.\"last_name\", \n        u.\"gender\", \n        u.\"age\", \n        'youngest' AS \"tag\"\n    FROM \n        filtered_users u\n    JOIN \n        youngest_ages y\n    ON \n        u.\"gender\" = y.\"gender\" AND u.\"age\" = y.\"age\"\n    \n    UNION ALL\n    \n    SELECT \n        u.\"first_name\", \n        u.\"last_name\", \n        u.\"gender\", \n        u.\"age\", \n        'oldest' AS \"tag\"\n    FROM \n        filtered_users u\n    JOIN \n        oldest_ages o\n    ON \n        u.\"gender\" = o.\"gender\" AND u.\"age\" = o.\"age\"\n)\nSELECT \n    \"tag\", \n    \"gender\", \n    COUNT(*) AS \"num\"\nFROM \n    youngest_oldest\nGROUP BY \n    \"tag\", \"gender\"\nORDER BY \n    \"tag\", \"gender\";",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq260.csv"
    },
    {
        "instance_id": "sf_bq261",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "For each month prior to January 2024, identify the product that achieved the highest total profit (calculated as the sum of sale_price minus the productâ€™s cost) across all order items, then report the total cost and total profit for that top product per month, including all order items regardless of their status, and present the results chronologically by month.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq261.csv"
    },
    {
        "instance_id": "sf_bq262",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Generate a monthly analysis report for e-commerce sales from June 2019 to December 2019 that includes, for each product category and each month, the total number of orders, total revenue, and total profit, along with their month-over-month growth rates using the data from June 2019 as the basis for calculating growth starting from July 2019. Ensure that all orders are included regardless of their status, and present the results sorted in ascending order by month (formatted as \"2019-07\") and then by product category. Omitting June 2019 from the final output but using it for the growth calculations.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq262.csv"
    },
    {
        "instance_id": "sf_bq190",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Determine the number of users who are the youngest and oldest for each gender (male and female) separately, among those who signed up between January 1, 2019, and April 30, 2022. For each gender, identify the minimum and maximum ages within this date range, and count how many users fall into these respective age groups.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq190.csv"
    },
    {
        "instance_id": "sf_bq263",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Please create a month-by-month report for the year 2023 that focuses on the 'Sleep & Lounge' category, showing for each month the total sales, total cost, number of complete orders, total profit, and the profit-to-cost ratio, ensuring that the order is marked as 'Complete,' the creation date is between January 1, 2023, and December 31, 2023, and the cost data is accurately associated with the corresponding product through the order items. ",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "WITH d AS (\n    SELECT\n        a.\"order_id\", \n        TO_CHAR(TO_TIMESTAMP(a.\"created_at\" / 1000000.0), 'YYYY-MM') AS \"month\",  -- æ ¼å¼åŒ–ä¸ºå¹´æœˆ\n        TO_CHAR(TO_TIMESTAMP(a.\"created_at\" / 1000000.0), 'YYYY') AS \"year\",  -- æ ¼å¼åŒ–ä¸ºå¹´ä»½\n        b.\"product_id\", b.\"sale_price\", c.\"category\", c.\"cost\"\n    FROM \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\" AS a\n    JOIN \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\" AS b\n        ON a.\"order_id\" = b.\"order_id\"\n    JOIN \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"PRODUCTS\" AS c\n        ON b.\"product_id\" = c.\"id\"\n    WHERE \n        a.\"status\" = 'Complete'\n        AND TO_TIMESTAMP(a.\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2023-01-01') AND TO_TIMESTAMP('2023-12-31')\n        AND c.\"category\" = 'Sleep & Lounge'\n),\n\ne AS (\n    SELECT \n        \"month\", \n        \"year\", \n        \"sale_price\", \n        \"category\", \n        \"cost\",\n        SUM(\"sale_price\") OVER (PARTITION BY \"month\", \"category\") AS \"TPV\",\n        SUM(\"cost\") OVER (PARTITION BY \"month\", \"category\") AS \"total_cost\",\n        COUNT(DISTINCT \"order_id\") OVER (PARTITION BY \"month\", \"category\") AS \"TPO\",\n        SUM(\"sale_price\" - \"cost\") OVER (PARTITION BY \"month\", \"category\") AS \"total_profit\",\n        SUM((\"sale_price\" - \"cost\") / \"cost\") OVER (PARTITION BY \"month\", \"category\") AS \"Profit_to_cost_ratio\"\n    FROM \n        d\n)\n\nSELECT DISTINCT \n    \"month\", \n    \"category\", \n    \"TPV\", \n    \"total_cost\", \n    \"TPO\", \n    \"total_profit\", \n    \"Profit_to_cost_ratio\"\nFROM \n    e\nORDER BY \n    \"month\";",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq263.csv"
    },
    {
        "instance_id": "sf_bq264",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Identify the difference in the number of the oldest and youngest users registered between January 1, 2019, and April 30, 2022, from our e-commerce platform data.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "WITH youngest AS (\n    SELECT\n        \"gender\", \n        \"id\", \n        \"first_name\", \n        \"last_name\", \n        \"age\", \n        'youngest' AS \"tag\"\n    FROM \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n    WHERE \n        \"age\" = (SELECT MIN(\"age\") FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\")\n        AND TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2022-04-30')\n    GROUP BY \n        \"gender\", \"id\", \"first_name\", \"last_name\", \"age\"\n    ORDER BY \n        \"gender\"\n),\n\noldest AS (\n    SELECT\n        \"gender\", \n        \"id\", \n        \"first_name\", \n        \"last_name\", \n        \"age\", \n        'oldest' AS \"tag\"\n    FROM \n        \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n    WHERE \n        \"age\" = (SELECT MAX(\"age\") FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\")\n        AND TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2022-04-30')\n    GROUP BY \n        \"gender\", \"id\", \"first_name\", \"last_name\", \"age\"\n    ORDER BY \n        \"gender\"\n),\n\nTEMP_record AS (\n    SELECT * FROM youngest\n    UNION ALL\n    SELECT * FROM oldest\n)\n\nSELECT \n    SUM(CASE WHEN \"age\" = (SELECT MAX(\"age\") FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\") THEN 1 END) - \n    SUM(CASE WHEN \"age\" = (SELECT MIN(\"age\") FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\") THEN 1 END) AS \"diff\"\nFROM \n    TEMP_record;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq264.csv"
    },
    {
        "instance_id": "sf_bq197",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "For each month prior to July 2024, identify the single best-selling product (determined by highest sales volume, with total revenue as a tiebreaker) among all orders with a 'Complete' status and products with non-null brands. Return a report showing the month, product name, brand, category, total sales, rounded total revenue, and order status for these monthly top performers.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq197.csv"
    },
    {
        "instance_id": "sf_bq265",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Can you list the email addresses of the top 10 users who registered in 2019 and made purchases in 2019, ranking them by their highest average order value, where average order value is calculated by multiplying the number of items in each order by the sale price, summing this total across all orders for each user, and then dividing by the total number of orders?",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "WITH\n  main AS (\n    SELECT\n      \"id\" AS \"user_id\",\n      \"email\",\n      \"gender\",\n      \"country\",\n      \"traffic_source\"\n    FROM\n      \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n    WHERE\n      TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2019-12-31')\n  ),\n\n  daate AS (\n    SELECT\n      \"user_id\",\n      \"order_id\",\n      CAST(TO_TIMESTAMP(\"created_at\" / 1000000.0) AS DATE) AS \"order_date\",\n      \"num_of_item\"\n    FROM\n      \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\"\n    WHERE\n      TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2019-12-31')\n  ),\n\n  orders AS (\n    SELECT\n      \"user_id\",\n      \"order_id\",\n      \"product_id\",\n      \"sale_price\",\n      \"status\"\n    FROM\n      \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\"\n    WHERE\n      TO_TIMESTAMP(\"created_at\" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2019-12-31')\n  ),\n\n  nest AS (\n    SELECT\n      o.\"user_id\",\n      o.\"order_id\",\n      o.\"product_id\",\n      d.\"order_date\",\n      d.\"num_of_item\",\n      ROUND(o.\"sale_price\", 2) AS \"sale_price\",\n      ROUND(d.\"num_of_item\" * o.\"sale_price\", 2) AS \"total_sale\"\n    FROM\n      orders o\n    INNER JOIN\n      daate d\n    ON\n      o.\"order_id\" = d.\"order_id\"\n    ORDER BY\n      o.\"user_id\"\n  ),\n\n  type AS (\n    SELECT\n      \"user_id\",\n      MIN(nest.\"order_date\") AS \"cohort_date\",\n      MAX(nest.\"order_date\") AS \"latest_shopping_date\",\n      DATEDIFF(MONTH, MIN(nest.\"order_date\"), MAX(nest.\"order_date\")) AS \"lifespan_months\",\n      ROUND(SUM(\"total_sale\"), 2) AS \"ltv\",\n      COUNT(\"order_id\") AS \"no_of_order\"\n    FROM\n      nest\n    GROUP BY\n      \"user_id\"\n  ),\n\n  kite AS (\n    SELECT\n      m.\"user_id\",\n      m.\"email\",\n      m.\"gender\",\n      m.\"country\",\n      m.\"traffic_source\",\n      EXTRACT(YEAR FROM n.\"cohort_date\") AS \"cohort_year\",\n      n.\"latest_shopping_date\",\n      n.\"lifespan_months\",\n      n.\"ltv\",\n      n.\"no_of_order\",\n      ROUND(n.\"ltv\" / n.\"no_of_order\", 2) AS \"avg_order_value\"\n    FROM\n      main m\n    INNER JOIN\n      type n\n    ON\n      m.\"user_id\" = n.\"user_id\"\n  )\n\nSELECT\n  \"email\"\nFROM\n  kite\nORDER BY\n  \"avg_order_value\" DESC\nLIMIT 10;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq265.csv"
    },
    {
        "instance_id": "sf_bq266",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Please provide the names of the products that had sales in each month of 2020 and had the lowest profit, calculated as the difference between their retail price and cost from the products data. Exclude any months where this data isn't available. Please list the products in chronological order based on the month.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq266.csv"
    },
    {
        "instance_id": "sf_bq333",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Which three browsers have the shortest average session durationâ€”calculated by the difference in seconds between the earliest and latest timestamps for each userâ€™s sessionâ€”while only including browsers that have more than 10 total sessions, and what are their respective average session durations?",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq333.csv"
    },
    {
        "instance_id": "sf_bq361",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "For the user cohort with a first purchase date in January 2020, what proportion of users returned in the subsequent months of 2020?",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq361.csv"
    },
    {
        "instance_id": "sf_bq271",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Please generate a report that, for each month in 2021, provides the number of orders, the number of unique purchasers, and the profit (calculated as the sum of product retail prices minus the sum of product costs), where the orders were placed during 2021 by users who registered in 2021 for inventory items created in 2021, and group the results by the users' country, product department, and product category.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "WITH\norders_x_order_items AS (\n  SELECT orders.*,\n         order_items.\"inventory_item_id\",\n         order_items.\"sale_price\"\n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\" AS orders\n  LEFT JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\" AS order_items\n  ON orders.\"order_id\" = order_items.\"order_id\"\n  WHERE TO_TIMESTAMP_NTZ(orders.\"created_at\" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2021-01-01') AND TO_TIMESTAMP_NTZ('2021-12-31')\n),\n\norders_x_inventory AS (\n  SELECT orders_x_order_items.*,\n         inventory_items.\"product_category\",\n         inventory_items.\"product_department\",\n         inventory_items.\"product_retail_price\",\n         inventory_items.\"product_distribution_center_id\",\n         inventory_items.\"cost\",\n         distribution_centers.\"name\"\n  FROM orders_x_order_items\n  LEFT JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"INVENTORY_ITEMS\" AS inventory_items\n  ON orders_x_order_items.\"inventory_item_id\" = inventory_items.\"id\"\n  LEFT JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"DISTRIBUTION_CENTERS\" AS distribution_centers\n  ON inventory_items.\"product_distribution_center_id\" = distribution_centers.\"id\"\n  WHERE TO_TIMESTAMP_NTZ(inventory_items.\"created_at\" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2021-01-01') AND TO_TIMESTAMP_NTZ('2021-12-31')\n),\n\norders_x_users AS (\n  SELECT orders_x_inventory.*,\n         users.\"country\" AS \"users_country\"\n  FROM orders_x_inventory\n  LEFT JOIN \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\" AS users\n  ON orders_x_inventory.\"user_id\" = users.\"id\"\n  WHERE TO_TIMESTAMP_NTZ(users.\"created_at\" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2021-01-01') AND TO_TIMESTAMP_NTZ('2021-12-31')\n)\n\nSELECT \n  DATE_TRUNC('MONTH', TO_DATE(TO_TIMESTAMP_NTZ(orders_x_users.\"created_at\" / 1000000))) AS \"reporting_month\",\n  orders_x_users.\"users_country\",\n  orders_x_users.\"product_department\",\n  orders_x_users.\"product_category\",\n  COUNT(DISTINCT orders_x_users.\"order_id\") AS \"n_order\",\n  COUNT(DISTINCT orders_x_users.\"user_id\") AS \"n_purchasers\",\n  SUM(orders_x_users.\"product_retail_price\") - SUM(orders_x_users.\"cost\") AS \"profit\"\nFROM orders_x_users\nGROUP BY 1, 2, 3, 4\nORDER BY \"reporting_month\";",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq271.csv"
    },
    {
        "instance_id": "sf_bq272",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Please provide the names of the top three most profitable products for each month from January 2019 through August 2022, excluding any products associated with orders that were canceled or returned. For each product in each month, the profit should be calculated as the sum of the sale prices of all order items minus the sum of the costs of those sold items in that month.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq272.csv"
    },
    {
        "instance_id": "sf_bq273",
        "db_id": "THELOOK_ECOMMERCE",
        "question": "Can you list the top 5 months from August 2022 to November 2023 where the profit from Facebook-sourced completed orders showed the largest month-over-month increase? Calculate profit as sales minus costs, group by delivery month, and include only orders created between August 2022 and November 2023. Compare each month's profit to its previous month to find the largest increases.",
        "db_type": "snowflake",
        "db_size": 73,
        "query": "WITH \norders AS (\n  SELECT\n    \"order_id\", \n    \"user_id\", \n    \"created_at\",\n    DATE_TRUNC('MONTH', TO_TIMESTAMP_NTZ(\"delivered_at\" / 1000000)) AS \"delivery_month\",  -- Converting to timestamp\n    \"status\" \n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDERS\"\n),\n\norder_items AS (\n  SELECT \n    \"order_id\", \n    \"product_id\", \n    \"sale_price\" \n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"ORDER_ITEMS\"\n),\n\nproducts AS (\n  SELECT \n    \"id\", \n    \"cost\"\n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"PRODUCTS\"\n),\n\nusers AS (\n  SELECT\n    \"id\", \n    \"traffic_source\" \n  FROM \"THELOOK_ECOMMERCE\".\"THELOOK_ECOMMERCE\".\"USERS\"\n),\n\nfilter_join AS (\n  SELECT \n    orders.\"order_id\",\n    orders.\"user_id\",\n    order_items.\"product_id\",\n    orders.\"delivery_month\",\n    orders.\"status\",\n    order_items.\"sale_price\",\n    products.\"cost\",\n    users.\"traffic_source\"\n  FROM orders\n  JOIN order_items ON orders.\"order_id\" = order_items.\"order_id\"\n  JOIN products ON order_items.\"product_id\" = products.\"id\"\n  JOIN users ON orders.\"user_id\" = users.\"id\"\n  WHERE orders.\"status\" = 'Complete' \n    AND users.\"traffic_source\" = 'Facebook'\n    AND TO_TIMESTAMP_NTZ(orders.\"created_at\" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2022-07-01') AND TO_TIMESTAMP_NTZ('2023-11-30')  -- Include July for calculation\n),\n\nmonthly_sales AS (\n  SELECT \n    \"delivery_month\",\n    \"traffic_source\",\n    SUM(\"sale_price\") AS \"total_revenue\",\n    SUM(\"sale_price\") - SUM(\"cost\") AS \"total_profit\",\n    COUNT(DISTINCT \"product_id\") AS \"product_quantity\",\n    COUNT(DISTINCT \"order_id\") AS \"orders_quantity\",\n    COUNT(DISTINCT \"user_id\") AS \"users_quantity\"\n  FROM filter_join\n  GROUP BY \"delivery_month\", \"traffic_source\"\n)\n\n-- Filter to show only 8th month and onwards, but calculate using July\nSELECT \n  current_month.\"delivery_month\",\n  COALESCE(\n    current_month.\"total_profit\" - previous_month.\"total_profit\", \n    0  -- If there is no previous month (i.e. for 8æœˆ), return 0\n  ) AS \"profit_vs_prior_month\"\nFROM monthly_sales AS current_month\nLEFT JOIN monthly_sales AS previous_month\n  ON current_month.\"traffic_source\" = previous_month.\"traffic_source\"\n  AND current_month.\"delivery_month\" = DATEADD(MONTH, -1, previous_month.\"delivery_month\")  -- Correctly join to previous month\nWHERE current_month.\"delivery_month\" >= '2022-08-01'  -- Only show August and later data, but use July for calculation\nORDER BY \"profit_vs_prior_month\" DESC\nLIMIT 5;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq273.csv"
    },
    {
        "instance_id": "sf_bq020",
        "db_id": "GENOMICS_CANNABIS",
        "question": "What is the name of the reference sequence with the highest variant density in the given cannabis genome dataset?",
        "db_type": "snowflake",
        "db_size": 159,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 67: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GENOMICS_CANNABIS/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq107",
        "db_id": "GENOMICS_CANNABIS",
        "question": "What is the variant density of the cannabis reference with the longest reference length? Pay attention that a variant is present if there is at least one variant call with a genotype greater than 0.",
        "db_type": "snowflake",
        "db_size": 159,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 68: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GENOMICS_CANNABIS/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq025",
        "db_id": "census_bureau_international",
        "question": "Provide a list of the top 10 countries for the year 2020, ordered by the highest percentage of their population under 20 years old. For each country, include the total population under 20 years old, the total midyear population, and the percentage of the population that is under 20 years old.",
        "db_type": "big_query",
        "db_size": 165,
        "query": "SELECT\n  age.country_name,\n  SUM(age.population) AS under_25,\n  pop.midyear_population AS total,\n  ROUND((SUM(age.population) / pop.midyear_population) * 100,2) AS pct_under_25\nFROM (\n  SELECT\n    country_name,\n    population,\n    country_code\n  FROM\n    `bigquery-public-data.census_bureau_international.midyear_population_agespecific`\n  WHERE\n    year =2020\n    AND age < 20) age\nINNER JOIN (\n  SELECT\n    midyear_population,\n    country_code\n  FROM\n    `bigquery-public-data.census_bureau_international.midyear_population`\n  WHERE\n    year = 2020) pop\nON\n  age.country_code = pop.country_code\nGROUP BY\n  1,\n  3\nORDER BY\n  4 DESC\n/* Remove limit for visualization */\nLIMIT\n  10",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 69: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/census_bureau_international/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq115",
        "db_id": "census_bureau_international",
        "question": "Which country has the highest percentage of population under the age of 25 in 2017?",
        "db_type": "big_query",
        "db_size": 165,
        "query": "SELECT\ncountry_name\nFROM\n(SELECT\n  age.country_name,\n  SUM(age.population) AS under_25,\n  pop.midyear_population AS total,\n  ROUND((SUM(age.population) / pop.midyear_population) * 100,2) AS pct_under_25\nFROM (\n  SELECT\n    country_name,\n    population,\n    country_code\n  FROM\n    `bigquery-public-data.census_bureau_international.midyear_population_agespecific`\n  WHERE\n    year =2017\n    AND age < 25) age\nINNER JOIN (\n  SELECT\n    midyear_population,\n    country_code\n  FROM\n    `bigquery-public-data.census_bureau_international.midyear_population`\n  WHERE\n    year = 2017) pop\nON\n  age.country_code = pop.country_code\nGROUP BY\n  1,\n  3\nORDER BY\n  4 DESC\n)\nLIMIT\n1",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 70: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/census_bureau_international/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq130",
        "db_id": "covid19_nyt",
        "question": "Analyze daily new COVID-19 case counts from March to May 2020, identifying the top five states by daily increases. Please compile a ranking based on how often each state appears in these daily top fives. Then, examine the state that ranks fourth overall and identify its top five counties based on their frequency of appearing in the daily top five new case counts.",
        "db_type": "big_query",
        "db_size": 29,
        "query": "WITH StateCases AS (\n    SELECT\n        b.state_name,\n        b.date,\n        b.confirmed_cases - a.confirmed_cases AS daily_new_cases\n    FROM \n        (SELECT\n            state_name,\n            state_fips_code,\n            confirmed_cases,\n            DATE_ADD(date, INTERVAL 1 DAY) AS date_shift\n        FROM\n            `bigquery-public-data.covid19_nyt.us_states`\n        WHERE\n            date >= '2020-02-29' AND date <= '2020-05-30'\n        ) a\n    JOIN\n        `bigquery-public-data.covid19_nyt.us_states` b \n        ON a.state_fips_code = b.state_fips_code AND a.date_shift = b.date\n    WHERE\n        b.date >= '2020-03-01' AND b.date <= '2020-05-31'\n),\nRankedStatesPerDay AS (\n    SELECT\n        state_name,\n        date,\n        daily_new_cases,\n        RANK() OVER (PARTITION BY date ORDER BY daily_new_cases DESC) as rank\n    FROM\n        StateCases\n),\nTopStates AS (\n    SELECT\n        state_name,\n        COUNT(*) AS appearance_count\n    FROM\n        RankedStatesPerDay\n    WHERE\n        rank <= 5\n    GROUP BY\n        state_name\n    ORDER BY\n        appearance_count DESC\n),\nFourthState AS (\n    SELECT\n        state_name\n    FROM\n        TopStates\n    LIMIT 1\n    OFFSET 3\n),\nCountyCases AS (\n    SELECT\n        b.county,\n        b.date,\n        b.confirmed_cases - a.confirmed_cases AS daily_new_cases\n    FROM \n        (SELECT\n            county,\n            county_fips_code,\n            confirmed_cases,\n            DATE_ADD(date, INTERVAL 1 DAY) AS date_shift\n        FROM\n            `bigquery-public-data.covid19_nyt.us_counties`\n        WHERE\n            date >= '2020-02-29' AND date <= '2020-05-30'\n        ) a\n    JOIN\n        `bigquery-public-data.covid19_nyt.us_counties` b \n        ON a.county_fips_code = b.county_fips_code AND a.date_shift = b.date\n    WHERE\n        b.date >= '2020-03-01' AND b.date <= '2020-05-31'\n        AND b.state_name = (SELECT state_name FROM FourthState)\n),\nRankedCountiesPerDay AS (\n    SELECT\n        county,\n        date,\n        daily_new_cases,\n        RANK() OVER (PARTITION BY date ORDER BY daily_new_cases DESC) as rank\n    FROM\n        CountyCases\n),\nTopCounties AS (\n    SELECT\n        county,\n        COUNT(*) AS appearance_count\n    FROM\n        RankedCountiesPerDay\n    WHERE\n        rank <= 5\n    GROUP BY\n        county\n    ORDER BY\n        appearance_count DESC\n    LIMIT 5\n)\nSELECT\n    county\nFROM\n    TopCounties;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq130.csv"
    },
    {
        "instance_id": "bq137",
        "db_id": "census_bureau_usa",
        "question": "Please find all zip code areas located within 10 kilometers of the coordinates (-122.3321, 47.6062) by joining the 2010 census population data (summing only male and female populations with no age constraints) and the zip code area information, and return each areaâ€™s polygon, land and water area in meters, latitude and longitude, state code, state name, city, county, and total population.",
        "db_type": "big_query",
        "db_size": 132,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\functions_st_dwithin.md",
        "error_info": "Error occurred while executing act() on sample 72: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/census_bureau_usa/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq060",
        "db_id": "census_bureau_international",
        "question": "Which top 3 countries had the highest net migration in 2017 among those with an area greater than 500 square kilometers? And what are their migration rates?",
        "db_type": "big_query",
        "db_size": 165,
        "query": "WITH results AS (\n    SELECT\n        growth.country_name,\n        growth.net_migration,\n        CAST(area.country_area as INT64) as country_area\n    FROM (\n        SELECT\n            country_name,\n            net_migration,\n            country_code\n        FROM\n            `bigquery-public-data.census_bureau_international.birth_death_growth_rates`\n        WHERE\n            year = 2017\n    ) growth\n    INNER JOIN (\n        SELECT\n            country_area,\n            country_code\n        FROM\n            `bigquery-public-data.census_bureau_international.country_names_area`\n        WHERE\n            country_area > 500\n    ) area\n    ON\n        growth.country_code = area.country_code\n    ORDER BY\n        net_migration DESC\n    LIMIT 3\n)\nSELECT country_name, net_migration\nFROM results;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 73: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/census_bureau_international/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq084",
        "db_id": "GOOG_BLOCKCHAIN",
        "question": "For each month in the year 2023, how many total transactions occurred (counting all transaction records without removing duplicates of transaction hashes), and how many transactions per second were processed each month, where the transactions-per-second value is calculated by dividing the monthly total count by the exact number of seconds in that month, including the correct leap-year logic if applicable based on the extracted year from the transaction timestamp? Show the monthly transaction count, the computed transactions per second, the year, and the month, and present the rows in descending order of the monthly transaction count.",
        "db_type": "snowflake",
        "db_size": 108,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 74: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GOOG_BLOCKCHAIN/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq058",
        "db_id": "GOOG_BLOCKCHAIN",
        "question": "Retrieve all finalized deposits into Optimism at block 29815485 using the Optimism Standard Bridge, including transaction hash, an Etherscan link (the complete URL), L1 and L2 token addresses, sender and receiver addresses (with leading zeroes stripped), and the deposited amount (converted from hex to decimal). Ensure data is properly formatted and parsed according to Optimism's address and token standards, and remove the prefix '0x' except transaction hash. Note that, the keccak-256 hash of the Ethereum event signature for DepositFinalized is \"0x3303facd24627943a92e9dc87cfbb34b15c49b726eec3ad3487c16be9ab8efe8\".",
        "db_type": "snowflake",
        "db_size": 108,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\optimism_standard_bridge_contract.md",
        "error_info": "Error occurred while executing act() on sample 75: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GOOG_BLOCKCHAIN/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq416",
        "db_id": "GOOG_BLOCKCHAIN",
        "question": "Could you retrieve the top three largest USDT transfers on the TRON blockchain by listing the block numbers, source addresses, destination addresses (in TronLink format), and transfer amounts, using the USDT contract address '0xa614f803b6fd780986a42c78ec9c7f77e6ded13c' and the transfer event signature '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef', dividing the raw transfer value by 1,000,000 to convert it into the final USDT amount, and then ordering the results by the largest transferred amounts first?",
        "db_type": "snowflake",
        "db_size": 108,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\blockchain_data_transformations.md",
        "error_info": "Error occurred while executing act() on sample 76: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GOOG_BLOCKCHAIN/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq226",
        "db_id": "GOOG_BLOCKCHAIN",
        "question": "Which sender address, represented as a complete URL on https://cronoscan.com, has been used most frequently on the Cronos blockchain in transactions to non-null 'to_address' fields, within blocks larger than 4096 bytes, since January 1, 2023?",
        "db_type": "snowflake",
        "db_size": 108,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 77: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/GOOG_BLOCKCHAIN/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq016",
        "db_id": "DEPS_DEV_V1",
        "question": "Considering only the highest release versions of NPM packages, which dependency (package and its version) appears most frequently among the dependencies of these packages?",
        "db_type": "snowflake",
        "db_size": 78,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq016.csv"
    },
    {
        "instance_id": "sf_bq062",
        "db_id": "DEPS_DEV_V1",
        "question": "What is the most frequently used license by packages in each system?",
        "db_type": "snowflake",
        "db_size": 78,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq062.csv"
    },
    {
        "instance_id": "sf_bq063",
        "db_id": "DEPS_DEV_V1",
        "question": "Find the GitHub URL (with link label 'SOURCE_REPO') of the latest released version of the NPM package that has the highest number of dependencies in its latest released version, excluding packages whose names contain the character '@' and only considering URLs where the link label is 'SOURCE_REPO' and the URL contains 'github.com'.",
        "db_type": "snowflake",
        "db_size": 78,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq063.csv"
    },
    {
        "instance_id": "sf_bq028",
        "db_id": "DEPS_DEV_V1",
        "question": "Considering only the latest release versions of NPM package, which packages are the top 8 most popular based on the Github star number, as well as their versions?",
        "db_type": "snowflake",
        "db_size": 78,
        "query": "WITH HighestReleases AS (\n    SELECT\n        HR.\"Name\",\n        HR.\"Version\"\n    FROM (\n        SELECT\n            \"Name\",\n            \"Version\",\n            ROW_NUMBER() OVER (\n                PARTITION BY \"Name\"\n                ORDER BY \n                    TO_NUMBER(PARSE_JSON(\"VersionInfo\"):\"Ordinal\") DESC\n            ) AS RowNumber\n        FROM\n            DEPS_DEV_V1.DEPS_DEV_V1.PACKAGEVERSIONS\n        WHERE\n            \"System\" = 'NPM'\n            AND TO_BOOLEAN(PARSE_JSON(\"VersionInfo\"):\"IsRelease\") = TRUE\n    ) AS HR\n    WHERE HR.RowNumber = 1\n),\nPVP AS (\n    SELECT\n        PVP.\"Name\", \n        PVP.\"Version\", \n        PVP.\"ProjectType\", \n        PVP.\"ProjectName\"\n    FROM\n        DEPS_DEV_V1.DEPS_DEV_V1.PACKAGEVERSIONTOPROJECT AS PVP\n    JOIN\n        HighestReleases AS HR\n    ON\n        PVP.\"Name\" = HR.\"Name\"\n        AND PVP.\"Version\" = HR.\"Version\"\n    WHERE\n        PVP.\"System\" = 'NPM'\n        AND PVP.\"ProjectType\" = 'GITHUB'\n)\nSELECT\n    PVP.\"Name\", \n    PVP.\"Version\"\nFROM\n    PVP\nJOIN\n    DEPS_DEV_V1.DEPS_DEV_V1.PROJECTS AS P\nON\n    PVP.\"ProjectType\" = P.\"Type\" \n    AND PVP.\"ProjectName\" = P.\"Name\"\nORDER BY \n    P.\"StarsCount\" DESC\nLIMIT 8;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq028.csv"
    },
    {
        "instance_id": "bq022",
        "db_id": "chicago",
        "question": "Calculate the minimum and maximum trip duration in minutes (rounded to the nearest whole number), total number of trips, and average fare for each of six equal quantile groups based on trip duration, considering only trips between 0 and 60 minutes.",
        "db_type": "big_query",
        "db_size": 45,
        "query": "SELECT\n  ROUND(MIN(trip_seconds) / 60, 0) AS min_minutes,\n  ROUND(MAX(trip_seconds) / 60, 0) AS max_minutes,\n  COUNT(*) AS total_trips,\n  AVG(fare) AS average_fare\nFROM (\n  SELECT\n    trip_seconds,\n    NTILE(6) OVER (ORDER BY trip_seconds) AS quantile,\n    fare\n  FROM\n    `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n  WHERE\n    trip_seconds BETWEEN 0 AND 3600\n)\nGROUP BY\n  quantile\nORDER BY\n  min_minutes, max_minutes;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq022.csv"
    },
    {
        "instance_id": "bq362",
        "db_id": "chicago",
        "question": "Which three companies had the largest increase in trip numbers between two consecutive months in 2018?",
        "db_type": "big_query",
        "db_size": 45,
        "query": "select company from\n            (select *,\n            row_number() over(partition by company order by month_o_month_calc desc) as rownum\n            from\n            (select *,\n            num_trips - lag(num_trips) over(partition by company order by month) as month_o_month_calc\n                from\n                (SELECT \n                company,\n                format_date(\"%Y-%m\", date_sub((cast(trip_start_timestamp as date)), interval 1 month)) as prev_month,\n                format_date(\"%Y-%m\", cast(trip_start_timestamp as date)) AS month,\n                count(1) AS num_trips\n                from `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n                where extract(YEAR from trip_start_timestamp) = 2018\n                group by company, month, prev_month\n                order by company,month)\n            order by company, month_o_month_calc desc)\n            ) \n        where rownum = 1\n        order by month_o_month_calc desc, company \n        limit 3",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq362.csv"
    },
    {
        "instance_id": "bq363",
        "db_id": "chicago",
        "question": "Calculate the total number of trips and average fare (formatted to two decimal places) for ten equal-sized quantile groups. Each quantile group should contain approximately the same number of trips based on their rounded trip duration in minutes (between 1-50 minutes). Display each group's time range formatted as \"XXm to XXm\" (where the numbers are zero-padded to two digits), the total trips count, and the average fare. The time ranges should represent the minimum and maximum duration values within each quantile. Sort the results chronologically by time range. Use NTILE(10) to create the quantiles from the ordered trip durations.",
        "db_type": "big_query",
        "db_size": 45,
        "query": "SELECT\n  FORMAT('%02.0fm to %02.0fm', min_minutes, max_minutes) AS minutes_range,\n  SUM(trips) AS total_trips,\n  FORMAT('%3.2f', SUM(total_fare) / SUM(trips)) AS average_fare\nFROM (\n  SELECT\n    MIN(duration_in_minutes) OVER (quantiles) AS min_minutes,\n    MAX(duration_in_minutes) OVER (quantiles) AS max_minutes,\n    SUM(trips) AS trips,\n    SUM(total_fare) AS total_fare\n  FROM (\n    SELECT\n      ROUND(trip_seconds / 60) AS duration_in_minutes,\n      NTILE(10) OVER (ORDER BY trip_seconds / 60) AS quantile,\n      COUNT(1) AS trips,\n      SUM(fare) AS total_fare\n    FROM\n      `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n    WHERE\n      ROUND(trip_seconds / 60) BETWEEN 1 AND 50\n    GROUP BY\n      trip_seconds,\n      duration_in_minutes )\n  GROUP BY\n    duration_in_minutes,\n    quantile\n  WINDOW quantiles AS (PARTITION BY quantile)\n  )\nGROUP BY\n  minutes_range\nORDER BY\n  Minutes_range",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq363.csv"
    },
    {
        "instance_id": "bq076",
        "db_id": "chicago",
        "question": "What is the highest number of motor vehicle theft incidents that occurred in any single month during 2016?",
        "db_type": "big_query",
        "db_size": 45,
        "query": "SELECT\n  incidents AS highest_monthly_thefts\nFROM (\n  SELECT\n    year,\n    EXTRACT(MONTH FROM date) AS month,\n    COUNT(1) AS incidents,\n    RANK() OVER (PARTITION BY year ORDER BY COUNT(1) DESC) AS ranking\n  FROM\n    `bigquery-public-data.chicago_crime.crime`\n  WHERE\n    primary_type = 'MOTOR VEHICLE THEFT'\n    AND year = 2016\n  GROUP BY\n    year,\n    month\n)\nWHERE\n  ranking = 1\nORDER BY\n  year DESC\nLIMIT 1;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq076.csv"
    },
    {
        "instance_id": "bq077",
        "db_id": "chicago",
        "question": "For each year from 2010 to 2016, what is the highest number of motor thefts in one month?",
        "db_type": "big_query",
        "db_size": 45,
        "query": "SELECT\n  year,\n  incidents\nFROM (\n  SELECT\n    year,\n    EXTRACT(MONTH\n    FROM\n      date) AS month,\n    COUNT(1) AS incidents,\n    RANK() OVER (PARTITION BY year ORDER BY COUNT(1) DESC) AS ranking\n  FROM\n    `bigquery-public-data.chicago_crime.crime`\n  WHERE\n    primary_type = 'MOTOR VEHICLE THEFT'\n    AND year BETWEEN 2010 AND 2016\n  GROUP BY\n    year,\n    month )\nWHERE\n  ranking = 1\nORDER BY\n  year ASC",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq077.csv"
    },
    {
        "instance_id": "bq090",
        "db_id": "CYMBAL_INVESTMENTS",
        "question": "How much higher the average intrinsic value is for trades using the feeling-lucky strategy compared to those using the momentum strategy under long-side trades?",
        "db_type": "big_query",
        "db_size": 14,
        "query": "WITH MomentumTrades AS (\n  SELECT\n    StrikePrice - LastPx AS priceDifference\n  FROM\n    `bigquery-public-data.cymbal_investments.trade_capture_report`\n  WHERE\n    SUBSTR(TargetCompID, 0, 4) = 'MOMO'\n    AND (SELECT Side FROM UNNEST(Sides)) = 'LONG'\n),\n\nFeelingLuckyTrades AS (\n  SELECT\n    StrikePrice - LastPx AS priceDifference\n  FROM\n    `bigquery-public-data.cymbal_investments.trade_capture_report`\n  WHERE\n    SUBSTR(TargetCompID, 0, 4) = 'LUCK'\n    AND (SELECT Side FROM UNNEST(Sides)) = 'LONG'\n)\n\nSELECT\n  AVG(FeelingLuckyTrades.priceDifference) - AVG(MomentumTrades.priceDifference) AS averageDifference \nFROM\n  MomentumTrades,\n  FeelingLuckyTrades",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq090.csv"
    },
    {
        "instance_id": "bq442",
        "db_id": "CYMBAL_INVESTMENTS",
        "question": "Please collect the information of the top 6 trade report with the highest closing prices. Refer to the document for all the information I want.",
        "db_type": "big_query",
        "db_size": 14,
        "query": "SELECT\n  OrderID AS tradeID,\n  MaturityDate AS tradeTimestamp,\n  (\n    CASE SUBSTR(TargetCompID, 0, 4)\n      WHEN 'MOMO' THEN 'Momentum'\n      WHEN 'LUCK' THEN 'Feeling Lucky'\n      WHEN 'PRED' THEN 'Prediction'\n  END\n    ) AS algorithm,\n  Symbol AS symbol,\n  LastPx AS openPrice,\n  StrikePrice AS closePrice,\n  (\n  SELECT\n    Side\n  FROM\n    UNNEST(Sides)\n  ) AS tradeDirection,\n  (CASE (\n    SELECT\n      Side\n    FROM\n      UNNEST(Sides))\n      WHEN 'SHORT' THEN -1\n      WHEN 'LONG' THEN 1\n  END\n    ) AS tradeMultiplier\nFROM\n  `bigquery-public-data.cymbal_investments.trade_capture_report`cv\nORDER BY closePrice DESC\nLIMIT 6",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\Trade_Capture_Report_Data_List.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq442.csv"
    },
    {
        "instance_id": "bq096",
        "db_id": "gbif",
        "question": "Determine which year had the earliest date after January on which more than 10 sightings of Sterna paradisaea were recorded north of 40 degrees latitude. For each year, find the first day after January with over 10 sightings of this species in that region, and identify the year whose earliest such date is the earliest among all years.",
        "db_type": "big_query",
        "db_size": 50,
        "query": "WITH tenplus AS (\n  SELECT \n    year, \n    EXTRACT(DAYOFYEAR FROM DATE(eventdate)) AS dayofyear, \n    COUNT(*) AS count\n  FROM \n    bigquery-public-data.gbif.occurrences\n  WHERE \n    eventdate IS NOT NULL \n    AND species = 'Sterna paradisaea' \n    AND decimallatitude > 40.0 \n    AND month > 1\n  GROUP BY \n    year, \n    eventdate\n  HAVING \n    COUNT(*) > 10\n)\n\nSELECT \n  year AS year\nFROM \n  tenplus\nGROUP BY \n  year\nORDER BY \n  MIN(dayofyear)\nLIMIT 1;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq096.csv"
    },
    {
        "instance_id": "bq278",
        "db_id": "sunroof_solar",
        "question": "Please provide a detailed comparison of the solar potential for each state, distinguishing between postal code and census tract levels. For each state, include the total number of buildings available for solar installations, the average percentage of Google Maps area covered by Project Sunroof, the average percentage of that coverage which is suitable for solar, the total potential panel count, the total kilowatt capacity, the energy generation potential, the carbon dioxide offset, the current number of buildings with solar panels, and the gap in potential installations calculated by adjusting the total qualified buildings with the coverage and suitability percentages and subtracting the current installations.",
        "db_type": "big_query",
        "db_size": 64,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq278.csv"
    },
    {
        "instance_id": "sf_bq104",
        "db_id": "GOOGLE_TRENDS",
        "question": "Based on the most recent refresh date, identify the top-ranked rising search term for the week that is exactly one year prior to the latest available week in the dataset.",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "WITH LatestWeek AS (\n    SELECT\n        DATEADD(WEEK, -52, MAX(\"week\")) AS \"last_year_week\"\n    FROM\n        GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS\n),\nLatestRefreshDate AS (\n    SELECT\n        MAX(\"refresh_date\") AS \"latest_refresh_date\"\n    FROM\n        GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS\n),\nRankedTerms AS (\n    SELECT\n        \"term\",\n        \"week\",\n        CASE WHEN \"score\" IS NULL THEN NULL ELSE \"dma_name\" END AS \"dma_name\",\n        \"rank\",\n        \"score\",\n        ROW_NUMBER() OVER (\n            PARTITION BY \"term\", \"week\"\n            ORDER BY \"score\" DESC\n        ) AS rn\n    FROM\n        GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS\n    WHERE\n        \"week\" = (SELECT \"last_year_week\" FROM LatestWeek)\n        AND \"refresh_date\" = (SELECT \"latest_refresh_date\" FROM LatestRefreshDate)\n)\n\nSELECT\n    \"term\"\nFROM\n    RankedTerms\nWHERE\n    rn = 1\nORDER BY\n    \"rank\"\nLIMIT 1;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq104.csv"
    },
    {
        "instance_id": "sf_bq411",
        "db_id": "GOOGLE_TRENDS",
        "question": "Please retrieve the top three Google Trends search terms (ranks 1, 2, and 3) from top_terms for each weekday (Monday through Friday) between September 1, 2024, and September 14, 2024, grouped by the refresh_date column and ordered in descending order of refresh_date.",
        "db_type": "snowflake",
        "db_size": 34,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq411.csv"
    },
    {
        "instance_id": "bq116",
        "db_id": "sec_quarterly_financials",
        "question": "Which U.S. state reported the highest total annual revenue in billions of dollars during fiscal year 2016, considering companies that provided four quarters of data and reported measure tags in ('Revenues','SalesRevenueNet','SalesRevenueGoodsNet'), excluding any entries where the state field (stprba) is null or empty?",
        "db_type": "big_query",
        "db_size": 147,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 93: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/sec_quarterly_financials/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq126",
        "db_id": "the_met",
        "question": "What are the titles, artist names, mediums, and original image URLs of objects with 'Photograph' in their names from the 'Photographs' department, created not by an unknown artist, with an object end date of 1839 or earlier?",
        "db_type": "big_query",
        "db_size": 61,
        "query": "SELECT\n  o.artist_display_name,\n  o.title,\n  o.object_end_date,\n  o.medium,\n  i.original_image_url\nFROM (\n  SELECT\n    object_id,\n    title,\n    artist_display_name,\n    object_end_date,\n    medium\n  FROM\n    `bigquery-public-data.the_met.objects`\n  WHERE\n    department = \"Photographs\"\n    AND object_name LIKE \"%Photograph%\"\n    AND artist_display_name != \"Unknown\"\n    AND object_end_date <= 1839\n) o\nINNER JOIN (\n  SELECT\n    original_image_url,\n    object_id\n  FROM\n    `bigquery-public-data.the_met.images`\n) i\nON\n  o.object_id = i.object_id\nORDER BY\n  o.object_end_date\n;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq126.csv"
    },
    {
        "instance_id": "bq366",
        "db_id": "the_met",
        "question": "What are the top three most frequently associated labels with artworks from each historical period in The Met's collection, only considering labels linked to 500 or more artworks? Provide me with the period, label, and the associated count.",
        "db_type": "big_query",
        "db_size": 61,
        "query": "SELECT period, description, c FROM (\n  SELECT \na.period, \nb.description, \ncount(*) c, \nrow_number() over (partition by period order by count(*) desc) seqnum \n  FROM `bigquery-public-data.the_met.objects` a\n  JOIN (\n    SELECT \n        label.description as description, \n        object_id \n    FROM `bigquery-public-data.the_met.vision_api_data`, UNNEST(labelAnnotations) label\n  ) b\n  ON a.object_id = b.object_id\n  WHERE a.period is not null\n  group by 1,2\n)\nWHERE seqnum <= 3\nAND c >= 500 # only include labels that have 50 or more pieces associated with it\nORDER BY period, c desc;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq366.csv"
    },
    {
        "instance_id": "bq414",
        "db_id": "the_met",
        "question": "Retrieve the object id, title, and the formatted metadata date (as a string in 'YYYY-MM-DD' format) for objects in the \"The Libraries\" department where the cropConfidence is greater than 0.5, the object's title contains the word \"book\".",
        "db_type": "big_query",
        "db_size": 61,
        "query": "SELECT \n  a.object_id,\n  a.title,\n  FORMAT_TIMESTAMP('%Y-%m-%d', a.metadata_date) AS formatted_metadata_date\nFROM `bigquery-public-data.the_met.objects` a\nJOIN (\n  SELECT object_id,\n         cropHints.confidence AS cropConfidence\n  FROM `bigquery-public-data.the_met.vision_api_data`, \n       UNNEST(cropHintsAnnotation.cropHints) cropHints\n) b\nON a.object_id = b.object_id\nWHERE a.department = \"The Libraries\"\nAND b.cropConfidence > 0.5\nAND a.title LIKE \"%book%\"",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq414.csv"
    },
    {
        "instance_id": "sf_bq458",
        "db_id": "WORD_VECTORS_US",
        "question": "Tokenize the body text of each article into words, excluding stop words, and obtain the corresponding word vectors for these words from the glove vector. For each word, weight its word vector by dividing each component by the 0.4th power of the word's frequency from the word frequencies. Then, for each article, aggregate these weighted word vectors by summing their components to form an article vector. Normalize each article vector to unit length by dividing by its magnitude. Finally, retrieve the ID, date, title, and the normalized article vector for each article.",
        "db_type": "snowflake",
        "db_size": 19,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\tokenize_func.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq458.csv"
    },
    {
        "instance_id": "sf_bq459",
        "db_id": "WORD_VECTORS_US",
        "question": "Please find the top 10 most relevant articles by only processing each articleâ€™s 'body' field, where each body is tokenized with no stopwords, each remaining token is turned into a GloVe-based word vector and weighted by dividing each dimension by the 0.4th power of its word frequency, then these weighted vectors are summed and normalized to get a unit vector for each article. Perform the same weighting and normalization on the query phrase 'Epigenetics and cerebral organoids: promising directions in autism spectrum disorders' and compute the cosine similarity between the query vector and each article vector. Finally, return the id, date, title, and the cosine similarity score for the top 10 articles with the highest similarity.",
        "db_type": "snowflake",
        "db_size": 19,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\tokenize_func.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq459.csv"
    },
    {
        "instance_id": "sf_bq460",
        "db_id": "WORD_VECTORS_US",
        "question": "Please process the articles from the 'nature' dataset by first tokenizing the body text into words and removing stopwords. For each remaining word, retrieve its word vector from the glove_vectors table and its frequency from the word_frequencies table, then divide each word vector by the 0.4th power of the word's frequency to weight it. Sum the weighted vectors to obtain an aggregate vector for each article, normalize this aggregate vector to unit length, and then compute the cosine similarity scores between these normalized vectors. Finally, return the IDs, dates, titles, and cosine similarity scores of the top 10 articles most similar to the article with the ID '8a78ef2d-d5f7-4d2d-9b47-5adb25cbd373'.",
        "db_type": "snowflake",
        "db_size": 19,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\tokenize_func.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq460.csv"
    },
    {
        "instance_id": "bq204",
        "db_id": "eclipse_megamovie",
        "question": "Find the user with the highest total clicks across all records from all available photo collections.",
        "db_type": "big_query",
        "db_size": 129,
        "query": "SELECT user\nFROM (\nSelect user\n      From `bigquery-public-data.eclipse_megamovie.photos_v_0_1`\n      UNION ALL\n      Select user\n      From`bigquery-public-data.eclipse_megamovie.photos_v_0_2`\n      UNION ALL\n      Select user\n      From`bigquery-public-data.eclipse_megamovie.photos_v_0_3`\n) \nGROUP BY user \nHAVING COUNT (user)=( \nSELECT MAX(mycount) \nFROM ( \nSELECT user, COUNT(user) mycount \nFROM (\nSelect user\n      From `bigquery-public-data.eclipse_megamovie.photos_v_0_1`\n      UNION ALL\n      Select user\n      From`bigquery-public-data.eclipse_megamovie.photos_v_0_2`\n      UNION ALL\n      Select user\n      From`bigquery-public-data.eclipse_megamovie.photos_v_0_3`\n)\nGROUP BY user))\nORDER BY COUNT(user) \nLIMIT 1",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 100: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/eclipse_megamovie/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq219",
        "db_id": "IOWA_LIQUOR_SALES",
        "question": "In the Iowa Liquor Sales dataset, starting from January 1, 2022 through the last fully completed month, which two liquor categories, each contributing an average of at least 1% to the monthly sales volume over at least 24 months of available data, have the lowest Pearson correlation coefficient when comparing their monthly percentages of total liquor sales across those months, and what are their names?",
        "db_type": "snowflake",
        "db_size": 24,
        "query": "WITH\nMonthlyTotals AS\n(\n  SELECT\n    TO_CHAR(\"date\", 'YYYY-MM') AS \"month\",\n    SUM(\"volume_sold_gallons\") AS \"total_monthly_volume\"\n  FROM\n    IOWA_LIQUOR_SALES.IOWA_LIQUOR_SALES.\"SALES\"\n  WHERE\n    \"date\" >= '2022-01-01' \n    AND TO_CHAR(\"date\", 'YYYY-MM') < TO_CHAR(CURRENT_DATE(), 'YYYY-MM')\n  GROUP BY\n    TO_CHAR(\"date\", 'YYYY-MM')\n),\n\nMonthCategory AS\n(\n  SELECT\n    TO_CHAR(\"date\", 'YYYY-MM') AS \"month\",\n    \"category\",\n    \"category_name\",\n    SUM(\"volume_sold_gallons\") AS \"category_monthly_volume\",\n    CASE \n      WHEN \"total_monthly_volume\" != 0 THEN (SUM(\"volume_sold_gallons\") / \"total_monthly_volume\") * 100\n      ELSE NULL\n    END AS \"category_pct_of_month_volume\"\n  FROM\n    IOWA_LIQUOR_SALES.IOWA_LIQUOR_SALES.\"SALES\" AS Sales\n  LEFT JOIN\n    MonthlyTotals ON TO_CHAR(Sales.\"date\", 'YYYY-MM') = MonthlyTotals.\"month\"\n  WHERE\n    Sales.\"date\" >= '2022-01-01' \n    AND TO_CHAR(Sales.\"date\", 'YYYY-MM') < TO_CHAR(CURRENT_DATE(), 'YYYY-MM')\n  GROUP BY\n    TO_CHAR(Sales.\"date\", 'YYYY-MM'), \"category\", \"category_name\", \"total_monthly_volume\"\n),\n\nmiddle_info AS \n(\n  SELECT\n    Category1.\"category\" AS \"category1\",\n    Category1.\"category_name\" AS \"category_name1\",\n    Category2.\"category\" AS \"category2\",\n    Category2.\"category_name\" AS \"category_name2\",\n    COUNT(DISTINCT Category1.\"month\") AS \"num_months\",\n    CORR(Category1.\"category_pct_of_month_volume\", Category2.\"category_pct_of_month_volume\") AS \"category_corr_across_months\",\n    AVG(Category1.\"category_pct_of_month_volume\") AS \"category1_avg_pct_of_month_volume\",\n    AVG(Category2.\"category_pct_of_month_volume\") AS \"category2_avg_pct_of_month_volume\"\n  FROM\n    MonthCategory Category1\n  INNER JOIN\n    MonthCategory Category2 \n    ON Category1.\"month\" = Category2.\"month\"\n  GROUP BY\n    Category1.\"category\", Category1.\"category_name\", Category2.\"category\", Category2.\"category_name\"\n  HAVING\n    \"num_months\" >= 24\n    AND \"category1_avg_pct_of_month_volume\" >= 1\n    AND \"category2_avg_pct_of_month_volume\" >= 1\n)\n\nSELECT \n  \"category_name1\", \n  \"category_name2\"\nFROM \n  middle_info\nORDER BY \n  \"category_corr_across_months\"\nLIMIT 1;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq219.csv"
    },
    {
        "instance_id": "bq199",
        "db_id": "iowa_liquor_sales",
        "question": "Identify the top 10 liquor categories in Iowa in 2021 by calculating, for each category, the average of the per-liter retail prices across all sales transactions in that category during 2021. For these top categories, provide their average per-liter retail prices calculated in the same manner for the years 2019, 2020, and 2021.",
        "db_type": "big_query",
        "db_size": 24,
        "query": "WITH price_2020 AS (\n  SELECT \n    category_name AS category, \n    AVG(state_bottle_retail / (bottle_volume_ml / 1000)) AS avg_price_liter_2020\n  FROM \n    `bigquery-public-data.iowa_liquor_sales.sales`\n  WHERE \n    bottle_volume_ml > 0 \n    AND EXTRACT(YEAR FROM date) = 2020\n  GROUP BY \n    category\n),\nprice_2019 AS (\n  SELECT \n    category_name AS category, \n    AVG(state_bottle_retail / (bottle_volume_ml / 1000)) AS avg_price_liter_2019\n  FROM \n    `bigquery-public-data.iowa_liquor_sales.sales`\n  WHERE \n    bottle_volume_ml > 0 \n    AND EXTRACT(YEAR FROM date) = 2019\n  GROUP BY \n    category\n),\nprice_2021 AS (\n  SELECT \n    category_name AS category, \n    AVG(state_bottle_retail / (bottle_volume_ml / 1000)) AS avg_price_liter_2021\n  FROM \n    `bigquery-public-data.iowa_liquor_sales.sales`\n  WHERE \n    bottle_volume_ml > 0 \n    AND EXTRACT(YEAR FROM date) = 2021\n  GROUP BY \n    category\n)\nSELECT \n  price_2021.category, \n  price_2019.avg_price_liter_2019, \n  price_2020.avg_price_liter_2020, \n  price_2021.avg_price_liter_2021\nFROM \n  price_2021\nLEFT JOIN \n  price_2019 ON price_2021.category = price_2019.category\nLEFT JOIN \n  price_2020 ON price_2021.category = price_2020.category\nORDER BY \n  price_2021.avg_price_liter_2021 DESC\nLIMIT \n  10;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq199.csv"
    },
    {
        "instance_id": "bq218",
        "db_id": "iowa_liquor_sales",
        "question": "What are the top 5 items with the highest year-over-year growth percentage in total sales revenue for the year 2023?",
        "db_type": "big_query",
        "db_size": 24,
        "query": "WITH AnnualSales AS (\n  SELECT\n    item_description,\n    EXTRACT(YEAR FROM date) AS year,\n    SUM(sale_dollars) AS total_sales_revenue,\n    COUNT(DISTINCT invoice_and_item_number) AS unique_purchases\n  FROM\n    `bigquery-public-data.iowa_liquor_sales.sales`\n  WHERE\n    EXTRACT(YEAR FROM date) IN (2022, 2023)\n    AND item_description IS NOT NULL\n    AND sale_dollars IS NOT NULL\n  GROUP BY\n    item_description, year\n),\nYoYGrowth AS (\n  SELECT\n    curr.item_description,\n    curr.year,\n    curr.total_sales_revenue,\n    curr.unique_purchases,\n    LAG(curr.total_sales_revenue) OVER(PARTITION BY curr.item_description ORDER BY curr.year) AS prev_year_sales_revenue,\n    (curr.total_sales_revenue - LAG(curr.total_sales_revenue) OVER(PARTITION BY curr.item_description ORDER BY curr.year)) / LAG(curr.total_sales_revenue) OVER(PARTITION BY curr.item_description ORDER BY curr.year) * 100 AS yoy_growth_percentage\n  FROM\n    AnnualSales curr\n),\ntotal_info AS (\nSELECT\n  item_description,\n  year,\n  total_sales_revenue,\n  unique_purchases,\n  prev_year_sales_revenue,\n  yoy_growth_percentage\nFROM\n  YoYGrowth\nWHERE\n  year = 2023\n  AND prev_year_sales_revenue IS NOT NULL -- Exclude rows where there's no previous year data to calculate YoY growth\nORDER BY\n  year, total_sales_revenue \nDESC\n)\n\nSELECT item_description\nFROM total_info\norder by yoy_growth_percentage\nDESC\nLIMIT 5",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq218.csv"
    },
    {
        "instance_id": "bq049",
        "db_id": "iowa_liquor_sales_plus",
        "question": "Please show the monthly per capita Bourbon Whiskey sales during 2022 in Dubuque County for the zip code that ranks third in total Bourbon Whiskey sales, using only the population aged 21 and older.",
        "db_type": "big_query",
        "db_size": 36,
        "query": "WITH DUBUQUE_LIQUOR_CTE AS (\nSELECT\n  CASE\n      WHEN UPPER(category_name) LIKE 'BUTTERSCOTCH SCHNAPPS' THEN 'All Other' --Edge case is not a scotch\n      WHEN UPPER(category_name) LIKE '%WHISKIES' \n            AND UPPER(category_name) NOT LIKE '%RYE%'\n            AND UPPER(category_name) NOT LIKE '%BOURBON%'\n            AND UPPER(category_name) NOT LIKE '%SCOTCH%'     THEN 'Other Whiskey'\n      WHEN UPPER(category_name) LIKE '%RYE%'                 THEN 'Rye Whiskey'\n      WHEN UPPER(category_name) LIKE '%BOURBON%'             THEN 'Bourbon Whiskey'\n      WHEN UPPER(category_name) LIKE '%SCOTCH%'              THEN 'Scotch Whiskey'\n      ELSE 'All Other'\n  END                              AS category_group,\n  EXTRACT(MONTH FROM date)         AS month,    -- At the time of this query, there is only data until month 6.\n  LEFT(CAST(zip_code AS string),5) AS zip_code, -- Casting to string necessary because zip_code has a mix of int & str types.\n  ROUND(SUM(sale_dollars), 2)      AS sale_dollars_sum,\n\nFROM \n  bigquery-public-data.iowa_liquor_sales.sales\n\nWHERE\n  UPPER(county)               = 'DUBUQUE'\n  AND EXTRACT(YEAR FROM date) = 2022\n\nGROUP BY\n  category_group,\n  month,\n  zip_code\n  \nORDER BY \n  category_group,\n  month,\n  zip_code\n),\n\nDUBUQUE_POPULATION_CTE AS (\nSELECT\n  zipcode,\n  SUM(population) AS population_sum\nFROM bigquery-public-data.census_bureau_usa.population_by_zip_2010\nWHERE \n  minimum_age >= 21\nGROUP BY \n  zipcode\n),\nMONTH_INFO AS (\nSELECT \n  l.month,\n  l.zip_code,\n  l.sale_dollars_sum,\n  ROUND(sale_dollars_sum/p.population_sum, 2) AS dollars_per_capita\nFROM \n  DUBUQUE_LIQUOR_CTE AS l\n  LEFT JOIN \n  DUBUQUE_POPULATION_CTE AS p\n  ON l.zip_code = p.zipcode\nWHERE\n  category_group = 'Bourbon Whiskey'\nGROUP BY \n  category_group,\n  zip_code,\n  month,\n  sale_dollars_sum,\n  zipcode,\n  population_sum\nORDER BY\n  zip_code,\n  month\n),\nzip_code_sales AS (\n    SELECT\n        zip_code,\n        SUM(sale_dollars_sum) AS total_sale_dollars_sum\n    FROM MONTH_INFO\n    GROUP BY zip_code\n),\nranked_zip_codes AS (\n    SELECT\n        zip_code,\n        total_sale_dollars_sum,\n        ROW_NUMBER() OVER (ORDER BY total_sale_dollars_sum DESC) AS rank\n    FROM zip_code_sales\n)\nSELECT\n    t.month,\n    t.zip_code,\n    t.dollars_per_capita\nFROM MONTH_INFO t\nJOIN ranked_zip_codes r\nON t.zip_code = r.zip_code\nWHERE r.rank = 3\nORDER BY t.month;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq049.csv"
    },
    {
        "instance_id": "bq286",
        "db_id": "usa_names",
        "question": "Can you tell me the name of the most popular female baby in Wyoming for the year 2021, based on the proportion of female babies given that name compared to the total number of female babies given the same name across all states?",
        "db_type": "big_query",
        "db_size": 10,
        "query": "SELECT\n  a.name AS name\nFROM\n  `bigquery-public-data.usa_names.usa_1910_current` a\nJOIN (\n  SELECT\n    name,\n    gender,\n    year,\n    SUM(number) AS total_number\n  FROM\n    `bigquery-public-data.usa_names.usa_1910_current`\n  GROUP BY\n    name,\n    gender,\n    year) b\nON\n  a.name = b.name\n  AND a.gender = b.gender\n  AND a.year = b.year\nWHERE \n    a.gender = 'F' AND\n    a.state = 'WY' AND\n    a.year = 2021\nORDER BY (a.number / b.total_number) DESC\nLIMIT 1",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq286.csv"
    },
    {
        "instance_id": "bq165",
        "db_id": "mitelman",
        "question": "Can you use CytoConverter genomic coordinates to calculate the frequency of chromosomal gains and losses across a cohort of breast cancer (morphology='3111') and adenocarcinoma (topology='0401') samples? Concretely, please include the number and frequency (2 decimals in percentage) of amplifications (gains of more than 1 copy), gains (1 extra copy), losses (1 copy) and homozygous deletions (loss of 2 copies) for each chromosomal band. And sort the result by the ordinal of each chromosome and the starting-ending base-pair position of each band in ascending order.",
        "db_type": "big_query",
        "db_size": 165,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 106: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/mitelman/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq169",
        "db_id": "mitelman",
        "question": "Retrieve distinct case references, case numbers, investigation numbers, and clone information where a single clone simultaneously exhibits all three of the following genetic alterations: (1) a loss on chromosome 13 between positions 48,303,751 and 48,481,890, (2) a loss on chromosome 17 between positions 7,668,421 and 7,687,490, and (3) a gain on chromosome 11 between positions 108,223,067 and 108,369,102. For each matching clone, display the chromosomal details for each of these three regions (including chromosome number represented by ChrOrd, start position, and end position) and the corresponding karyotype short description from the KaryClone table. Use the CytoConverted and KaryClone.",
        "db_type": "big_query",
        "db_size": 165,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 107: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/mitelman/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq111",
        "db_id": "mitelman",
        "question": "Could you compute, by chromosome, the Pearson correlation between the frequency of copy number aberrations (including amplifications, gains, losses, and deletions) from the Mitelman database for cases with morph = 3111 and topo = 0401, and those computed from TCGA data, returning correlation coefficients and corresponding p-values for each aberration type, ensuring only results with at least five matching records are shown.",
        "db_type": "big_query",
        "db_size": 165,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\Correlations_between_Mitelman_and_TCGA_datasets.md",
        "error_info": "Error occurred while executing act() on sample 108: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/mitelman/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq451",
        "db_id": "_1000_GENOMES",
        "question": "Extract genotype data for single nucleotide polymorphisms (SNPs) on chromosome X, excluding positions where the `start` value is between 59999 and 2699519 or between 154931042 and 155260559. For each sample, identify genotype calls where the genotype array has at least one allele. Classify each genotype call into one of the following categories: homozygous reference alleles (both alleles are 0), homozygous alternate alleles (both alleles are the same and greater than 0), or heterozygous alleles (alleles are different, or any allele is null, and at least one allele is greater than 0). Compute the total number of callable sites (the sum of all three genotype categories), the number of homozygous reference, homozygous alternate, and heterozygous genotype calls, the total number of single nucleotide variants (SNVs) as the sum of homozygous alternate and heterozygous genotype calls, the percentage of heterozygous genotype calls among all SNVs, and the percentage of homozygous alternate genotype calls among all SNVs. Output the sample ID along with these computed counts and percentages, and order the results by the percentage of heterozygous genotype calls among SNVs in descending order, then by sample ID.",
        "db_type": "snowflake",
        "db_size": 114,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\1000_genomes_alleles_type.md",
        "error_info": "Error occurred while executing act() on sample 109: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/_1000_GENOMES/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq452",
        "db_id": "_1000_GENOMES",
        "question": "Identify variants on chromosome 12 and, for each variant, calculate the chi-squared score using allele counts in cases and controls, where cases are individuals from the 'EAS' super population and controls are individuals from all other super populations. Apply Yates's correction for continuity in the chi-squared calculation, ensuring that the expected counts for each allele in both groups are at least 5. Return the start position, end position, and chi-squared score of the top variants where the chi-squared score is no less than 29.71679.",
        "db_type": "snowflake",
        "db_size": 114,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 110: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/_1000_GENOMES/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq453",
        "db_id": "_1000_GENOMES",
        "question": "In chromosome 17 between positions 41196311 and 41277499, what are the reference names, start and end positions, reference bases, distinct alternate bases, variant types, and the chi-squared scores (calculated from Hardy-Weinberg equilibrium) along with the total number of genotypes, their observed and expected counts for homozygous reference, heterozygous, and homozygous alternate genotypes, as well as allele frequencies (including those from 1KG), for each variant?",
        "db_type": "snowflake",
        "db_size": 114,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 111: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/_1000_GENOMES/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq454",
        "db_id": "_1000_GENOMES",
        "question": "For the 1000 Genomes dataset, analyze common autosomal variants (those with an allele frequency of at least 0.05) across different super populations. For each super population, count how many variants are shared by each specific number of samples within that super population. Include in your results the total population size of each super population, whether the variant is common (allele frequency â‰¥ 0.05), the number of samples having each variant, and the total count of variants shared by that many samples. Only include autosomal variants by explicitly excluding sex chromosomes (X, Y) and mitochondrial DNA (MT) from the analysis. Consider only samples that have at least one alternate allele (non-reference) for the variant.",
        "db_type": "snowflake",
        "db_size": 114,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 112: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/_1000_GENOMES/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq279",
        "db_id": "austin",
        "question": "Can you provide the number of distinct active and closed bike share stations for each year 2013 and 2014?",
        "db_type": "big_query",
        "db_size": 117,
        "query": "SELECT\n    t.year,\n    CASE \n        WHEN t.year = 2013 THEN (\n                                  SELECT \n                                    COUNT(DISTINCT station_id)\n                                  FROM \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_trips` t\n                                  INNER JOIN \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\n                                  ON \n                                    t.start_station_id = s.station_id\n                                  WHERE \n                                    s.status = 'active' AND EXTRACT(YEAR FROM start_time) = 2013\n                                 ) \n        WHEN t.year = 2014 THEN (\n                                  SELECT \n                                    COUNT(DISTINCT station_id)\n                                  FROM \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_trips` t\n                                  INNER JOIN \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\n                                  ON \n                                    t.start_station_id = s.station_id\n                                  WHERE \n                                    s.status = 'active' AND EXTRACT(YEAR FROM start_time) = 2014\n                                 )\n    END\n    AS number_status_active,\n    CASE \n        WHEN t.year = 2013 THEN (\n                                  SELECT \n                                   COUNT(DISTINCT station_id)\n                                  FROM \n                                  `bigquery-public-data.austin_bikeshare.bikeshare_trips` t\n                                  INNER JOIN \n                                  `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\n                                  ON \n                                   t.start_station_id = s.station_id\n                                  WHERE \n                                   s.status = 'closed' AND EXTRACT(YEAR FROM start_time) = 2013\n                                 ) \n        WHEN t.year = 2014 THEN (\n                                  SELECT \n                                  COUNT(DISTINCT station_id)\n                                  FROM \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_trips` t\n                                  INNER JOIN \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\n                                  ON \n                                    t.start_station_id = s.station_id\n                                  WHERE \n                                    s.status = 'closed' AND EXTRACT(YEAR FROM start_time) = 2014\n                                 )\n    END\n    AS number_status_closed\nFROM\n    (\n      SELECT \n         EXTRACT(YEAR FROM start_time) AS year,\n         start_station_id\n      FROM\n         `bigquery-public-data.austin_bikeshare.bikeshare_trips`\n    ) \n    AS t\nINNER JOIN\n    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\nON\n    t.start_station_id = s.station_id\nWHERE\n    t.year BETWEEN 2013 AND 2014\nGROUP BY\n    t.year\nORDER BY\n    t.year",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 113: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/austin/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq281",
        "db_id": "austin",
        "question": "What is the highest number of electric bike rides lasting more than 10 minutes taken by subscribers with 'Student Membership' in a single day, excluding rides starting or ending at 'Mobile Station' or 'Repair Shop'?",
        "db_type": "big_query",
        "db_size": 117,
        "query": "SELECT\n  COUNT(1) AS num_rides\nFROM\n  `bigquery-public-data.austin_bikeshare.bikeshare_trips` \nWHERE \nstart_station_name \n    NOT IN ('Mobile Station', 'Repair Shop')\nAND\nend_station_name \n    NOT IN ('Mobile Station', 'Repair Shop')\nAND \nsubscriber_type = 'Student Membership'\nAND\nbike_type = 'electric'\nAND\nduration_minutes > 10\nGROUP BY \n    EXTRACT(YEAR from start_time), \n    EXTRACT(MONTH from start_time), \n    EXTRACT(DAY from start_time)\nORDER BY num_rides DESC\nLIMIT 1",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 114: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/austin/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq282",
        "db_id": "austin",
        "question": "Can you tell me the numeric value of the active council district in Austin which has the highest number of bike trips that start and end within the same district, but not at the same station?",
        "db_type": "big_query",
        "db_size": 117,
        "query": "SELECT \n  district\nFROM (\n  SELECT\n    S.starting_district AS district,\n    T.start_station_id,\n    T.end_station_id\n  FROM\n    `bigquery-public-data.austin_bikeshare.bikeshare_trips` AS T\n  INNER JOIN (\n    SELECT\n      station_id,\n      council_district AS starting_district\n    FROM\n      `bigquery-public-data.austin_bikeshare.bikeshare_stations`\n    WHERE\n      status = \"active\"\n  ) AS S ON T.start_station_id = S.station_id\n  WHERE\n    S.starting_district IN (\n      SELECT council_district\n      FROM `bigquery-public-data.austin_bikeshare.bikeshare_stations`\n      WHERE\n        status = \"active\" AND\n        station_id = SAFE_CAST(T.end_station_id AS INT64)\n    )\n    AND T.start_station_id != SAFE_CAST(T.end_station_id AS INT64)\n) \nGROUP BY district\nORDER BY COUNT(*) DESC\nLIMIT 1;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 115: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/austin/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq006",
        "db_id": "austin",
        "question": "What is the date with the second highest Z-score for daily counts of 'PUBLIC INTOXICATION' incidents in Austin for the year 2016? List the date in the format of '2016-xx-xx'.",
        "db_type": "big_query",
        "db_size": 117,
        "query": "WITH incident_stats AS (\n  SELECT \n    COUNT(descript) AS total_pub_intox\n  FROM \n    `bigquery-public-data.austin_incidents.incidents_2016` \n  WHERE \n    descript = 'PUBLIC INTOXICATION' \n  GROUP BY \n    date\n),\naverage_and_stddev AS (\n  SELECT \n    AVG(total_pub_intox) AS avg, \n    STDDEV(total_pub_intox) AS stddev \n  FROM \n    incident_stats\n),\ndaily_z_scores AS (\n  SELECT \n    date, \n    COUNT(descript) AS total_pub_intox, \n    ROUND((COUNT(descript) - a.avg) / a.stddev, 2) AS z_score\n  FROM \n    `bigquery-public-data.austin_incidents.incidents_2016`,\n    (SELECT avg, stddev FROM average_and_stddev) AS a\n  WHERE \n    descript = 'PUBLIC INTOXICATION'\n  GROUP BY \n    date, avg, stddev\n)\n\nSELECT \n  date\nFROM \n  daily_z_scores\nORDER BY \n  z_score DESC\nLIMIT 1\nOFFSET 1",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 116: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/austin/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq283",
        "db_id": "AUSTIN",
        "question": "Among all stations that are currently active, identify those that rank in the top 15 (including ties) based on the total number of trips that start at each station. For each of these stations, return the station ID, the total number of starting trips, the percentage of those trips out of the overall starting trips from active stations, and the average trip duration in minutes. Order the results by the stationâ€™s rank.",
        "db_type": "snowflake",
        "db_size": 119,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 117: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/AUSTIN/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq284",
        "db_id": "bbc",
        "question": "Can you provide a breakdown of the total number of articles into different categories and the percentage of those articles that mention \"education\" within each category from the BBC News?",
        "db_type": "big_query",
        "db_size": 4,
        "query": "SELECT \n  category,\n  COUNT(*) AS number_total_by_category,  \n  CASE \n    WHEN category = 'tech' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'tech') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'tech')\n    WHEN category = 'sport' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'sport') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'sport')\n    WHEN category = 'business' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'business') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'business')\n    WHEN category = 'politics' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'politics') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'politics')\n    WHEN category = 'entertainment' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'entertainment') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'entertainment')\n  END AS percent_education\nFROM `bigquery-public-data.bbc_news.fulltext`\nGROUP BY\n  category;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq284.csv"
    },
    {
        "instance_id": "sf_bq412",
        "db_id": "GOOGLE_ADS",
        "question": "Please retrieve the page URLs, first shown time, last shown time, removal reason, violation category, and the lower and upper bounds of times shown for the five most recently removed ads in the Croatia region (region code 'HR'), where the times shown availability date is null, the times shown lower bound exceeds 10,000, the times shown upper bound is below 25,000, and the ads used at least one non-unused audience selection approach among demographics, geographic location, contextual signals, customer lists, or topics of interest, ordering the resulting ads by their last shown time in descending order.",
        "db_type": "snowflake",
        "db_size": 16,
        "query": "SELECT\n    \"creative_page_url\",\n    TO_TIMESTAMP(GET(\"region_stat\".value, 'first_shown')) AS \"first_shown\",\n    TO_TIMESTAMP(GET(\"region_stat\".value, 'last_shown')) AS \"last_shown\",\n    REPLACE(REPLACE(\"disapproval\"[0].\"removal_reason\", '\"\"', '\"'), '\"', '') AS \"removal_reason\", \n    REPLACE(REPLACE(\"disapproval\"[0].\"violation_category\", '\"\"', '\"'), '\"', '') AS \"violation_category\",\n    GET(\"region_stat\".value, 'times_shown_lower_bound') AS \"times_shown_lower\",\n    GET(\"region_stat\".value, 'times_shown_upper_bound') AS \"times_shown_upper\"\nFROM\n    \"GOOGLE_ADS\".\"GOOGLE_ADS_TRANSPARENCY_CENTER\".\"REMOVED_CREATIVE_STATS\",\n    LATERAL FLATTEN(input => \"region_stats\") AS \"region_stat\"\nWHERE\n    GET(\"region_stat\".value, 'region_code') = 'HR' \n    AND GET(\"region_stat\".value, 'times_shown_availability_date') IS NULL \n    AND GET(\"region_stat\".value, 'times_shown_lower_bound') > 10000 \n    AND GET(\"region_stat\".value, 'times_shown_upper_bound') < 25000\n    AND (\n        GET(\"audience_selection_approach_info\", 'demographic_info') != 'CRITERIA_UNUSED' \n        OR GET(\"audience_selection_approach_info\", 'geo_location') != 'CRITERIA_UNUSED' \n        OR GET(\"audience_selection_approach_info\", 'contextual_signals') != 'CRITERIA_UNUSED' \n        OR GET(\"audience_selection_approach_info\", 'customer_lists') != 'CRITERIA_UNUSED' \n        OR GET(\"audience_selection_approach_info\", 'topics_of_interest') != 'CRITERIA_UNUSED'\n    )\nORDER BY\n    \"last_shown\" DESC\nLIMIT 5;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq412.csv"
    },
    {
        "instance_id": "sf_bq423",
        "db_id": "GOOGLE_ADS",
        "question": "Between January 1, 2023, and January 1, 2024, which image-type advertisement on the topic of Health, published by a verified advertiser located in Cyprus, was shown in Croatia, has times_shown_availability_date as NULL (meaning the times shown data is available), utilized demographic information, geo-location targeting, contextual signals, customer lists, and topics of interest without any of these selection methods being unused, and additionally had its first shown date strictly after January 1, 2023, and last shown date strictly before January 1, 2024? Among such ads, provide the page URL of the one with the highest upper bound of times shown.",
        "db_type": "snowflake",
        "db_size": 16,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf_bq423.csv"
    },
    {
        "instance_id": "bq418",
        "db_id": "targetome_reactome",
        "question": "Determine which three lowest-level Reactome pathways (with TAS evidence) have the highest chi-squared statistics, considering only Homo sapiens targets associated with sorafenib under the conditions that the median assay value is â‰¤ 100 and both low and high assay values are â‰¤ 100 or null. For each of these three pathways, how many of these targets and non-targets lie within the pathway and outside it?",
        "db_type": "big_query",
        "db_size": 58,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq418.csv"
    },
    {
        "instance_id": "bq457",
        "db_id": "libraries_io",
        "question": "Get details of repositories that use specific feature toggle libraries. For each repository, include the full name with owner, hosting platform type, size in bytes, primary programming language, fork source name (if any), last update timestamp, the artifact and library names of the feature toggle used, and the library's programming languages. Include repositories that depend on the specified feature toggle libraries, defined by their artifact names, library names, platforms, and languages.",
        "db_type": "big_query",
        "db_size": 163,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\feature_toggle_libraries.md",
        "error_info": "Error occurred while executing act() on sample 122: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/libraries_io/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq227",
        "db_id": "london",
        "question": "Could you provide the annual percentage shares, rounded to two decimal places, of the top 5 minor crime categories from 2008 in London's total crimes, with each year displayed in one row?",
        "db_type": "big_query",
        "db_size": 39,
        "query": "WITH top5_categories AS (\n  SELECT minor_category\n  FROM `bigquery-public-data.london_crime.crime_by_lsoa`\n  WHERE year = 2008\n  GROUP BY minor_category\n  ORDER BY SUM(value) DESC\n  LIMIT 5\n),\n\ntotal_crimes_per_year AS (\n  SELECT \n    year, \n    SUM(value) AS total_crimes_year\n  FROM `bigquery-public-data.london_crime.crime_by_lsoa`\n  GROUP BY year\n),\n\ntop5_crimes_per_year AS (\n  SELECT\n    year,\n    minor_category,\n    SUM(value) AS total_crimes_category_year\n  FROM `bigquery-public-data.london_crime.crime_by_lsoa`\n  WHERE minor_category IN (SELECT minor_category FROM top5_categories)\n  GROUP BY year, minor_category\n)\n\nSELECT\n  t.year,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 0) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 1`,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 1) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 2`,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 2) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 3`,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 3) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 4`,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 4) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 5`\nFROM\n  top5_crimes_per_year t\nJOIN\n  total_crimes_per_year y ON t.year = y.year\nGROUP BY\n  t.year, y.total_crimes_year\nORDER BY\n  t.year;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq227.csv"
    },
    {
        "instance_id": "bq232",
        "db_id": "london",
        "question": "Could you provide the total number of 'Other Theft' incidents within the 'Theft and Handling' category for each year in the Westminster borough?",
        "db_type": "big_query",
        "db_size": 39,
        "query": "WITH borough_data AS (\n    SELECT \n        year, \n        month, \n        borough, \n        major_category, \n        minor_category, \n        SUM(value) AS total,\n    CASE \n        WHEN \n            major_category = 'Theft and Handling' \n        THEN \n            'Theft and Handling'\n        ELSE \n            'Other' \n    END AS major_division,\n    CASE \n        WHEN \n            minor_category = 'Other Theft' THEN minor_category\n        ELSE \n            'Other'\n    END AS minor_division,\n    FROM \n        bigquery-public-data.london_crime.crime_by_lsoa\n    GROUP BY 1,2,3,4,5\n    ORDER BY 1,2\n)\n\nSELECT year, SUM(total) AS year_total\nFROM borough_data\nWHERE \n    borough = 'Westminster'\nAND\n    major_division != 'Other'\nAND \n    minor_division != 'Other'\nGROUP BY year, major_division, minor_division\nORDER BY year;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq232.csv"
    },
    {
        "instance_id": "bq228",
        "db_id": "london",
        "question": "Please provide a list of the top three major crime categories in the borough of Barking and Dagenham, along with the number of incidents in each category.",
        "db_type": "big_query",
        "db_size": 39,
        "query": "WITH ranked_crimes AS (\n    SELECT\n        borough,\n        major_category,\n        RANK() OVER(PARTITION BY borough ORDER BY SUM(value) DESC) AS rank_per_borough,\n        SUM(value) AS no_of_incidents\n    FROM\n        `bigquery-public-data.london_crime.crime_by_lsoa`\n    GROUP BY\n        borough,\n        major_category\n)\n\nSELECT\n    borough,\n    major_category,\n    rank_per_borough,\n    no_of_incidents\nFROM\n    ranked_crimes\nWHERE\n    rank_per_borough <= 3\nAND \n    borough = 'Barking and Dagenham'\nORDER BY\n    borough,\n    rank_per_borough;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq228.csv"
    },
    {
        "instance_id": "bq229",
        "db_id": "open_images",
        "question": "Using the bigquery-public-data.open_images dataset, can you provide a count of how many distinct image URLs are categorized as 'cat' (where the image has label '/m/01yrx' with confidence=1) and how many distinct image URLs are categorized as 'other' (meaning they have no cat label '/m/01yrx' at all)?",
        "db_type": "big_query",
        "db_size": 30,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq229.csv"
    },
    {
        "instance_id": "bq393",
        "db_id": "hacker_news",
        "question": "I want to identify users who had activity followed by inactivity. Specifically, find the user ID and their corresponding month number (counting from their first activity month) for the user with the highest month number who became inactive (no activity recorded) after their last recorded activity month. For this analysis, only consider data up until September 10, 2024, and ensure the month number represents the count of months since the user's first activity. The user should have at least one month where they were expected to be active (within their activity span) but actually had no records.",
        "db_type": "big_query",
        "db_size": 14,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_bq393.csv"
    },
    {
        "instance_id": "bq397",
        "db_id": "ecommerce",
        "question": "After removing any duplicate records from the rev_transactions dataset, identify each channel grouping that has transactions from more than one country. For each such channel grouping, find the country with the highest total number of transactions and report both the country name and the sum of transactions for that channel grouping.",
        "db_type": "big_query",
        "db_size": 154,
        "query": "WITH tmp AS (\n  SELECT DISTINCT *\n  FROM `data-to-insights.ecommerce.rev_transactions`\n  -- Removing duplicated values\n),\ntmp1 AS (\n  SELECT \n    tmp.channelGrouping,\n    tmp.geoNetwork_country,\n    SUM(tmp.totals_transactions) AS tt\n  FROM tmp\n  GROUP BY 1, 2\n),\ntmp2 AS (\n  SELECT \n    channelGrouping,\n    geoNetwork_country,\n    SUM(tt) AS TotalTransaction,\n    COUNT(DISTINCT geoNetwork_country) OVER (PARTITION BY channelGrouping) AS CountryCount\n  FROM tmp1\n  GROUP BY channelGrouping, geoNetwork_country\n),\ntmp3 AS (\n  SELECT\n    channelGrouping,\n    geoNetwork_country AS Country,\n    TotalTransaction,\n    RANK() OVER (PARTITION BY channelGrouping ORDER BY TotalTransaction DESC) AS rnk\n  FROM tmp2\n  WHERE CountryCount > 1\n)\nSELECT\n  channelGrouping,\n  Country,\n  TotalTransaction\nFROM tmp3\nWHERE rnk = 1;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 128: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/ecommerce/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "bq402",
        "db_id": "ecommerce",
        "question": "Calculate the conversion rate from unique visitors to purchasers by using data exclusively from the `web_analytics` table in the `data-to-insights.ecommerce` dataset. A visitor is defined as a unique `fullVisitorId` present in the table, while a purchaser is a visitor who has at least one transaction recorded (`totals.transactions` is not null). The conversion rate is computed by dividing the number of unique purchasers by the total number of unique visitors. Additionally, calculate the average number of transactions per purchaser, considering only those visitors who have made at least one transaction.",
        "db_type": "big_query",
        "db_size": 154,
        "query": "WITH visitors AS (\n  SELECT\n    COUNT(DISTINCT fullVisitorId) AS total_visitors\n  FROM \n    `data-to-insights.ecommerce.web_analytics`\n),\n\npurchasers AS (\n  SELECT\n    COUNT(DISTINCT fullVisitorId) AS total_purchasers\n  FROM \n    `data-to-insights.ecommerce.web_analytics`\n  WHERE \n    totals.transactions IS NOT NULL\n),\n\ntransactions AS (\n  SELECT\n    COUNT(*) AS total_transactions,\n    AVG(totals.transactions) AS avg_transactions_per_purchaser\n  FROM \n    `data-to-insights.ecommerce.web_analytics`\n  WHERE \n    totals.transactions IS NOT NULL\n)\n\nSELECT\n  p.total_purchasers / v.total_visitors AS conversion_rate,\n  a.avg_transactions_per_purchaser AS avg_transactions_per_purchaser\nFROM\n  visitors v,\n  purchasers p,\n  transactions a;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 129: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/ecommerce/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq118",
        "db_id": "DEATH",
        "question": "Among individuals identified as white, how much higher is the average number of deaths from ICD-10 codes whose descriptions contain the word â€œdischargeâ€ (specifically excluding â€œUrethral discharge,â€ â€œDischarge of firework,â€ and â€œLegal intervention involving firearm dischargeâ€) compared to the average number of deaths from ICD-10 codes whose descriptions contain the word â€œvehicle,â€ when aggregated by age groups?",
        "db_type": "snowflake",
        "db_size": 99,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 130: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/DEATH/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf_bq072",
        "db_id": "DEATH",
        "question": "Please provide, for each age from 12 through 18 (inclusive), the total number of deaths and the number of deaths among individuals identified as Black (based on race descriptions containing the word â€˜blackâ€™), specifically for deaths associated with ICD-10 codes whose descriptions include the word â€˜vehicleâ€™ and for deaths associated with ICD-10 codes whose descriptions include the word â€˜firearm.â€™ Use the EntityAxisConditions table to determine which ICD-10 codes were involved in each death, rather than joining ICD-10 code information directly on the death records.",
        "db_type": "snowflake",
        "db_size": 99,
        "query": "WITH BlackRace AS (\n    SELECT CAST(\"Code\" AS INT) AS CODE\n    FROM DEATH.DEATH.RACE\n    WHERE LOWER(\"Description\") LIKE '%black%'\n)\nSELECT \n    v.\"Age\", \n    v.\"Total\" AS \"Vehicle_Total\", \n    v.\"Black\" AS \"Vehicle_Black\",\n    g.\"Total\" AS \"Gun_Total\", \n    g.\"Black\" AS \"Gun_Black\"\nFROM (\n    SELECT \n        \"Age\", \n        COUNT(*) AS \"Total\", \n        COUNT_IF(\"Race\" IN (SELECT CODE FROM BlackRace)) AS \"Black\"\n    FROM DEATH.DEATH.DEATHRECORDS d\n    JOIN (\n        SELECT \n            DISTINCT e.\"DeathRecordId\" AS \"id\"\n        FROM DEATH.DEATH.ENTITYAXISCONDITIONS e\n        JOIN (\n            SELECT * \n            FROM DEATH.DEATH.ICD10CODE \n            WHERE LOWER(\"Description\") LIKE '%vehicle%'\n        ) c \n        ON e.\"Icd10Code\" = c.\"Code\"\n    ) f\n    ON d.\"Id\" = f.\"id\"\n    WHERE \"Age\" BETWEEN 12 AND 18\n    GROUP BY \"Age\"\n) v  -- Vehicle\n\nJOIN (\n    SELECT \n        \"Age\", \n        COUNT(*) AS \"Total\", \n        COUNT_IF(\"Race\" IN (SELECT CODE FROM BlackRace)) AS \"Black\"\n    FROM DEATH.DEATH.DEATHRECORDS d\n    JOIN (\n        SELECT \n            DISTINCT e.\"DeathRecordId\" AS \"id\"\n        FROM DEATH.DEATH.ENTITYAXISCONDITIONS e\n        JOIN (\n            SELECT \n                \"Code\", \"Description\" \n            FROM DEATH.DEATH.ICD10CODE\n            WHERE \"Description\" LIKE '%firearm%'\n        ) c \n        ON e.\"Icd10Code\" = c.\"Code\"\n    ) f\n    ON d.\"Id\" = f.\"id\"\n    WHERE \"Age\" BETWEEN 12 AND 18\n    GROUP BY \"Age\"\n) g\nON g.\"Age\" = v.\"Age\";",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 131: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/DEATH/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local002",
        "db_id": "E_commerce",
        "question": "Can you calculate the 5-day symmetric moving average of predicted toy sales for December 5 to 8, 2018, using daily sales data from January 1, 2017, to August 29, 2018, with a simple linear regression model? Finally provide the sum of those four 5-day moving averages?",
        "db_type": "sqlite",
        "db_size": 70,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local002.csv"
    },
    {
        "instance_id": "local003",
        "db_id": "E_commerce",
        "question": "According to the RFM definition document, calculate the average sales per order for each customer within distinct RFM segments, considering only 'delivered' orders. Use the customer unique identifier. Clearly define how to calculate Recency based on the latest purchase timestamp and specify the criteria for classifying RFM segments. The average sales should be computed as the total spend divided by the total number of orders. Please analyze and report the differences in average sales across the RFM segments",
        "db_type": "sqlite",
        "db_size": 70,
        "query": "WITH RecencyScore AS (\n    SELECT customer_unique_id,\n           MAX(order_purchase_timestamp) AS last_purchase,\n           NTILE(5) OVER (ORDER BY MAX(order_purchase_timestamp) DESC) AS recency\n    FROM orders\n        JOIN customers USING (customer_id)\n    WHERE order_status = 'delivered'\n    GROUP BY customer_unique_id\n),\nFrequencyScore AS (\n    SELECT customer_unique_id,\n           COUNT(order_id) AS total_orders,\n           NTILE(5) OVER (ORDER BY COUNT(order_id) DESC) AS frequency\n    FROM orders\n        JOIN customers USING (customer_id)\n    WHERE order_status = 'delivered'\n    GROUP BY customer_unique_id\n),\nMonetaryScore AS (\n    SELECT customer_unique_id,\n           SUM(price) AS total_spent,\n           NTILE(5) OVER (ORDER BY SUM(price) DESC) AS monetary\n    FROM orders\n        JOIN order_items USING (order_id)\n        JOIN customers USING (customer_id)\n    WHERE order_status = 'delivered'\n    GROUP BY customer_unique_id\n),\n\n-- 2. Assign each customer to a group\nRFM AS (\n    SELECT last_purchase, total_orders, total_spent,\n        CASE\n            WHEN recency = 1 AND frequency + monetary IN (1, 2, 3, 4) THEN \"Champions\"\n            WHEN recency IN (4, 5) AND frequency + monetary IN (1, 2) THEN \"Can't Lose Them\"\n            WHEN recency IN (4, 5) AND frequency + monetary IN (3, 4, 5, 6) THEN \"Hibernating\"\n            WHEN recency IN (4, 5) AND frequency + monetary IN (7, 8, 9, 10) THEN \"Lost\"\n            WHEN recency IN (2, 3) AND frequency + monetary IN (1, 2, 3, 4) THEN \"Loyal Customers\"\n            WHEN recency = 3 AND frequency + monetary IN (5, 6) THEN \"Needs Attention\"\n            WHEN recency = 1 AND frequency + monetary IN (7, 8) THEN \"Recent Users\"\n            WHEN recency = 1 AND frequency + monetary IN (5, 6) OR\n                recency = 2 AND frequency + monetary IN (5, 6, 7, 8) THEN \"Potentital Loyalists\"\n            WHEN recency = 1 AND frequency + monetary IN (9, 10) THEN \"Price Sensitive\"\n            WHEN recency = 2 AND frequency + monetary IN (9, 10) THEN \"Promising\"\n            WHEN recency = 3 AND frequency + monetary IN (7, 8, 9, 10) THEN \"About to Sleep\"\n        END AS RFM_Bucket\n    FROM RecencyScore\n        JOIN FrequencyScore USING (customer_unique_id)\n        JOIN MonetaryScore USING (customer_unique_id)\n)\n\nSELECT RFM_Bucket, \n       AVG(total_spent / total_orders) AS avg_sales_per_customer\nFROM RFM\nGROUP BY RFM_Bucket",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\RFM.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local003.csv"
    },
    {
        "instance_id": "local004",
        "db_id": "E_commerce",
        "question": "Could you tell me the number of orders, average payment per order and customer lifespan in weeks of the 3 custumers with the highest average payment per order, where the lifespan is calculated by subtracting the earliest purchase date from the latest purchase date in days, dividing by seven, and if the result is less than seven days, setting it to 1.0?",
        "db_type": "sqlite",
        "db_size": 70,
        "query": "WITH CustomerData AS (\n    SELECT\n        customer_unique_id,\n        COUNT(DISTINCT orders.order_id) AS order_count,\n        SUM(payment_value) AS total_payment,\n        JULIANDAY(MIN(order_purchase_timestamp)) AS first_order_day,\n        JULIANDAY(MAX(order_purchase_timestamp)) AS last_order_day\n    FROM customers\n        JOIN orders USING (customer_id)\n        JOIN order_payments USING (order_id)\n    GROUP BY customer_unique_id\n)\nSELECT\n    customer_unique_id,\n    order_count AS PF,\n    ROUND(total_payment / order_count, 2) AS AOV,\n    CASE\n        WHEN (last_order_day - first_order_day) < 7 THEN\n            1\n        ELSE\n            (last_order_day - first_order_day) / 7\n        END AS ACL\nFROM CustomerData\nORDER BY AOV DESC\nLIMIT 3",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local004.csv"
    },
    {
        "instance_id": "local009",
        "db_id": "Airlines",
        "question": "What is the distance of the longest route where Abakan is either the departure or destination city (in kilometers)?",
        "db_type": "sqlite",
        "db_size": 35,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\haversine_formula.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local009.csv"
    },
    {
        "instance_id": "local010",
        "db_id": "Airlines",
        "question": "Distribute all the unique city pairs into the distance ranges 0, 1000, 2000, 3000, 4000, 5000, and 6000+, based on their average distance of all routes between them. Then how many pairs are there in the distance range with the fewest unique city paires?",
        "db_type": "sqlite",
        "db_size": 35,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\haversine_formula.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local010.csv"
    },
    {
        "instance_id": "local015",
        "db_id": "California_Traffic_Collision",
        "question": "Please calculate the fatality rate for motorcycle collisions, separated by helmet usage. Specifically, calculate two percentages: 1) the percentage of motorcyclist fatalities in collisions where parties (drivers or passengers) were wearing helmets, and 2) the percentage of motorcyclist fatalities in collisions where parties were not wearing helmets. For each group, compute this by dividing the total number of motorcyclist fatalities by the total number of collisions involving that group. Use the parties table to determine helmet usage (from party_safety_equipment fields).",
        "db_type": "sqlite",
        "db_size": 120,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 137: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/California_Traffic_Collision/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local017",
        "db_id": "California_Traffic_Collision",
        "question": "In which year were the two most common causes of traffic accidents different from those in other years?",
        "db_type": "sqlite",
        "db_size": 120,
        "query": "WITH AnnualTotals AS (\n    SELECT \n        STRFTIME('%Y', collision_date) AS Year, \n        COUNT(case_id) AS AnnualTotal\n    FROM \n        collisions\n    GROUP BY \n        Year\n),\nCategoryTotals AS (\n    SELECT \n        STRFTIME('%Y', collision_date) AS Year,\n        pcf_violation_category AS Category,\n        COUNT(case_id) AS Subtotal\n    FROM \n        collisions\n    GROUP BY \n        Year, Category\n),\nCategoryPercentages AS (\n    SELECT \n        ct.Year,\n        ct.Category,\n        ROUND((ct.Subtotal * 100.0) / at.AnnualTotal, 1) AS PercentageOfAnnualRoadIncidents\n    FROM \n        CategoryTotals ct\n    JOIN \n        AnnualTotals at ON ct.Year = at.Year\n),\nRankedCategories AS (\n    SELECT\n        Year,\n        Category,\n        PercentageOfAnnualRoadIncidents,\n        ROW_NUMBER() OVER (PARTITION BY Year ORDER BY PercentageOfAnnualRoadIncidents DESC) AS Rank\n    FROM\n        CategoryPercentages\n),\nTopTwoCategories AS (\n    SELECT\n        Year,\n        GROUP_CONCAT(Category, ', ') AS TopCategories\n    FROM\n        RankedCategories\n    WHERE\n        Rank <= 2\n    GROUP BY\n        Year\n),\nUniqueYear AS (\n    SELECT\n        Year\n    FROM\n        TopTwoCategories\n    GROUP BY\n        TopCategories\n    HAVING COUNT(Year) = 1\n),\nresults AS (\nSELECT \n    rc.Year, \n    rc.Category, \n    rc.PercentageOfAnnualRoadIncidents\nFROM \n    UniqueYear u\nJOIN \n    RankedCategories rc ON u.Year = rc.Year\nWHERE \n    rc.Rank <= 2\n)\n\nSELECT distinct Year FROM results",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 138: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/California_Traffic_Collision/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local018",
        "db_id": "California_Traffic_Collision",
        "question": "For the primary collision factor violation category that was the most common cause of traffic accidents in 2021, how many percentage points did its share of annual road incidents in 2021 decrease compared to its share in 2011?",
        "db_type": "sqlite",
        "db_size": 120,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 139: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/California_Traffic_Collision/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local019",
        "db_id": "WWE",
        "question": "For the NXT title that had the shortest match (excluding titles with \"title change\"), what were the names of the two wrestlers involved?",
        "db_type": "sqlite",
        "db_size": 35,
        "query": "WITH MatchDetails AS (\n    SELECT\n        b.name AS titles,\n        m.duration AS match_duration,\n        w1.name || ' vs ' || w2.name AS matches,\n        m.win_type AS win_type,\n        l.name AS location,\n        e.name AS event,\n        ROW_NUMBER() OVER (PARTITION BY b.name ORDER BY m.duration ASC) AS rank\n    FROM \n        Belts b\n    INNER JOIN Matches m ON m.title_id = b.id\n    INNER JOIN Wrestlers w1 ON w1.id = m.winner_id\n    INNER JOIN Wrestlers w2 ON w2.id = m.loser_id\n    INNER JOIN Cards c ON c.id = m.card_id\n    INNER JOIN Locations l ON l.id = c.location_id\n    INNER JOIN Events e ON e.id = c.event_id\n    INNER JOIN Promotions p ON p.id = c.promotion_id\n    WHERE\n        p.name = 'NXT'\n        AND m.duration <> ''\n        AND b.name <> ''\n        AND b.name NOT IN (\n            SELECT name \n            FROM Belts \n            WHERE name LIKE '%title change%'\n        )\n),\nRank1 AS (\nSELECT \n    titles,\n    match_duration,\n    matches,\n    win_type,\n    location,\n    event\nFROM \n    MatchDetails\nWHERE \n    rank = 1\n)\nSELECT\n    SUBSTR(matches, 1, INSTR(matches, ' vs ') - 1) AS wrestler1,\n    SUBSTR(matches, INSTR(matches, ' vs ') + 4) AS wrestler2\nFROM\nRank1\nORDER BY match_duration \nLIMIT 1",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local019.csv"
    },
    {
        "instance_id": "local026",
        "db_id": "IPL",
        "question": "Please help me identify the top 3 bowlers who, in the overs where the maximum runs were conceded in each match, gave up the highest number of runs in a single over across all matches. For each of these bowlers, provide the match in which they conceded these maximum runs. Only consider overs that had the most runs conceded within their respective matches, and among these, determine which bowlers conceded the most runs in a single over overall.",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local026.csv"
    },
    {
        "instance_id": "local020",
        "db_id": "IPL",
        "question": "Which bowler has the lowest bowling average per wicket taken?",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local020.csv"
    },
    {
        "instance_id": "local021",
        "db_id": "IPL",
        "question": "Could you calculate the average of the total runs scored by all strikers who have scored more than 50 runs in any single match?",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local021.csv"
    },
    {
        "instance_id": "local022",
        "db_id": "IPL",
        "question": "Retrieve the names of players who scored no less than 100 runs in a match while playing for the team that lost that match.",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "-- Step 1: Calculate players' total runs in each match\nWITH player_runs AS (\n    SELECT \n        bbb.striker AS player_id, \n        bbb.match_id, \n        SUM(bsc.runs_scored) AS total_runs \n    FROM \n        ball_by_ball AS bbb\n    JOIN \n        batsman_scored AS bsc\n    ON \n        bbb.match_id = bsc.match_id \n        AND bbb.over_id = bsc.over_id \n        AND bbb.ball_id = bsc.ball_id \n        AND bbb.innings_no = bsc.innings_no\n    GROUP BY \n        bbb.striker, bbb.match_id\n    HAVING \n        SUM(bsc.runs_scored) >= 100\n),\n\n-- Step 2: Identify losing teams for each match\nlosing_teams AS (\n    SELECT \n        match_id, \n        CASE \n            WHEN match_winner = team_1 THEN team_2 \n            ELSE team_1 \n        END AS loser \n    FROM \n        match\n),\n\n-- Step 3: Combine the above results to get players who scored 100 or more runs in losing teams\nplayers_in_losing_teams AS (\n    SELECT \n        pr.player_id, \n        pr.match_id \n    FROM \n        player_runs AS pr\n    JOIN \n        losing_teams AS lt\n    ON \n        pr.match_id = lt.match_id\n    JOIN \n        player_match AS pm\n    ON \n        pr.player_id = pm.player_id \n        AND pr.match_id = pm.match_id \n        AND lt.loser = pm.team_id\n)\n\n-- Step 4: Select distinct player names from the player table\nSELECT DISTINCT \n    p.player_name \nFROM \n    player AS p\nJOIN \n    players_in_losing_teams AS plt\nON \n    p.player_id = plt.player_id\nORDER BY \n    p.player_name;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local022.csv"
    },
    {
        "instance_id": "local023",
        "db_id": "IPL",
        "question": "Please help me find the names of top 5 players with the highest average runs per match in season 5, along with their batting averages.",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "WITH runs_scored AS (\n    SELECT \n        bb.striker AS player_id,\n        bb.match_id,\n        bs.runs_scored AS runs\n    FROM \n        ball_by_ball AS bb\n    JOIN \n        batsman_scored AS bs ON bb.match_id = bs.match_id \n            AND bb.over_id = bs.over_id \n            AND bb.ball_id = bs.ball_id \n            AND bb.innings_no = bs.innings_no\n    WHERE \n        bb.match_id IN (SELECT match_id FROM match WHERE season_id = 5)\n),\ntotal_runs AS (\n    SELECT \n        player_id, \n        match_id, \n        SUM(runs) AS total_runs \n    FROM \n        runs_scored \n    GROUP BY \n        player_id, match_id\n),\nbatting_averages AS (\n    SELECT \n        player_id, \n        SUM(total_runs) AS runs, \n        COUNT(match_id) AS num_matches,\n        ROUND(SUM(total_runs) / CAST(COUNT(match_id) AS FLOAT), 3) AS batting_avg\n    FROM \n        total_runs \n    GROUP BY \n        player_id \n    ORDER BY \n        batting_avg DESC \n    LIMIT 5\n)\nSELECT \n    p.player_name,\n    b.batting_avg\nFROM \n    player AS p\nJOIN \n    batting_averages AS b ON p.player_id = b.player_id\nORDER BY \n    b.batting_avg DESC;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local023.csv"
    },
    {
        "instance_id": "local024",
        "db_id": "IPL",
        "question": "Can you help me find the top 5 countries whose players have the highest average of their individual average runs per match across all seasons? Specifically, for each player, calculate their average runs per match over all matches they played, then compute the average of these player averages for each country, and include these country batting averages in the result.",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local024.csv"
    },
    {
        "instance_id": "local025",
        "db_id": "IPL",
        "question": "For each match, considering every innings, please combine runs from both batsman scored and extra runs for each over, then identify the single over with the highest total runs, retrieve the bowler for that over from the ball by ball table, and calculate the average of these highest over totals across all matches, ensuring that all runs and bowler details are accurately reflected.",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local025.csv"
    },
    {
        "instance_id": "local028",
        "db_id": "Brazilian_E_Commerce",
        "question": "Could you generate a report that shows the number of delivered orders for each month in the years 2016, 2017, and 2018? Each column represents a year, and each row represents a month",
        "db_type": "sqlite",
        "db_size": 62,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local028.csv"
    },
    {
        "instance_id": "local031",
        "db_id": "Brazilian_E_Commerce",
        "question": "What is the highest monthly delivered orders volume in the year with the lowest annual delivered orders volume among 2016, 2017, and 2018?",
        "db_type": "sqlite",
        "db_size": 62,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local031.csv"
    },
    {
        "instance_id": "local029",
        "db_id": "Brazilian_E_Commerce",
        "question": "Please identify the top three customers, based on their customer_unique_id, who have the highest number of delivered orders, and provide the average payment value, city, and state for each of these customers.",
        "db_type": "sqlite",
        "db_size": 62,
        "query": "WITH customer_orders AS (\n    SELECT\n        c.customer_unique_id,\n        COUNT(o.order_id) AS Total_Orders_By_Customers,\n        AVG(p.payment_value) AS Average_Payment_By_Customer,\n        c.customer_city,\n        c.customer_state\n    FROM olist_customers c\n    JOIN olist_orders o ON c.customer_id = o.customer_id\n    JOIN olist_order_payments p ON o.order_id = p.order_id\n    WHERE o.order_status = 'delivered'\n    GROUP BY c.customer_unique_id, c.customer_city, c.customer_state\n)\n\nSELECT \n    Average_Payment_By_Customer,\n    customer_city,\n    customer_state\nFROM customer_orders\nORDER BY Total_Orders_By_Customers DESC\nLIMIT 3;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local029.csv"
    },
    {
        "instance_id": "local030",
        "db_id": "Brazilian_E_Commerce",
        "question": "Among all cities with delivered orders, find the five cities whose summed payments are the lowest, then calculate the average of their total payments and the average of their total delivered order counts.",
        "db_type": "sqlite",
        "db_size": 62,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local030.csv"
    },
    {
        "instance_id": "local032",
        "db_id": "Brazilian_E_Commerce",
        "question": "Could you help me find the sellers who excel in the following categories, considering only delivered orders: the seller with the highest number of distinct customer unique IDs, the seller with the highest profit (calculated as price minus freight value), the seller with the highest number of distinct orders, and the seller with the most 5-star ratings? For each category, please provide the seller ID and the corresponding value, labeling each row with a description of the achievement.",
        "db_type": "sqlite",
        "db_size": 62,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local032.csv"
    },
    {
        "instance_id": "local034",
        "db_id": "Brazilian_E_Commerce",
        "question": "Could you help me calculate the average of the total number of payments made using the most preferred payment method for each product category, where the most preferred payment method in a category is the one with the highest number of payments?",
        "db_type": "sqlite",
        "db_size": 62,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local034.csv"
    },
    {
        "instance_id": "local037",
        "db_id": "Brazilian_E_Commerce",
        "question": "Identify the top three product categories whose most commonly used payment type has the highest number of payments across all categories, and specify the number of payments made in each category using that payment type.",
        "db_type": "sqlite",
        "db_size": 62,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local037.csv"
    },
    {
        "instance_id": "local035",
        "db_id": "Brazilian_E_Commerce",
        "question": "In the â€œolist_geolocationâ€ table, please identify which two consecutive cities, when sorted by geolocation_state, geolocation_city, geolocation_zip_code_prefix, geolocation_lat, and geolocation_lng, have the greatest distance between them based on the difference in distance computed between each city and its immediate predecessor in that ordering.",
        "db_type": "sqlite",
        "db_size": 62,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\spherical_law.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local035.csv"
    },
    {
        "instance_id": "local038",
        "db_id": "Pagila",
        "question": "Could you help me determine which actor starred most frequently in English-language children's category films that were rated either G or PG, had a running time of 120 minutes or less, and were released between 2000 and 2010? Please provide the actor's full name.",
        "db_type": "sqlite",
        "db_size": 120,
        "query": "SELECT\n    actor.first_name || ' ' || actor.last_name AS full_name\nFROM\n    actor\nINNER JOIN film_actor ON actor.actor_id = film_actor.actor_id\nINNER JOIN film ON film_actor.film_id = film.film_id\nINNER JOIN film_category ON film.film_id = film_category.film_id\nINNER JOIN category ON film_category.category_id = category.category_id\n-- Join with the language table\nINNER JOIN language ON film.language_id = language.language_id\nWHERE\n    category.name = 'Children' AND\n    film.release_year BETWEEN 2000 AND 2010 AND\n    film.rating IN ('G', 'PG') AND\n    language.name = 'English' AND\n    film.length <= 120\nGROUP BY\n    actor.actor_id, actor.first_name, actor.last_name\nORDER BY\n    COUNT(film.film_id) DESC\nLIMIT 1;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 156: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/Pagila/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local039",
        "db_id": "Pagila",
        "question": "Please help me find the film category with the highest total rental hours in cities where the city's name either starts with \"A\" or contains a hyphen. ",
        "db_type": "sqlite",
        "db_size": 120,
        "query": "SELECT\n    category.name\nFROM\n    category\nINNER JOIN film_category USING (category_id)\nINNER JOIN film USING (film_id)\nINNER JOIN inventory USING (film_id)\nINNER JOIN rental USING (inventory_id)\nINNER JOIN customer USING (customer_id)\nINNER JOIN address USING (address_id)\nINNER JOIN city USING (city_id)\nWHERE\n    LOWER(city.city) LIKE 'a%' OR city.city LIKE '%-%'\nGROUP BY\n    category.name\nORDER BY\n    SUM(CAST((julianday(rental.return_date) - julianday(rental.rental_date)) * 24 AS INTEGER)) DESC\nLIMIT\n    1;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 157: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/Pagila/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local040",
        "db_id": "modern_data",
        "question": "In the combined dataset that unifies the trees data with the income data by ZIP code, filling missing ZIP values where necessary, which three boroughs, restricted to records with median and mean income both greater than zero and a valid borough name, contain the highest number of trees, and what is the average mean income for each of these three boroughs?",
        "db_type": "sqlite",
        "db_size": 77,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local040.csv"
    },
    {
        "instance_id": "local041",
        "db_id": "modern_data",
        "question": "What percentage of trees in the Bronx have a health status of Good?",
        "db_type": "sqlite",
        "db_size": 77,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local041.csv"
    },
    {
        "instance_id": "local049",
        "db_id": "modern_data",
        "question": "Can you help me calculate the average number of new unicorn companies per year in the top industry from 2019 to 2021?",
        "db_type": "sqlite",
        "db_size": 77,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local049.csv"
    },
    {
        "instance_id": "local054",
        "db_id": "chinook",
        "question": "Could you tell me the first names of customers who spent less than $1 on albums by the best-selling artist, along with the amounts they spent?",
        "db_type": "sqlite",
        "db_size": 69,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local054.csv"
    },
    {
        "instance_id": "local055",
        "db_id": "chinook",
        "question": "Identify the artist with the highest overall sales of albums (tie broken by alphabetical order) and the artist with the lowest overall sales of albums (tie broken by alphabetical order), then calculate the amount each customer spent specifically on those two artistsâ€™ albums. Next, compute the average spending for the customers who purchased from the top-selling artist and the average spending for the customers who purchased from the lowest-selling artist, and finally return the absolute difference between these two averages.",
        "db_type": "sqlite",
        "db_size": 69,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local055.csv"
    },
    {
        "instance_id": "local198",
        "db_id": "chinook",
        "question": "Using the sales data, what is the median value of total sales made in countries where the number of customers is greater than 4?",
        "db_type": "sqlite",
        "db_size": 69,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local198.csv"
    },
    {
        "instance_id": "local058",
        "db_id": "education_business",
        "question": "Can you provide a list of hardware product segments along with their unique product counts for 2020 in the output, ordered by the highest percentage increase in unique fact sales products from 2020 to 2021?",
        "db_type": "sqlite",
        "db_size": 98,
        "query": "WITH UniqueProducts2020 AS (\n    SELECT\n        dp.segment,\n        COUNT(DISTINCT fsm.product_code) AS unique_products_2020\n    FROM\n        hardware_fact_sales_monthly fsm\n    JOIN\n        hardware_dim_product dp ON fsm.product_code = dp.product_code\n    WHERE\n        fsm.fiscal_year = 2020\n    GROUP BY\n        dp.segment\n),\nUniqueProducts2021 AS (\n    SELECT\n        dp.segment,\n        COUNT(DISTINCT fsm.product_code) AS unique_products_2021\n    FROM\n        hardware_fact_sales_monthly fsm\n    JOIN\n        hardware_dim_product dp ON fsm.product_code = dp.product_code\n    WHERE\n        fsm.fiscal_year = 2021\n    GROUP BY\n        dp.segment\n)\nSELECT\n    spc.segment,\n    spc.unique_products_2020 AS product_count_2020\nFROM\n    UniqueProducts2020 spc\nJOIN\n    UniqueProducts2021 fup ON spc.segment = fup.segment\nORDER BY\n    ((fup.unique_products_2021 - spc.unique_products_2020) * 100.0) / (spc.unique_products_2020) DESC;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 164: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/education_business/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local059",
        "db_id": "education_business",
        "question": "For the calendar year 2021, what is the overall average quantity sold of the top three best-selling hardware products (by total quantity sold) in each division?",
        "db_type": "sqlite",
        "db_size": 98,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 165: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/education_business/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local060",
        "db_id": "complex_oracle",
        "question": "In the United States, for Q4 2019 and Q4 2020, first select only those cities where total sales (with no promotions) rose by at least 20% from Q4 2019 to Q4 2020. Among these cities, rank products by their overall sales (still excluding promotions) in those quarters and take the top 20%. Then compute each top productâ€™s share of total sales in Q4 2019 and Q4 2020 and calculate the difference in share from Q4 2019 to Q4 2020, returning the results in descending order of that share change.",
        "db_type": "sqlite",
        "db_size": 140,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 166: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/complex_oracle/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local063",
        "db_id": "complex_oracle",
        "question": "Among all products sold in the United States with promo_id=999, considering only those cities whose sales increased by at least 20% from Q4 2019 (calendar_quarter_id=1772) to Q4 2020 (calendar_quarter_id=1776), which product that ranks in the top 20% of total sales has the smallest percentage-point change in its share of total sales between these two quarters?",
        "db_type": "sqlite",
        "db_size": 140,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 167: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/complex_oracle/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local061",
        "db_id": "complex_oracle",
        "question": "What is the average projected monthly sales in USD for France in 2021, considering only product sales with promotions where promo_total_id = 1 and channels where channel_total_id = 1, by taking each productâ€™s monthly sales from 2019 and 2020, calculating the growth rate from 2019 to 2020 for that same product and month, applying this growth rate to project 2021 monthly sales, converting all projected 2021 amounts to USD with the 2021 exchange rates, and finally averaging and listing them by month?",
        "db_type": "sqlite",
        "db_size": 140,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\projection_calculation.md",
        "error_info": "Error occurred while executing act() on sample 168: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/complex_oracle/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local050",
        "db_id": "complex_oracle",
        "question": "What is the median of the average monthly projected sales in USD for France in 2021, calculated by using the monthly sales data from 2019 and 2020 (filtered by promo_total_id=1 and channel_total_id=1), applying the growth rate from 2019 to 2020 to project 2021, converting to USD based on the currency table, and then determining the monthly averages before finding their median?",
        "db_type": "sqlite",
        "db_size": 140,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\projection_calculation.md",
        "error_info": "Error occurred while executing act() on sample 169: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/complex_oracle/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local062",
        "db_id": "complex_oracle",
        "question": "Please group all Italian customers into ten buckets for December 2021 by summing their profits from all products purchased (where profit is calculated as quantity_sold multiplied by the difference between unit_price and unit_cost), then divide the overall range of total monthly profits into ten equal intervals. For each bucket, provide the number of customers, and identify the minimum and maximum total profits within that bucket.",
        "db_type": "sqlite",
        "db_size": 140,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 170: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/complex_oracle/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local067",
        "db_id": "complex_oracle",
        "question": "Can you provide the highest and lowest profits for Italian customers segmented into ten evenly divided tiers based on their December 2021 sales profits?",
        "db_type": "sqlite",
        "db_size": 140,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 171: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/complex_oracle/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local070",
        "db_id": "city_legislation",
        "question": "Please examine our database records for Chinese cities (country_code_2 = 'cn') during July 2021 and identify both the shortest and longest streaks of consecutive date entries. For each date in these streaks, return exactly one record per date along with the corresponding city name. In your output, please ensure the first letter of each city name is capitalized and the rest are lowercase. Display the dates and city names for both the shortest and longest consecutive date streaks, ordered by date.",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 172: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local071",
        "db_id": "city_legislation",
        "question": "Could you review our records in June 2022 and identify which countries have the longest streak of consecutive inserted city dates? Please list the 2-letter length country codes of these countries.",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 173: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local072",
        "db_id": "city_legislation",
        "question": "Identify the country with data inserted on nine different days in January 2022. Then, find the longest consecutive period with data insertions for this country during January 2022, and calculate the proportion of entries that are from its capital city within this longest consecutive insertion period.",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 174: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local068",
        "db_id": "city_legislation",
        "question": "Calculate the number of new cities inserted in April, May, and June for each year from 2021 to 2023. For each month, compute the cumulative running total of cities added for that specific month across the years up to and including the given year (i.e., sum the counts of that month over the years). Additionally, calculate the year-over-year growth percentages for both the monthly total and the running total for each month, comparing each year to the previous year. Present the results only for 2022 and 2023, listing the year, the month, the total number of cities added in that month, the cumulative running total for that month, and the year-over-year growth percentages for both the monthly total and the running total. Use the data from 2021 solely as a baseline for calculating growth rates, and exclude it from the final output.",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 175: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local073",
        "db_id": "modern_data",
        "question": "For each pizza order, provide a single result row with the row ID, order ID, customer ID, pizza name, and final set of ingredients. The final ingredients are determined by starting with the standard toppings from the pizzaâ€™s recipe, removing any excluded toppings, and adding any extra toppings. Present the ingredients in a string starting with the pizza name followed by ': ', with ingredients listed in alphabetical order. Ingredients appearing multiple times (e.g., from standard and extra toppings) should be prefixed with '2x' and listed first, followed by single-occurrence ingredients, both in alphabetical order. Group by row ID, order ID, pizza name, and order time to ensure each order appears once. Sort results by row ID in ascending order. Assign pizza_id 1 to 'Meatlovers' pizzas and pizza_id 2 to all others.",
        "db_type": "sqlite",
        "db_size": 77,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local073.csv"
    },
    {
        "instance_id": "local066",
        "db_id": "modern_data",
        "question": "Based on our customer pizza order information, summarize the total quantity of each ingredient used in the pizzas we delivered. Output the name and quantity for each ingredient.",
        "db_type": "sqlite",
        "db_size": 77,
        "query": "WITH cte_cleaned_customer_orders AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER () AS original_row_number\n    FROM \n        pizza_clean_customer_orders\n),\nsplit_regular_toppings AS (\n    SELECT\n        pizza_id,\n        TRIM(SUBSTR(toppings, 1, INSTR(toppings || ',', ',') - 1)) AS topping_id,\n        SUBSTR(toppings || ',', INSTR(toppings || ',', ',') + 1) AS remaining_toppings\n    FROM \n        pizza_recipes\n    UNION ALL\n    SELECT\n        pizza_id,\n        TRIM(SUBSTR(remaining_toppings, 1, INSTR(remaining_toppings, ',') - 1)) AS topping_id,\n        SUBSTR(remaining_toppings, INSTR(remaining_toppings, ',') + 1) AS remaining_toppings\n    FROM \n        split_regular_toppings\n    WHERE\n        remaining_toppings <> ''\n),\ncte_base_toppings AS (\n    SELECT\n        t1.order_id,\n        t1.customer_id,\n        t1.pizza_id,\n        t1.order_time,\n        t1.original_row_number,\n        t2.topping_id\n    FROM \n        cte_cleaned_customer_orders AS t1\n    LEFT JOIN \n        split_regular_toppings AS t2\n    ON \n        t1.pizza_id = t2.pizza_id\n),\nsplit_exclusions AS (\n    SELECT\n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        TRIM(SUBSTR(exclusions, 1, INSTR(exclusions || ',', ',') - 1)) AS topping_id,\n        SUBSTR(exclusions || ',', INSTR(exclusions || ',', ',') + 1) AS remaining_exclusions\n    FROM \n        cte_cleaned_customer_orders\n    WHERE \n        exclusions IS NOT NULL\n    UNION ALL\n    SELECT\n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        TRIM(SUBSTR(remaining_exclusions, 1, INSTR(remaining_exclusions, ',') - 1)) AS topping_id,\n        SUBSTR(remaining_exclusions, INSTR(remaining_exclusions, ',') + 1) AS remaining_exclusions\n    FROM \n        split_exclusions\n    WHERE\n        remaining_exclusions <> ''\n),\nsplit_extras AS (\n    SELECT\n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        TRIM(SUBSTR(extras, 1, INSTR(extras || ',', ',') - 1)) AS topping_id,\n        SUBSTR(extras || ',', INSTR(extras || ',', ',') + 1) AS remaining_extras\n    FROM \n        cte_cleaned_customer_orders\n    WHERE \n        extras IS NOT NULL\n    UNION ALL\n    SELECT\n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        TRIM(SUBSTR(remaining_extras, 1, INSTR(remaining_extras, ',') - 1)) AS topping_id,\n        SUBSTR(remaining_extras, INSTR(remaining_extras, ',') + 1) AS remaining_extras\n    FROM \n        split_extras\n    WHERE\n        remaining_extras <> ''\n),\ncte_combined_orders AS (\n    SELECT \n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        topping_id\n    FROM \n        cte_base_toppings\n    WHERE topping_id NOT IN (SELECT topping_id FROM split_exclusions WHERE split_exclusions.order_id = cte_base_toppings.order_id)\n    UNION ALL\n    SELECT \n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        topping_id\n    FROM \n        split_extras\n)\nSELECT\n    t2.topping_name,\n    COUNT(*) AS topping_count\nFROM \n    cte_combined_orders AS t1\nJOIN \n    pizza_toppings AS t2\nON \n    t1.topping_id = t2.topping_id\nGROUP BY \n    t2.topping_name\nORDER BY \n    topping_count DESC;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local066.csv"
    },
    {
        "instance_id": "local065",
        "db_id": "modern_data",
        "question": "Calculate the total income from Meat Lovers pizzas priced at $12 and Vegetarian pizzas at $10. Include any extra toppings charged at $1 each. Ensure that canceled orders are filtered out. How much money has Pizza Runner earned in total?",
        "db_type": "sqlite",
        "db_size": 77,
        "query": "WITH get_extras_count AS (\n    WITH RECURSIVE split_extras AS (\n        SELECT\n            order_id,\n            TRIM(SUBSTR(extras, 1, INSTR(extras || ',', ',') - 1)) AS each_extra,\n            SUBSTR(extras || ',', INSTR(extras || ',', ',') + 1) AS remaining_extras\n        FROM\n            pizza_clean_customer_orders\n        UNION ALL\n        SELECT\n            order_id,\n            TRIM(SUBSTR(remaining_extras, 1, INSTR(remaining_extras, ',') - 1)) AS each_extra,\n            SUBSTR(remaining_extras, INSTR(remaining_extras, ',') + 1)\n        FROM\n            split_extras\n        WHERE\n            remaining_extras <> ''\n    )\n    SELECT\n        order_id,\n        COUNT(each_extra) AS total_extras\n    FROM\n        split_extras\n    GROUP BY\n        order_id\n),\ncalculate_totals AS (\n    SELECT\n        t1.order_id,\n        t1.pizza_id,\n        SUM(\n            CASE\n                WHEN pizza_id = 1 THEN 12\n                WHEN pizza_id = 2 THEN 10\n            END\n        ) AS total_price,\n        t3.total_extras\n    FROM \n        pizza_clean_customer_orders AS t1\n    JOIN\n        pizza_clean_runner_orders AS t2 \n    ON\n        t2.order_id = t1.order_id\n    LEFT JOIN\n        get_extras_count AS t3\n    ON\n        t3.order_id = t1.order_id\n    WHERE\n        t2.cancellation IS NULL\n    GROUP BY \n        t1.order_id,\n        t1.pizza_id,\n        t3.total_extras\n)\nSELECT \n    SUM(total_price) + SUM(total_extras) AS total_income\nFROM \n    calculate_totals;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local065.csv"
    },
    {
        "instance_id": "local074",
        "db_id": "bank_sales_trading",
        "question": "Please generate a summary of the closing balances at the end of each month for each customer transactions, show the monthly changes and monthly cumulative bank account balances. Ensure that even if a customer has no account activity in a given month, the balance for that month is still included in the output.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 179: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local064",
        "db_id": "bank_sales_trading",
        "question": "For each customer and each month of 2020, first calculate the month-end balance by adding all deposit amounts and subtracting all withdrawal amounts that occurred during that specific month. Then determine which month in 2020 has the highest count of customers with a positive month-end balance and which month has the lowest count. For each of these two months, compute the average month-end balance across all customers and provide the difference between these two averages",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 180: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local297",
        "db_id": "bank_sales_trading",
        "question": "For each customer, group all deposits and withdrawals by the first day of each month to obtain a monthly net amount, then calculate each monthâ€™s closing balance by cumulatively summing these monthly nets. Next, determine the most recent monthâ€™s growth rate by comparing its closing balance to the prior monthâ€™s balance, treating deposits as positive and withdrawals as negative, and if the previous monthâ€™s balance is zero, the growth rate should be the current monthâ€™s balance multiplied by 100. Finally, compute the percentage of customers whose most recent month shows a growth rate of more than 5%.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 181: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local298",
        "db_id": "bank_sales_trading",
        "question": "For each month, calculate the total balance from all users for the previous month (measured as of the 1st of each month), replacing any negative balances with zero. Ensure that data from the first month is used only as a baseline for calculating previous total balance, and exclude it from the final output. Sort the results in ascending order by month. ",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 182: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local299",
        "db_id": "bank_sales_trading",
        "question": "For a bank database with customer transactions, calculate each customer's daily running balance (where deposits add to the balance and other transaction types subtract). For each customer and each day, compute the 30-day rolling average balance (only after having 30 days of data, and treating negative averages as zero). Then group these daily averages by month and find each customer's maximum 30-day average balance within each month. Sum these maximum values across all customers for each month. Consider the first month of each customer's transaction history as the baseline period and exclude it from the final results, presenting monthly totals of these summed maximum 30-day average balances.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 183: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local300",
        "db_id": "bank_sales_trading",
        "question": "For each customer, calculate their daily balances for every day between their earliest and latest transaction dates, including days without transactions by carrying forward the previous day's balance. Treat any negative daily balances as zero. Then, for each month, determine the highest daily balance each customer had during that month. Finally, for each month, sum these maximum daily balances across all customers to obtain a monthly total.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 184: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local075",
        "db_id": "bank_sales_trading",
        "question": "Can you provide a breakdown of how many times each product was viewed, how many times they were added to the shopping cart, and how many times they were left in the cart without being purchased? Also, give me the count of actual purchases for each product. Ensure that products with a page id in (1, 2, 12, 13) are filtered out.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "WITH product_viewed AS (\n    SELECT\n        t1.page_id,\n        SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS n_page_views,\n        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS n_added_to_cart\n    FROM\n        shopping_cart_page_hierarchy AS t1\n    JOIN\n        shopping_cart_events AS t2\n    ON\n        t1.page_id = t2.page_id\n    WHERE\n        t1.product_id IS NOT NULL\n    GROUP BY\n        t1.page_id\n),\nproduct_purchased AS (\n    SELECT\n        t2.page_id,\n        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS purchased_from_cart\n    FROM\n        shopping_cart_page_hierarchy AS t1\n    JOIN\n        shopping_cart_events AS t2\n    ON\n        t1.page_id = t2.page_id\n    WHERE\n        t1.product_id IS NOT NULL\n        AND EXISTS (\n            SELECT\n                visit_id\n            FROM\n                shopping_cart_events\n            WHERE\n                event_type = 3\n                AND t2.visit_id = visit_id\n        )\n        AND t1.page_id NOT IN (1, 2, 12, 13)\n    GROUP BY\n        t2.page_id\n),\nproduct_abandoned AS (\n    SELECT\n        t2.page_id,\n        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS abandoned_in_cart\n    FROM\n        shopping_cart_page_hierarchy AS t1\n    JOIN\n        shopping_cart_events AS t2\n    ON\n        t1.page_id = t2.page_id\n    WHERE\n        t1.product_id IS NOT NULL\n        AND NOT EXISTS (\n            SELECT\n                visit_id\n            FROM\n                shopping_cart_events\n            WHERE\n                event_type = 3\n                AND t2.visit_id = visit_id\n        )\n        AND t1.page_id NOT IN (1, 2, 12, 13)\n    GROUP BY\n        t2.page_id\n)\nSELECT\n    t1.page_id,\n    t1.page_name,\n    t2.n_page_views AS 'number of product being viewed',\n    t2.n_added_to_cart AS 'number added to the cart',\n    t4.abandoned_in_cart AS 'without being purchased in cart',\n    t3.purchased_from_cart AS 'count of actual purchases'\nFROM\n    shopping_cart_page_hierarchy AS t1\nJOIN\n    product_viewed AS t2 \nON\n    t2.page_id = t1.page_id\nJOIN\n    product_purchased AS t3 \nON \n    t3.page_id = t1.page_id\nJOIN\n    product_abandoned AS t4 \nON \n    t4.page_id = t1.page_id;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 185: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local077",
        "db_id": "bank_sales_trading",
        "question": "Please analyze our interest data from September 2018 to August 2019. For each month, calculate the average composition for each interest by dividing the composition by the index value. Identify the interest with the highest average composition value each month and report its average composition as the max index composition for that month. Compute the three-month rolling average of these monthly max index compositions. Ensure the output includes the date, the interest name, the max index composition for that month, the rolling average, and the names and max index compositions of the top interests from one month ago and two months ago.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 186: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local078",
        "db_id": "bank_sales_trading",
        "question": "Identify the top 10 and bottom 10 interest categories based on their highest composition values across all months. For each category, display the time(MM-YYYY), interest name, and the composition value",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "WITH get_interest_rank AS (\n    SELECT\n        t1.month_year,\n        t2.interest_name,\n        t1.composition,\n        RANK() OVER (\n            PARTITION BY t2.interest_name\n            ORDER BY t1.composition DESC\n        ) AS interest_rank\n    FROM \n        interest_metrics AS t1\n    JOIN \n        interest_map AS t2\n    ON \n        t1.interest_id = t2.id\n    WHERE \n        t1.month_year IS NOT NULL\n),\nget_top_10 AS (\n    SELECT\n        month_year,\n        interest_name,\n        composition\n    FROM \n        get_interest_rank\n    WHERE \n        interest_rank = 1\n    ORDER BY \n        composition DESC\n    LIMIT 10\n),\nget_bottom_10 AS (\n    SELECT\n        month_year,\n        interest_name,\n        composition\n    FROM \n        get_interest_rank\n    WHERE \n        interest_rank = 1\n    ORDER BY \n        composition ASC\n    LIMIT 10\n)\nSELECT * \nFROM \n    get_top_10\nUNION\nSELECT * \nFROM \n    get_bottom_10\nORDER BY \n    composition DESC;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 187: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local081",
        "db_id": "northwind",
        "question": "Considering only the customers who placed orders in 1998, calculate the total amount each customer spent by summing the unit price multiplied by the quantity of all products in their orders, excluding any discounts. Assign each customer to a spending group based on the customer group thresholds, and determine how many customers are in each spending group and what percentage of the total number of customers who placed orders in 1998 each group represents.",
        "db_type": "sqlite",
        "db_size": 95,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 188: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/northwind/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local085",
        "db_id": "northwind",
        "question": "Among employees who have more than 50 total orders, which three have the highest percentage of late orders, where an order is considered late if the shipped date is on or after its required date? Please list each employee's ID, the number of late orders, and the corresponding late-order percentage.",
        "db_type": "sqlite",
        "db_size": 95,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 189: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/northwind/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local114",
        "db_id": "education_business",
        "question": "Provide a detailed web sales report for each region, including the number of orders, total sales amount, and the name and sales amount of all sales representatives who achieved the highest total sales amount in that region (include all representatives in case of a tie).",
        "db_type": "sqlite",
        "db_size": 98,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 190: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/education_business/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local128",
        "db_id": "BowlingLeague",
        "question": "List the bowlers (including their ID, first name, and last name), match number, game number, handicap score, tournament date, and location for only those bowlers who have won games with a handicap score of 190 or less at all three venues: Thunderbird Lanes, Totem Lanes, and Bolero Lanes. Only include the specific game records where they won with a handicap score of 190 or less at these three locations.",
        "db_type": "sqlite",
        "db_size": 56,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local128.csv"
    },
    {
        "instance_id": "local130",
        "db_id": "school_scheduling",
        "question": "Could you provide a list of last names for all students who have completed English courses (where completion is defined as having a ClassStatus of 2), along with their quintile ranks based on their individual grades in those courses? The quintile should be determined by calculating how many students have grades greater than or equal to each student's grade, then dividing this ranking by the total number of students who completed English courses. The quintiles should be labeled as \"First\" (top 20%), \"Second\" (top 21-40%), \"Third\" (top 41-60%), \"Fourth\" (top 61-80%), and \"Fifth\" (bottom 20%). Please sort the results from highest performing quintile to lowest (First to Fifth).",
        "db_type": "sqlite",
        "db_size": 77,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local130.csv"
    },
    {
        "instance_id": "local131",
        "db_id": "EntertainmentAgency",
        "question": "Could you list each musical style with the number of times it appears as a 1st, 2nd, or 3rd preference in a single row per style?",
        "db_type": "sqlite",
        "db_size": 76,
        "query": "SELECT \n  Musical_Styles.StyleName,\n  COUNT(RankedPreferences.FirstStyle)\n    AS FirstPreference,\n  COUNT(RankedPreferences.SecondStyle)\n    AS SecondPreference,\n  COUNT(RankedPreferences.ThirdStyle)\n    AS ThirdPreference\nFROM Musical_Styles,\n (SELECT (CASE WHEN\n    Musical_Preferences.PreferenceSeq = 1\n               THEN Musical_Preferences.StyleID\n               ELSE Null END) As FirstStyle,\n         (CASE WHEN\n    Musical_Preferences.PreferenceSeq = 2\n               THEN Musical_Preferences.StyleID\n               ELSE Null END) As SecondStyle,\n         (CASE WHEN\n    Musical_Preferences.PreferenceSeq = 3\n               THEN Musical_Preferences.StyleID\n               ELSE Null END) AS ThirdStyle\n   FROM Musical_Preferences)  AS RankedPreferences\nWHERE Musical_Styles.StyleID =\n         RankedPreferences.FirstStyle\n  OR Musical_Styles.StyleID =\n         RankedPreferences.SecondStyle\n  OR Musical_Styles.StyleID =\n         RankedPreferences.ThirdStyle\nGROUP BY StyleID, StyleName\nHAVING COUNT(FirstStyle) > 0\n     OR     COUNT(SecondStyle) > 0\n     OR     COUNT(ThirdStyle) > 0\nORDER BY FirstPreference DESC,\n        SecondPreference DESC,\n        ThirdPreference DESC, StyleID;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local131.csv"
    },
    {
        "instance_id": "local133",
        "db_id": "EntertainmentAgency",
        "question": "Given a database of musical styles and user preferences, where Musical_Preferences contains user rankings of musical styles (PreferenceSeq=1 for first choice, PreferenceSeq=2 for second choice, PreferenceSeq=3 for third choice): Calculate a weighted score for each musical style by assigning 3 points for each time it was ranked as first choice, 2 points for each second choice, and 1 point for each third choice ranking. Calculate the total weighted score for each musical style that has been ranked by at least one user. Then, compute the absolute difference between each style's total weighted score and the average total weighted score across all such styles.\t",
        "db_type": "sqlite",
        "db_size": 76,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local133.csv"
    },
    {
        "instance_id": "local132",
        "db_id": "EntertainmentAgency",
        "question": "Show all pairs of entertainers and customers who each have up to three style strengths or preferences, where the first and second style preferences of the customers match the first and second style strengths of the entertainers (or in reverse order). Only return the entertainerâ€™s stage name and the customerâ€™s last name",
        "db_type": "sqlite",
        "db_size": 76,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local132.csv"
    },
    {
        "instance_id": "local141",
        "db_id": "AdventureWorks",
        "question": "How did each salesperson's annual total sales compare to their annual sales quota? Provide the difference between their total sales and the quota for each year, organized by salesperson and year.",
        "db_type": "sqlite",
        "db_size": 120,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local141.csv"
    },
    {
        "instance_id": "local152",
        "db_id": "imdb_movies",
        "question": "Can you provide the top 9 directors by movie count, including their ID, name, number of movies, average inter-movie duration (rounded to the nearest integer), average rating (rounded to 2 decimals), total votes, minimum and maximum ratings, and total movie duration? Sort the output first by movie count in descending order and then by total movie duration in descending order.",
        "db_type": "sqlite",
        "db_size": 38,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 197: 'NoneType' object has no attribute 'keys'."
    },
    {
        "instance_id": "local230",
        "db_id": "imdb_movies",
        "question": "Determine the top three genres with the most movies rated above 8, and then identify the top four directors who have directed the most films rated above 8 within those genres. List these directors and their respective movie counts.",
        "db_type": "sqlite",
        "db_size": 38,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 198: 'NoneType' object has no attribute 'keys'."
    },
    {
        "instance_id": "local156",
        "db_id": "bank_sales_trading",
        "question": "Analyze the annual average purchase price per Bitcoin by region, computed as the total dollar amount spent divided by the total quantity purchased each year, excluding the first year's data for each region. Then, for each year, rank the regions based on these average purchase prices, and calculate the annual percentage change in cost for each region compared to the previous year.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 199: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local157",
        "db_id": "bank_sales_trading",
        "question": "Using the \"bitcoin_prices\" table, please calculate the daily percentage change in trading volume for each ticker from August 1 to August 10, 2021, ensuring that any volume ending in \"K\" or \"M\" is accurately converted to thousands or millions, any \"-\" volume is treated as zero, only non-zero volumes are used to determine the previous day's volume, and the results are ordered by ticker and date.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 200: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local163",
        "db_id": "education_business",
        "question": "Which university faculty members' salaries are closest to the average salary for their respective ranks? Please provide the ranks, first names, last names, and salaries.university",
        "db_type": "sqlite",
        "db_size": 98,
        "query": "WITH AvgSalaries AS (\n    SELECT \n        facrank AS FacRank,\n        AVG(facsalary) AS AvSalary\n    FROM \n        university_faculty\n    GROUP BY \n        facrank\n),\nSalaryDifferences AS (\n    SELECT \n        university_faculty.facrank AS FacRank, \n        university_faculty.facfirstname AS FacFirstName, \n        university_faculty.faclastname AS FacLastName, \n        university_faculty.facsalary AS Salary, \n        ABS(university_faculty.facsalary - AvgSalaries.AvSalary) AS Diff\n    FROM \n        university_faculty\n    JOIN \n        AvgSalaries ON university_faculty.facrank = AvgSalaries.FacRank\n),\nMinDifferences AS (\n    SELECT \n        FacRank, \n        MIN(Diff) AS MinDiff\n    FROM \n        SalaryDifferences\n    GROUP BY \n        FacRank\n)\nSELECT \n    s.FacRank, \n    s.FacFirstName, \n    s.FacLastName, \n    s.Salary\nFROM \n    SalaryDifferences s\nJOIN \n    MinDifferences m ON s.FacRank = m.FacRank AND s.Diff = m.MinDiff;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 201: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/education_business/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local168",
        "db_id": "city_legislation",
        "question": "Among job postings that specifically have the Data Analyst, require a non-null annual average salary, and are remote, what is the overall average salary when considering only the top three most frequently demanded skills for these positions?",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 202: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local169",
        "db_id": "city_legislation",
        "question": "What is the annual retention rate of legislators who began their first term between January 1, 1917 and December 31, 1999, measured as the proportion of this cohort still in office on December 31st for each of the first 20 years following their initial term start? The results should show all 20 periods in sequence regardless of whether any legislators were retained in a particular year.",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 203: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local171",
        "db_id": "city_legislation",
        "question": "For male legislators from Louisiana, how many distinct legislators were actively serving on December 31 of each year from more than 30 years since their first term up to less than 50 years, grouping the results by the exact number of years elapsed since their first term?",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 204: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local167",
        "db_id": "city_legislation",
        "question": "Based on the state each female legislator first represented, which state has the highest number of female legislators whose terms included December 31st at any point, and what is that count? Please provide the state's abbreviation.",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 205: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local170",
        "db_id": "city_legislation",
        "question": "Identify the state abbreviations where, for both male and female legislators, the retention rate remains greater than zero at specific intervals of 0, 2, 4, 6, 8, and 10 years after their first term start date. A legislator is considered retained if they are serving on December 31 of the respective year. Only include states where both gender cohorts maintain non-zero retention rates at all six of these time points during the first decade of service.",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 206: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local201",
        "db_id": "modern_data",
        "question": "Identify the first 10 words, sorted alphabetically, that are 4 to 5 characters long, start with 'r', and have at least one anagram of the same length, considering case-sensitive letters. Provide the count of such anagrams for each word.",
        "db_type": "sqlite",
        "db_size": 77,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local201.csv"
    },
    {
        "instance_id": "local202",
        "db_id": "city_legislation",
        "question": "For alien data, how many of the top 10 states by alien population have a higher percentage of friendly aliens than hostile aliens, with an average alien age exceeding 200?",
        "db_type": "sqlite",
        "db_size": 133,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 208: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/city_legislation/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local209",
        "db_id": "delivery_center",
        "question": "In the dataset of orders joined with store information, which store has the highest total number of orders, and among that storeâ€™s orders, what is the ratio of orders that appear in the deliveries table with a 'DELIVERED' status to the total orders for that store?",
        "db_type": "sqlite",
        "db_size": 59,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local209.csv"
    },
    {
        "instance_id": "local210",
        "db_id": "delivery_center",
        "question": "Can you identify the hubs that saw more than a 20% increase in finished orders from February to March?",
        "db_type": "sqlite",
        "db_size": 59,
        "query": "WITH february_orders AS (\n    SELECT\n        h.hub_name AS hub_name,\n        COUNT(*) AS orders_february\n    FROM \n        orders o \n    LEFT JOIN\n        stores s ON o.store_id = s.store_id \n    LEFT JOIN \n        hubs h ON s.hub_id = h.hub_id \n    WHERE o.order_created_month = 2 AND o.order_status = 'FINISHED'\n    GROUP BY\n        h.hub_name\n),\nmarch_orders AS (\n    SELECT\n        h.hub_name AS hub_name,\n        COUNT(*) AS orders_march\n    FROM \n        orders o \n    LEFT JOIN\n        stores s ON o.store_id = s.store_id \n    LEFT JOIN \n        hubs h ON s.hub_id = h.hub_id \n    WHERE o.order_created_month = 3 AND o.order_status = 'FINISHED'\n    GROUP BY\n        h.hub_name\n)\nSELECT\n    fo.hub_name\nFROM\n    february_orders fo\nLEFT JOIN \n    march_orders mo ON fo.hub_name = mo.hub_name\nWHERE \n    fo.orders_february > 0 AND \n    mo.orders_march > 0 AND\n    (CAST((mo.orders_march - fo.orders_february) AS REAL) / CAST(fo.orders_february AS REAL)) > 0.2  -- Filter for hubs with more than a 20% increase",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local210.csv"
    },
    {
        "instance_id": "local212",
        "db_id": "delivery_center",
        "question": "Can you find 5 delivery drivers with the highest average number of daily deliveries?",
        "db_type": "sqlite",
        "db_size": 59,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local212.csv"
    },
    {
        "instance_id": "local228",
        "db_id": "IPL",
        "question": "For each IPL season, identify the top three batsmen with the highest total runs scored and the top three bowlers with the most wickets taken, excluding â€˜run outâ€™, â€˜hit wicketâ€™, and â€˜retired hurtâ€™ dismissals. In the event of ties in runs or wickets, break the tie using the smaller player ID. Then output these six players in matched positionsâ€”batsman 1 with bowler 1, batsman 2 with bowler 2, and batsman 3 with bowler 3â€”in ascending order of the season ID, along with each playerâ€™s total runs or wickets.",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local228.csv"
    },
    {
        "instance_id": "local229",
        "db_id": "IPL",
        "question": "Find the IDs of players who scored the highest number of partnership runs for each match. The output should include the IDs of two players, each with their individual scores and the total partnership score. For each pair, the player with the higher individual score should be listed as player 1, and the player with the lower score as player 2. In cases where both players have the same score, the player with the higher ID should be player 1, and the player with the lower ID should be player 2. There can be multiple rows for a single match.",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local229.csv"
    },
    {
        "instance_id": "local244",
        "db_id": "music",
        "question": "Calculate the duration of each track, classify them as short, medium, or long, output the minimum and maximum time for each kind (in minutes) and the total revenue for each category, group by the category.",
        "db_type": "sqlite",
        "db_size": 64,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\music_length_type.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local244.csv"
    },
    {
        "instance_id": "local253",
        "db_id": "education_business",
        "question": "Using a Salary Dataset where the salary values need to be cleaned by removing non-numeric characters and converting them to a numeric type, write a detailed SQL query that identifies the top 5 companies by average salary in each of Mumbai, Pune, New Delhi, and Hyderabad, then compares each companyâ€™s average salary in those cities to the overall national average salary. The final result should display four columns: Location, Company Name, Average Salary in State, and Average Salary in Country, listing only the top 5 companies in each of the specified locations.",
        "db_type": "sqlite",
        "db_size": 98,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 215: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/education_business/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local258",
        "db_id": "IPL",
        "question": "Calculate the total number of wickets taken by each bowler (excluding run-outs and other dismissals not attributed to the bowler), their economy rate (total runs conceded divided by total overs bowled, considering only runs scored off the bat and ignoring any extra runs like wides and no-balls), their strike rate (average number of balls bowled per wicket taken), and their best bowling performance in a single match (the match with the most wickets taken by the bowler, formatted as \"wickets-runs\" where runs are the runs conceded excluding extras).",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\baseball_game_special_words_definition.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local258.csv"
    },
    {
        "instance_id": "local259",
        "db_id": "IPL",
        "question": "For each player, list their ID, name, their most frequent role across all matches, batting hand, bowling skill, total runs scored, total matches played, total times they were dismissed, batting average (total runs divided by total dismissals), highest score in a single match, the number of matches in which they scored at least 30 runs, at least 50 runs, and at least 100 runs, total balls faced in their career, strike rate (total runs divided by total balls faced, multiplied by 100), total wickets taken, economy rate (average runs conceded per over), and their best bowling performance in a single match (most wickets taken in a match, formatted as \"wickets taken-runs given\", where the best performance is the one with the most wickets, and if tied, the fewest runs conceded). Ignore the extra runs data.",
        "db_type": "sqlite",
        "db_size": 52,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\baseball_game_special_words_definition.md",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local259.csv"
    },
    {
        "instance_id": "local262",
        "db_id": "stacking",
        "question": "Which problems exceed the total number of times they appear in the solution table when counting all occurrences, across steps 1, 2, and 3, where any non-\"Stack\" model's maximum test score is lower than the \"Stack\" model's test score for the same step and version?",
        "db_type": "sqlite",
        "db_size": 53,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local262.csv"
    },
    {
        "instance_id": "local263",
        "db_id": "stacking",
        "question": "Identify the L1_model associated with each model (specified by name and version) that occurs most frequently for each status ('strong' or 'soft'), along with the number of times it occurs. A model has a 'strong' status if, for any of its steps, the maximum test score among non-'Stack' models is less than the 'Stack' model's test score. It has a 'soft' status if the maximum test score among non-'Stack' models equals the 'Stack' model's test score. Count how many times each L1_model is associated with a 'strong' or 'soft' status across all models, and determine which L1_model has the highest occurrence for each status.",
        "db_type": "sqlite",
        "db_size": 53,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local263.csv"
    },
    {
        "instance_id": "local264",
        "db_id": "stacking",
        "question": "Which model category (L1_model) appears the most frequently across all steps and versions when comparing traditional models to the Stack model, and what is the total count of its occurrences?",
        "db_type": "sqlite",
        "db_size": 53,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local264.csv"
    },
    {
        "instance_id": "local269",
        "db_id": "oracle_sql",
        "question": "What is the average total quantity across all final packaging combinations, considering only the leaf-level items within each combination after fully expanding any nested packaging relationships?",
        "db_type": "sqlite",
        "db_size": 124,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 221: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/oracle_sql/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local270",
        "db_id": "oracle_sql",
        "question": "Which top-level packaging containers, meaning those not contained within any other packaging, have any item for which the total quantity accumulated across all nested levels in the hierarchy exceeds 500, and what are the names of both these containers and the corresponding items?",
        "db_type": "sqlite",
        "db_size": 124,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 222: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/oracle_sql/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local272",
        "db_id": "oracle_sql",
        "question": "For order 423, identify the product IDs, aisles, and positions from which to pick the exact quantities needed for each order line, ensuring that the total picked quantity for each product matches the cumulative quantities ordered without exceeding the available inventory in warehouse 1. Calculate the quantities to be picked from each location by prioritizing inventory with earlier purchased dates and smaller quantities, and ensure that picking respects the sequence and cumulative quantities of the order lines for products with multiple entries.",
        "db_type": "sqlite",
        "db_size": 124,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 223: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/oracle_sql/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local273",
        "db_id": "oracle_sql",
        "question": "Calculate the average pick percentage for each product name, using a first-in-first-out approach that selects from inventory locations based on the earliest purchase date and smallest available quantity, ensuring that the picked quantity reflects only the overlapping range between each orderâ€™s required quantity and the inventoryâ€™s available quantity, and then grouping and ordering the results by product name?",
        "db_type": "sqlite",
        "db_size": 124,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 224: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/oracle_sql/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local274",
        "db_id": "oracle_sql",
        "question": "Which products were picked for order 421, and what is the average number of units picked for each product, using FIFO (First-In, First-Out) method?",
        "db_type": "sqlite",
        "db_size": 124,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 225: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/oracle_sql/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local275",
        "db_id": "oracle_sql",
        "question": "Based on monthly sales data starting in January 2016 and using a centered moving average to adjust for seasonality, which products had a seasonality-adjusted sales ratio that stayed consistently above 2 for every month in the year 2017?",
        "db_type": "sqlite",
        "db_size": 124,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\calculation_method.md",
        "error_info": "Error occurred while executing act() on sample 226: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/oracle_sql/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local277",
        "db_id": "oracle_sql",
        "question": "What is the average forecasted annual sales for products 4160 and 7790 during 2018, using monthly sales data starting from January 2016 for the first 36 months, applying seasonality adjustments from time steps 7 through 30, and employing a weighted regression method to estimate sales?",
        "db_type": "sqlite",
        "db_size": 124,
        "query": "",
        "external_path": "..\\benchmarks\\spider2\\lite\\external\\calculation_method.md",
        "error_info": "Error occurred while executing act() on sample 227: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/oracle_sql/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local279",
        "db_id": "oracle_sql",
        "question": "Using a recursive monthly inventory adjustment model starting from December 2018 inventory levels, where we restock a product if its ending inventory drops below the minimum required level, determine for each product the month in 2019 where the absolute difference between its ending inventory and the minimum required level is the smallest, and return the product_id, that month, and the absolute difference.",
        "db_type": "sqlite",
        "db_size": 124,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 228: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/oracle_sql/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local284",
        "db_id": "bank_sales_trading",
        "question": "For veg whsle data, can you generate a summary of our items' loss rates? Include the average loss rate, and also break down the count of items that are below, above, and within one standard deviation from this average.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 229: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local285",
        "db_id": "bank_sales_trading",
        "question": "For veg whsle data, can you analyze our financial performance over the years 2020 to 2023? I need insights into the average wholesale price, maximum wholesale price, minimum wholesale price, wholesale price difference, total wholesale price, total selling price, average loss rate, total loss, and profit for each category within each year. Round all calculated values to two decimal places.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 230: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local286",
        "db_id": "electronic_sales",
        "question": "Prepare a comprehensive performance report on our sellers, focusing on total sales, average item price, average review scores, and packing times. Ensure that the report includes only those sellers who have sold a quantity of more than 100 products and highlight the product category names in English with the highest sales volume.",
        "db_type": "sqlite",
        "db_size": 61,
        "query": "",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_local286.csv"
    },
    {
        "instance_id": "local301",
        "db_id": "bank_sales_trading",
        "question": "For weekly-sales data, I need an analysis of our sales performance around mid-June for the years 2018, 2019, and 2020. Specifically, calculate the percentage change in sales between the four weeks leading up to June 15 and the four weeks following June 15 for each year.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "SELECT \n    before_effect,\n    after_effect,\n    after_effect - before_effect AS change_amount,\n    ROUND(((after_effect * 1.0 / before_effect) - 1) * 100, 2) AS percent_change,\n    '2018' AS year\nFROM (\n    SELECT \n        SUM(CASE WHEN delta_weeks BETWEEN 1 AND 4 THEN sales END) AS after_effect,\n        SUM(CASE WHEN delta_weeks BETWEEN -3 AND 0 THEN sales END) AS before_effect\n    FROM (\n        SELECT \n            week_date,\n            ROUND((JULIANDAY(week_date) - JULIANDAY('2018-06-15')) / 7.0) + 1 AS delta_weeks,\n            sales \n        FROM cleaned_weekly_sales\n    ) add_delta_weeks\n) AS add_before_after\nUNION ALL\nSELECT \n    before_effect,\n    after_effect,\n    after_effect - before_effect AS change_amount,\n    ROUND(((after_effect * 1.0 / before_effect) - 1) * 100, 2) AS percent_change,\n    '2019' AS year\nFROM (\n    SELECT \n        SUM(CASE WHEN delta_weeks BETWEEN 1 AND 4 THEN sales END) AS after_effect,\n        SUM(CASE WHEN delta_weeks BETWEEN -3 AND 0 THEN sales END) AS before_effect\n    FROM (\n        SELECT \n            week_date,\n            ROUND((JULIANDAY(week_date) - JULIANDAY('2019-06-15')) / 7.0) + 1 AS delta_weeks,\n            sales \n        FROM cleaned_weekly_sales\n    ) add_delta_weeks\n) AS add_before_after\nUNION ALL\nSELECT \n    before_effect,\n    after_effect,\n    after_effect - before_effect AS change_amount,\n    ROUND(((after_effect * 1.0 / before_effect) - 1) * 100, 2) AS percent_change,\n    '2020' AS year\nFROM (\n    SELECT \n        SUM(CASE WHEN delta_weeks BETWEEN 1 AND 4 THEN sales END) AS after_effect,\n        SUM(CASE WHEN delta_weeks BETWEEN -3 AND 0 THEN sales END) AS before_effect\n    FROM (\n        SELECT \n            week_date,\n            ROUND((JULIANDAY(week_date) - JULIANDAY('2020-06-15')) / 7.0) + 1 AS delta_weeks,\n            sales \n        FROM cleaned_weekly_sales\n    ) add_delta_weeks\n) AS add_before_after\nORDER BY year;",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 232: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local302",
        "db_id": "bank_sales_trading",
        "question": "Analyze the average percentage change in sales between the 12 weeks before and after June 15, 2020, for each attribute type: region, platform, age band, demographic, and customer type. For each attribute type, calculate the average percentage change in sales across all its attribute values. Identify the attribute type with the highest negative impact on sales and provide its average percentage change in sales.",
        "db_type": "sqlite",
        "db_size": 106,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 233: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/bank_sales_trading/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local329",
        "db_id": "log",
        "question": "How many unique sessions visited the /regist/input page and then the /regist/confirm page, in that order?",
        "db_type": "sqlite",
        "db_size": 88,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 234: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/log/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local330",
        "db_id": "log",
        "question": "Using the activity log table, compute the total number of unique user sessions where each web page appears as either a landing page (the first page visited in a session based on timestamp) or an exit page (the last page visited in a session based on timestamp), or both. Count each session only once per page even if the page serves as both landing and exit for that session. ",
        "db_type": "sqlite",
        "db_size": 88,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 235: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/log/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local331",
        "db_id": "log",
        "question": "Which three distinct third-page visits are most frequently observed immediately after two consecutive visits to the '/detail' page, and how many times does each third-page visit occur?",
        "db_type": "sqlite",
        "db_size": 88,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 236: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/log/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local358",
        "db_id": "log",
        "question": "How many users are there in each age category (20s, 30s, 40s, 50s, and others)?",
        "db_type": "sqlite",
        "db_size": 88,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 237: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/log/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "local360",
        "db_id": "log",
        "question": "For each user session in the activity log table, identify the number of events that occurred before the first '/detail' click or '/complete' conversion, counting only events that have a non-empty search type. Find the sessions with the minimum count of such pre-click/pre-conversion events. If multiple sessions share this minimum count, include all of them in the results. Return each qualifying session along with the corresponding path and search type.",
        "db_type": "sqlite",
        "db_size": 88,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 238: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/log/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf040",
        "db_id": "US_ADDRESSES__POI",
        "question": "Find the top 10 northernmost addresses in Florida's largest zip code area. What are their address numbers, street names, and types?",
        "db_type": "snowflake",
        "db_size": 61,
        "query": "WITH zip_areas AS (\n    SELECT\n        geo.geo_id,\n        geo.geo_name AS zip,\n        states.related_geo_name AS state,\n        countries.related_geo_name AS country,\n        ST_AREA(TRY_TO_GEOGRAPHY(value)) AS area\n    FROM US_ADDRESSES__POI.CYBERSYN.geography_index AS geo\n    JOIN US_ADDRESSES__POI.CYBERSYN.geography_relationships AS states\n        ON (geo.geo_id = states.geo_id AND states.related_level = 'State')\n    JOIN US_ADDRESSES__POI.CYBERSYN.geography_relationships AS countries\n        ON (geo.geo_id = countries.geo_id AND countries.related_level = 'Country')\n    JOIN US_ADDRESSES__POI.CYBERSYN.geography_characteristics AS chars\n        ON (geo.geo_id = chars.geo_id AND chars.relationship_type = 'coordinates_geojson')\n    WHERE geo.level = 'CensusZipCodeTabulationArea'\n),\n\nzip_area_ranks AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (PARTITION BY country, state ORDER BY area DESC, geo_id) AS zip_area_rank\n    FROM zip_areas\n)\n\nSELECT addr.number, addr.street, addr.street_type\nFROM US_ADDRESSES__POI.CYBERSYN.us_addresses AS addr\nJOIN zip_area_ranks AS areas\n    ON (addr.id_zip = areas.geo_id)\nWHERE addr.state = 'FL' AND areas.country = 'United States' AND areas.zip_area_rank = 1\nORDER BY LATITUDE DESC\nLIMIT 10;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf040.csv"
    },
    {
        "instance_id": "sf009",
        "db_id": "NETHERLANDS_OPEN_MAP_DATA",
        "question": "A real estate company needs a detailed side-by-side comparison of buildings in Amsterdam and Rotterdam. They require a report showing each building class and subclass, with the total surface area (in square meters) and the total number of buildings for each classification category in both cities. The data should be organized by building class and subclass in ascending order, with Amsterdam and Rotterdam statistics presented in parallel columns to facilitate direct comparison. Can you generate this comprehensive building classification comparison report?",
        "db_type": "snowflake",
        "db_size": 91,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 240: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/NETHERLANDS_OPEN_MAP_DATA/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf013",
        "db_id": "NETHERLANDS_OPEN_MAP_DATA",
        "question": "Compare the total road lengths in Amsterdam and Rotterdam by creating a side-by-side analysis of both cities. For each combination of road class and subclass, calculate the total length of roads (in meters) specifically for QUADKEY segments '12020210' and '12020211'. Present the results with columns for class, subclass, Amsterdam's road lengths, and Rotterdam's road lengths.",
        "db_type": "snowflake",
        "db_size": 91,
        "query": "",
        "external_path": "",
        "error_info": "Error occurred while executing act() on sample 241: [Errno 2] No such file or directory: 'D:/project/Squrve/files/schema_source/spider2_lite/single_db/NETHERLANDS_OPEN_MAP_DATA/vector_store/bge_large_en_v15/docstore.json'."
    },
    {
        "instance_id": "sf011",
        "db_id": "CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE",
        "question": "Determine the population distribution within each block group relative to its census tract in New York State using 2021 ACS data. Include block group ID, census value, state county tract ID, total tract population, and the population ratio of each block group.",
        "db_type": "snowflake",
        "db_size": 57,
        "query": "WITH TractPop AS (\n    SELECT\n        CG.\"BlockGroupID\",\n        FCV.\"CensusValue\",\n        CG.\"StateCountyTractID\",\n        CG.\"BlockGroupPolygon\"\n    FROM\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Dim_CensusGeography\" CG\n    JOIN\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021\" FCV\n        ON CG.\"BlockGroupID\" = FCV.\"BlockGroupID\"\n    WHERE\n        CG.\"StateAbbrev\" = 'NY'\n        AND FCV.\"MetricID\" = 'B01003_001E'\n),\n\nTractGroup AS (\n    SELECT\n        CG.\"StateCountyTractID\",\n        SUM(FCV.\"CensusValue\") AS \"TotalTractPop\"\n    FROM\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Dim_CensusGeography\" CG\n    JOIN\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021\" FCV\n        ON CG.\"BlockGroupID\" = FCV.\"BlockGroupID\"\n    WHERE\n        CG.\"StateAbbrev\" = 'NY'\n        AND FCV.\"MetricID\" = 'B01003_001E'\n    GROUP BY\n        CG.\"StateCountyTractID\"\n)\n\nSELECT\n    TP.\"BlockGroupID\",\n    TP.\"CensusValue\",\n    TP.\"StateCountyTractID\",\n    TG.\"TotalTractPop\",\n    CASE WHEN TG.\"TotalTractPop\" <> 0 THEN TP.\"CensusValue\" / TG.\"TotalTractPop\" ELSE 0 END AS \"BlockGroupRatio\"\nFROM\n    TractPop TP\nJOIN\n    TractGroup TG\n    ON TP.\"StateCountyTractID\" = TG.\"StateCountyTractID\";",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf011.csv"
    },
    {
        "instance_id": "sf014",
        "db_id": "CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE",
        "question": "What is the New York State ZIP code with the highest number of commuters traveling over one hour, according to 2021 ACS data? Include the zip code, the total commuters, state benchmark for this duration, and state population.",
        "db_type": "snowflake",
        "db_size": 57,
        "query": "WITH Commuters AS (\n    SELECT\n        GE.\"ZipCode\",\n        SUM(CASE WHEN M.\"MetricID\" = 'B08303_013E' THEN F.\"CensusValueByZip\" ELSE 0 END +\n            CASE WHEN M.\"MetricID\" = 'B08303_012E' THEN F.\"CensusValueByZip\" ELSE 0 END) AS \"Num_Commuters_1Hr_Travel_Time\"\n    FROM\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"LU_GeographyExpanded\" GE\n    JOIN\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021_ByZip\" F\n        ON GE.\"ZipCode\" = F.\"ZipCode\"\n    JOIN\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Dim_CensusMetrics\" M\n        ON F.\"MetricID\" = M.\"MetricID\"\n    WHERE\n        GE.\"PreferredStateAbbrev\" = 'NY'\n        AND (M.\"MetricID\" = 'B08303_013E' OR M.\"MetricID\" = 'B08303_012E') -- Metric IDs for commuters with 1+ hour travel time\n    GROUP BY\n        GE.\"ZipCode\"\n),\n\nStateBenchmark AS (\n    SELECT\n        SB.\"StateAbbrev\",\n        SUM(SB.\"StateBenchmarkValue\") AS \"StateBenchmark_Over1HrTravelTime\",\n        SB.\"TotalStatePopulation\"\n    FROM\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Fact_StateBenchmark_ACS2021\" SB\n    WHERE\n        SB.\"MetricID\" IN ('B08303_013E', 'B08303_012E')\n        AND SB.\"StateAbbrev\" = 'NY'\n    GROUP BY\n        SB.\"StateAbbrev\", SB.\"TotalStatePopulation\"\n)\n\nSELECT\n    C.\"ZipCode\",\n    SUM(C.\"Num_Commuters_1Hr_Travel_Time\") AS \"Total_Commuters_1Hr_Travel_Time\",\n    SB.\"StateBenchmark_Over1HrTravelTime\",\n    SB.\"TotalStatePopulation\",\nFROM\n    Commuters C\nCROSS JOIN\n    StateBenchmark SB\nGROUP BY\n    C.\"ZipCode\", SB.\"StateBenchmark_Over1HrTravelTime\", SB.\"TotalStatePopulation\"\nORDER BY\n    \"Total_Commuters_1Hr_Travel_Time\" DESC\nLIMIT 1;",
        "external_path": "",
        "instance_schemas": "..\\files\\instance_schemas\\spider2_lite_db_size-200\\LinkAlignReducer_sf014.csv"
    }
]